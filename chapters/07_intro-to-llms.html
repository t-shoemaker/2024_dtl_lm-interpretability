
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7. Large Language Models: An Introduction &#8212; Introduction to Interpretability for Language Models</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/07_intro-to-llms';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8. Bidirectional Encoder Representations from Transformers (BERT)" href="08_bert.html" />
    <link rel="prev" title="6. Vector Space Semantics" href="06_vector-spaces.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Introduction to Interpretability for Language Models</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Python Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_getting-started.html">1. Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_python-basics.html">2. Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_data-analysis-basics.html">3. Data Analysis in Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Language Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_ngram-models.html">4. N-gram Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_vectorization.html">5. Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_vector-spaces.html">6. Vector Space Semantics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Large Language Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">7. Large Language Models: An Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_bert.html">8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_cross-entropy.html">9. Cross Entropy</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_attention-in-vector-space.html">10. Attention in Vector Space</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/t-shoemaker/2024_dtl_lm-interpretability" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/07_intro-to-llms.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Large Language Models: An Introduction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">7.1. Preliminaries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-a-pretrained-model">7.1.1. Using a pretrained model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-a-model">7.1.2. Loading a model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subword-tokenization">7.2. Subword Tokenization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-ids">7.2.1. Input IDs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-type-ids">7.2.2. Token type IDs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mask">7.2.3. Attention mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#padding-and-truncation">7.2.4. Padding and truncation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#components-of-the-transformer">7.3. Components of the Transformer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-embeddings">7.3.1. Input embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-default-embeddings">7.3.2. Other default embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">7.3.3. Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-transform">7.3.4. Linear transform</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-and-dropout">7.3.5. Normalization and dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-layer">7.3.6. Activation layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-the-model">7.4. Running the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-outputs">7.4.1. Model outputs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-layer-which-token">7.4.2. Which layer? Which token?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-attention">7.5. Examining Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-attention-weights">7.5.1. Visualizing attention weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-to-token-relationships">7.5.2. Token-to-token relationships</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-context">7.6. Examining Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-cls-tokens">7.6.1. Comparing <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> tokens</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-a-pooler">7.6.2. Defining a pooler</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-document-embeddings">7.6.3. Comparing document embeddings</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="large-language-models-an-introduction">
<h1><span class="section-number">7. </span>Large Language Models: An Introduction<a class="headerlink" href="#large-language-models-an-introduction" title="Link to this heading">#</a></h1>
<p>This chapter introduces large language models (LLMs). We will discuss
tokenization strategies, model architecture, the attention mechanism, and
dynamic embeddings. Using an example model, we end by dynamically embedding
documents to examine how each layer in the model changes documents’
representations.</p>
<ul class="simple">
<li><p><strong>Data:</strong> 59 <a class="reference external" href="https://www.poetryfoundation.org/poets/emily-dickinson#tab-poems">Emily Dickinson poems</a> collected from the Poetry
Foundation</p></li>
<li><p><strong>Credits:</strong> Portions of this chapter are adapted from the UC Davis DataLab’s
<a class="reference external" href="https://ucdavisdatalab.github.io/workshop_nlp_reader">Natural Language Processing for Data Science</a></p></li>
</ul>
<section id="preliminaries">
<h2><span class="section-number">7.1. </span>Preliminaries<a class="headerlink" href="#preliminaries" title="Link to this heading">#</a></h2>
<p>We need the following libraries:</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">import</span> <span class="nn">circuitsvis</span> <span class="k">as</span> <span class="nn">cv</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<p>Later, we will work with the Emily Dickinson poems we’ve already seen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">poems</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s2">&quot;data/datasets/dickinson_poems.parquet&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="using-a-pretrained-model">
<h3><span class="section-number">7.1.1. </span>Using a pretrained model<a class="headerlink" href="#using-a-pretrained-model" title="Link to this heading">#</a></h3>
<p>Training LLMs requires vast amounts of data and computational resources. While
these resources are expensive, the very scale of these models contributes to
their ability to generalize. Practitioners will therefore use the same model
for a variety of tasks. They do this by <strong>pretraining</strong> a general model to
perform a foundational task, usually next-token prediction. Then, once that
model is trained, practitioners <strong>fine-tune</strong> that model for other tasks. The
fine-tuned variants benefit from the generalized language representations
learned during pretraining but they adapt those representations to more
specific contexts and tasks.</p>
<p>The best place to find these pretrained models is <a class="reference external" href="https://huggingface.co/">Hugging Face</a>. The
company hosts thousands of them on its platform, and it also develops various
machine learning tools for working with these models. Hugging Face also
features fine-tuned models for various tasks, which may work out of the box for
your needs. Take a look at the <a class="reference external" href="https://huggingface.co/models">model listing</a> to see all models on the
platform. At left you’ll see categories for model types, task types, and more.</p>
</section>
<section id="loading-a-model">
<h3><span class="section-number">7.1.2. </span>Loading a model<a class="headerlink" href="#loading-a-model" title="Link to this heading">#</a></h3>
<p>To load a model from Hugging Face, specify the <strong>checkpoint</strong> you’d like to
use. Typically this is just the name of the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google-bert/bert-base-uncased&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library has different tokenizer and model classes for
different models/architectures and tasks. You can write these out directly, or
use the <code class="docutils literal notranslate"><span class="pre">Auto*</span></code> classes, which dynamically determine what class you’ll need for
a model and task. Below, we load the base BERT model without specifying a task.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">bert</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If you don’t have this model stored on your own computer, it will download
directly from Hugging Face. The default directory for storing Hugging Face data
is <code class="docutils literal notranslate"><span class="pre">~/.cache/hugggingface</span></code>. Set a <code class="docutils literal notranslate"><span class="pre">HF_HOME</span></code> environment variable from the
command line do direct downloads to a different location on your computer.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">HF_HOME</span><span class="o">=</span>/path/to/another/directory
</pre></div>
</div>
</section>
</section>
<section id="subword-tokenization">
<h2><span class="section-number">7.2. </span>Subword Tokenization<a class="headerlink" href="#subword-tokenization" title="Link to this heading">#</a></h2>
<p>You may have noticed that we initialized a tokenizer and model from the same
checkpoint. This is important: LLMs depend on specific tokenizers, which are
themselves trained on corpus data before their corresponding models even see
that data. But why do tokenizers need to be trained in the first place?</p>
<p>The answer has to do with the highly general nature of LLMs. These models are
trained on huge corpora, which means they must represent millions of different
pieces of text. Model vocabularies would quickly balloon to a huge size if they
represented all unique tokens, however, and at any rate this would be both
inefficient and a waste of resources, since some tokens are extremely rare. In
traditional tokenization and model building, you’d set a cutoff below which
rare tokens could be ignored, but LLMs need <em>all</em> text. That means they need to
represent every token in a corpus—without storing representations for every
token in a corpus.</p>
<p>Model developers square this circle by using pieces of words, or <strong>subwords</strong>,
to represent other tokens. That way, a model can literally spell out any text
sequence it needs to build without having representations for every unique
token in its training corpus. (This also means LLMs can handle text they’ve
never seen before.) Setting the cutoff for which tokens should be represented
in full and which are best represented by subwords requires training a
tokenizer to learn the token distribution in a corpus, build subwords, and
determine said cutoff.</p>
<p>With subword tokenization, the following phrase:</p>
<blockquote>
<div><p>large language models use subword tokenization</p>
</div></blockquote>
<p>…becomes:</p>
<blockquote>
<div><p>large language models use sub ##word token ##ization</p>
</div></blockquote>
<p>See the hashes? This tokenizer prepends them to its subwords.</p>
<section id="input-ids">
<h3><span class="section-number">7.2.1. </span>Input IDs<a class="headerlink" href="#input-ids" title="Link to this heading">#</a></h3>
<p>The actual output of <code class="docutils literal notranslate"><span class="pre">transformers</span></code> tokenizer has a few parts. We use the
following sentence as an example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;Then I tried to find some way of embracing my mother&#39;s ghost.&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>Send this to the tokenizer, setting the return type to PyTorch tensors. We also
return the attention mask.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Input IDs</strong> are the unique identifiers for every token in the input text.
These are what the model actually looks at.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[  101,  2059,  1045,  2699,  2000,  2424,  2070,  2126,  1997, 23581,
          2026,  2388,  1005,  1055,  5745,  1012,   102]])
</pre></div>
</div>
</div>
</div>
<p>Use the <code class="docutils literal notranslate"><span class="pre">.decode()</span></code> method to transform an ID (or sequences of ids) back to
text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="mi">5745</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;ghost&#39;
</pre></div>
</div>
</div>
</div>
<p>The tokenizer has entries for punctuation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">1005</span><span class="p">,</span> <span class="mi">1012</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;&#39;.&quot;
</pre></div>
</div>
</div>
</div>
<p>Whitespace tokens are often removed, however:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ws</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot; </span><span class="se">\t\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ws</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[101, 102]
</pre></div>
</div>
</div>
</div>
<p>But if that’s the case, what are those two IDs? These are two special tokens
that BERT uses for a few different tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">101</span><span class="p">,</span> <span class="mi">102</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;[CLS] [SEP]&#39;
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> is prepended to every input sequence. It marks the start of a sequence,
and it also serves as a “summarization” token for sequences, a kind of
aggregate representation of model outputs. When you train a model for
classification tasks, the model uses <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> to decide how to categorize a
sequence.</p>
<p><code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> is appended to every input sequence. It marks the end of a sequence,
and it is used to separate input pairs for tasks like sentence similarity,
question answering, and summarization. When training, a model looks to <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code>
to distinguish which parts of the input correspond to task components.</p>
</section>
<section id="token-type-ids">
<h3><span class="section-number">7.2.2. </span>Token type IDs<a class="headerlink" href="#token-type-ids" title="Link to this heading">#</a></h3>
<p>Some models don’t need anything more than <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> and <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> to make the above
distinctions. But other models also incorporate <strong>token type IDs</strong> to further
distinguish individual pieces of input. These IDs are binary values that tell
the model which parts of the input belong to what components in the task.</p>
<p>Our sentence makes no distinction:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
</pre></div>
</div>
</div>
</div>
<p>But a pair of sentences would:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;What did I do then?&quot;</span>
<span class="n">with_token_types</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
<span class="n">with_token_types</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
</pre></div>
</div>
</div>
</div>
</section>
<section id="attention-mask">
<h3><span class="section-number">7.2.3. </span>Attention mask<a class="headerlink" href="#attention-mask" title="Link to this heading">#</a></h3>
<p>A final output, <strong>attention mask</strong>, tells the model what part of the input
it should use when it processes the sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="padding-and-truncation">
<h3><span class="section-number">7.2.4. </span>Padding and truncation<a class="headerlink" href="#padding-and-truncation" title="Link to this heading">#</a></h3>
<p>It may seem like a redundancy to add an attention mask, but tokenizers often
<strong>pad</strong> input sequences. While Transformer models can process sequences in
parallel, which massively speeds up their run time, each sequence in a
<strong>batch</strong> needs to be the same length. Texts, however, are rarely the same
length, hence the padding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">two_sequence_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="n">question</span><span class="p">,</span> <span class="n">sentence</span><span class="p">],</span>
    <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">,</span>
    <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;longest&quot;</span>
<span class="p">)</span>
<span class="n">two_sequence_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[  101,  2054,  2106,  1045,  2079,  2059,  1029,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0],
        [  101,  2059,  1045,  2699,  2000,  2424,  2070,  2126,  1997, 23581,
          2026,  2388,  1005,  1055,  5745,  1012,   102]])
</pre></div>
</div>
</div>
</div>
<p>Token ID <code class="docutils literal notranslate"><span class="pre">0</span></code> is the <code class="docutils literal notranslate"><span class="pre">[PAD]</span></code> token.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;[PAD]&#39;
</pre></div>
</div>
</div>
</div>
<p>And here are the attention masks:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">two_sequence_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
</pre></div>
</div>
</div>
</div>
<p>There are a few different strategies for padding. Above, we had the tokenizer
pad to the longest sequence in the input. But usually it’s best to set it to
<code class="docutils literal notranslate"><span class="pre">max_length</span></code>:</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">two_sequence_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="n">question</span><span class="p">,</span> <span class="n">sentence</span><span class="p">],</span>
    <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">,</span>
    <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;max_length&quot;</span>
<span class="p">)</span>
<span class="n">two_sequence_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 101, 2054, 2106, 1045, 2079, 2059, 1029,  102,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0])
</pre></div>
</div>
</div>
</div>
<p>This will pad the text out to the maximum number of tokens the model can
process at once. This number is known as the <strong>context window</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Context window size:&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Context window size: 512
</pre></div>
</div>
</div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Not all tokenizers have this information stored in their configuration. You
should always check whether this is the case before you use a tokenizer. If it
doesn’t have this information, take a look at the model documentation.</p>
</div>
<p>If your input exceeds the number above, you will need to <strong>truncate</strong> it,
otherwise the model may not process input properly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">too_long</span> <span class="o">=</span> <span class="s2">&quot;a &quot;</span> <span class="o">*</span> <span class="mi">10_000</span>
<span class="n">too_long_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">too_long</span><span class="p">,</span> <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token indices sequence length is longer than the specified maximum sequence length for this model (10002 &gt; 512). Running this sequence through the model will result in indexing errors
</pre></div>
</div>
</div>
</div>
<p>Set <code class="docutils literal notranslate"><span class="pre">truncation</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> to avoid this problem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">too_long_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">too_long</span><span class="p">,</span>
    <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">,</span>
    <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
    <span class="n">truncation</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>What if you have long texts, like novels? You’ll need to make some decisions.
You could, for example, look for a model with a bigger context window; several
of the newest LLMs can process novel-length documents now. Or, you might
strategically chunk your text. Perhaps you’re only interested in dialogue, or
maybe paragraph-length descriptions. You could preprocess your texts to create
chunks of this kind, ensure they do not exceed your context window size, and
then send them to the model.</p>
<p>Regardless of what strategy you use, it will take iterative tries to settle on
a final tokenization workflow.</p>
</section>
</section>
<section id="components-of-the-transformer">
<h2><span class="section-number">7.3. </span>Components of the Transformer<a class="headerlink" href="#components-of-the-transformer" title="Link to this heading">#</a></h2>
<p>Before we process our tokens, let’s overview what happens when we send input to
the model. Here are all the components of the model:</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bert</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0-11): 12 x BertLayer(
        (attention): BertAttention(
          (self): BertSdpaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
</pre></div>
</div>
</div>
</div>
<p>These pieces are divided up into an embeddings portion and an encoder portion.
Both are accessible:</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bert</span><span class="o">.</span><span class="n">embeddings</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BertEmbeddings(
  (word_embeddings): Embedding(30522, 768, padding_idx=0)
  (position_embeddings): Embedding(512, 768)
  (token_type_embeddings): Embedding(2, 768)
  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
</pre></div>
</div>
</div>
</div>
<p>…and</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bert</span><span class="o">.</span><span class="n">encoder</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BertEncoder(
  (layer): ModuleList(
    (0-11): 12 x BertLayer(
      (attention): BertAttention(
        (self): BertSdpaSelfAttention(
          (query): Linear(in_features=768, out_features=768, bias=True)
          (key): Linear(in_features=768, out_features=768, bias=True)
          (value): Linear(in_features=768, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (output): BertSelfOutput(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (intermediate): BertIntermediate(
        (dense): Linear(in_features=768, out_features=3072, bias=True)
        (intermediate_act_fn): GELUActivation()
      )
      (output): BertOutput(
        (dense): Linear(in_features=3072, out_features=768, bias=True)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
)
</pre></div>
</div>
</div>
</div>
<section id="input-embeddings">
<h3><span class="section-number">7.3.1. </span>Input embeddings<a class="headerlink" href="#input-embeddings" title="Link to this heading">#</a></h3>
<p>The first layer in a LLM is typically the word embeddings matrix. These
embeddings are the starting values for every token in the model’s vocabulary
and have not been encoded with any contextual information. Think of them like
model defaults.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">bert</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Note the shape of these embeddings:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 17, 768])
</pre></div>
</div>
</div>
</div>
<p>Models assume you are working with batches, so the first number corresponds to
the number of sequences in the batch. The second number corresponds to tokens
and the third to each feature in the vectors that represent those tokens.</p>
<p>For the purposes of demonstration, we drop the batch layer with <code class="docutils literal notranslate"><span class="pre">.squeeze()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="other-default-embeddings">
<h3><span class="section-number">7.3.2. </span>Other default embeddings<a class="headerlink" href="#other-default-embeddings" title="Link to this heading">#</a></h3>
<p>BERT-style models (like the one here) also have <strong>positional embeddings</strong>,
which are learned during training. Each index in the context window has a
positional embedding vector that corresponds with it.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">bert</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">position_embeddings</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
<p>There is also a embedding matrix for <strong>token type embeddings</strong>. These
differentiate input segments.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">bert</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">token_type_embeddings</span>
</pre></div>
</div>
</section>
<section id="attention">
<h3><span class="section-number">7.3.3. </span>Attention<a class="headerlink" href="#attention" title="Link to this heading">#</a></h3>
<p>If you look back to the encoder part of the model, you’ll see that first
component in a layer block is an attention mechanism. This mechanism enables
the model to draw many-to-many relationships between tokens. During training,
attention helps the model form strong (or weak) relationships between certain
tokens, which in turn allows it to focus on different parts of input sequences
dynamically. When fed an input sequence, the model learns to privilege
relationships between certain parts of the input over others, and in doing so,
it captures complex patterns, local contexts, and long-range dependencies among
the tokens.</p>
<p>You will often see different forms of attention described in the context of
Transformers. We will walk through the three main ones. At score, however,
attention is expressed as the following:</p>
<div class="math notranslate nohighlight">
\[
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q\)</span>, <span class="math notranslate nohighlight">\(K\)</span>, and <span class="math notranslate nohighlight">\(V\)</span> are query, key, and value matrices, which correspond to all
tokens in an input sequence</p></li>
<li><p><span class="math notranslate nohighlight">\(d_k\)</span> is the dimensionality of the key matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(softmax\)</span> expresses result as a probability distribution of possible outcomes</p></li>
</ul>
<p>The following function implements this equation.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate scaled dot-product attention.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    Q, K, V : torch.Tensor</span>
<span class="sd">        Query, key, and value matrices</span>
<span class="sd">    mask : None or torch.Tensor</span>
<span class="sd">        A triangular masking matrix</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    weighted : torch.Tensor</span>
<span class="sd">        Word embeddings weighted by attention</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Perform matrix multiplication to query the keys (dot product of i-th and</span>
    <span class="c1"># j-th scores of `Q` and `K`). Note the transpose of `K`</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Normalize the scores with the square root of the dimensionality of `K`.</span>
    <span class="c1"># This reduces the magnitude of the scores, thereby preventing them from</span>
    <span class="c1"># becoming too large (which would in turn create vanishingly small</span>
    <span class="c1"># gradients during back propagation)</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">d_k</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

    <span class="c1"># Are we masking tokens to the right?</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>

    <span class="c1"># Compute softmax to convert attention scores into probabilities. Every row</span>
    <span class="c1"># in `probs` is a probability distribution across every token in the model</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Perform a final matrix multiplication between `probs` and `V`. Here,</span>
    <span class="c1"># `probs` acts as a set of weights by which to modify the original</span>
    <span class="c1"># embeddings. Matrix multiplication will aggregate all values in `V`,</span>
    <span class="c1"># producing a weighted sum</span>
    <span class="n">weighted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">weighted</span>
</pre></div>
</div>
<p><strong>Self-attention</strong> means that each token in an input sequence is compared with
every other token. This enables the model to capture relationships across the
entire input sequence.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span> <span class="o">=</span> <span class="n">K</span> <span class="o">=</span> <span class="n">V</span> <span class="o">=</span> <span class="n">embeddings</span>
<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</pre></div>
</div>
<p>The above calculates attention in a <strong>bi-directional</strong> manner, meaning tokens
to the left and right of a token are taken into account. This is what models
like BERT use. Calculating attention in a <strong>uni-directional</strong> manner, which is
what models like GPT use, requires masking out tokens to the right.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">K</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)),</span> <span class="n">diagonal</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
<p>You will often see the above referred to as <strong>causal-attention</strong>. Note however
that it is still technically self-attention.</p>
<p><strong>Cross-attention</strong> takes an external set of embeddings for the query matrix.
In translation models, for example, these are embeddings for a target language
into which a model is transforming sentences from a source language. We
simulate the external embeddings with random ones below.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">K_source</span> <span class="o">=</span> <span class="n">V_source</span> <span class="o">=</span> <span class="n">embeddings</span>
<span class="n">Q_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q_target</span><span class="p">,</span> <span class="n">K_source</span><span class="p">,</span> <span class="n">V_source</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Multi-head attention</strong> involves using multiple attention mechanisms, or
<strong>heads</strong>, in parallel, which are then concatenated together when they are
passed elsewhere in the network. During training, each head learns to focus on
different kinds of relationships in the text data.</p>
<p>This is what a head split looks like:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">n_heads</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">n_tokens</span><span class="p">,</span> <span class="n">n_dim</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span>
<span class="n">head_dim</span> <span class="o">=</span> <span class="n">n_dim</span> <span class="o">//</span> <span class="n">n_heads</span>
<span class="n">heads</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, you perform attention for each one, concatenate them all, and reshape.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">head_outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_heads</span><span class="p">):</span>
    <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">heads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
    <span class="n">head_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">head_outputs</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">)</span>
</pre></div>
</div>
<p>All of the above, however, is implemented in PyTorch. First, we initialize the
layer.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">attention_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
<p>Then we build the key, query, and value matrices. With those built, we run them
through the layer.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span> <span class="o">=</span> <span class="n">K</span> <span class="o">=</span> <span class="n">V</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">attention_scores</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">attention_layer</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="linear-transform">
<h3><span class="section-number">7.3.4. </span>Linear transform<a class="headerlink" href="#linear-transform" title="Link to this heading">#</a></h3>
<p>Once attention scores are computed, the model passes them through a <strong>linear
layer</strong>. You will also see this called a “fully connected” layer—that is, it
connects every input neuron to every output neuron. This layer maps an input
matrix to an output matrix via learnable parameters: a weight matrix and a bias
vector.</p>
<p>We express the transformation as follows:</p>
<div class="math notranslate nohighlight">
\[
y = Wx + b
\]</div>
<p>Where, for the resultant matrix <span class="math notranslate nohighlight">\(y\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(W\)</span> is a weight matrix with dimensions <span class="math notranslate nohighlight">\((m, p)\)</span>, where <span class="math notranslate nohighlight">\(m\)</span> is the number of
input features and <span class="math notranslate nohighlight">\(p\)</span> is the number of output features</p></li>
<li><p><span class="math notranslate nohighlight">\(x\)</span> is an input matrix with dimensions <span class="math notranslate nohighlight">\((n, m)\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the batch size</p></li>
<li><p><span class="math notranslate nohighlight">\(b\)</span> is a bias vector with dimensions <span class="math notranslate nohighlight">\(p\)</span></p></li>
</ul>
<p>Every value in <span class="math notranslate nohighlight">\(W\)</span> determines how much each input feature contributes to the
output. This emphasizes the importance (or lack of importance) of features for
a training task. Additionally, a linear transformation projects attention
scores into a new feature space, which helps to summarize relationships in the
input data and capture complex patterns.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">3072</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">transformed</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="normalization-and-dropout">
<h3><span class="section-number">7.3.5. </span>Normalization and dropout<a class="headerlink" href="#normalization-and-dropout" title="Link to this heading">#</a></h3>
<p>With the new projection created, the model then applies normalization and
dropout.</p>
<p><strong>Normalization</strong> standardizes inputs. This helps to stabilize the model
training by constraining values to a smaller range. It also speeds up
computations.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">norm_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span> <span class="o">=</span> <span class="mi">768</span><span class="p">)</span>
<span class="n">normed</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">transformed</span><span class="p">)</span>
</pre></div>
</div>
<p>The dropout layer randomly zeros-out a set percentage of values in its input
matrix. This combats overfitting by preventing the model from becoming too
reliant on any single dimension.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">dropped</span> <span class="o">=</span> <span class="n">dropout_layer</span><span class="p">(</span><span class="n">normalized</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="activation-layer">
<h3><span class="section-number">7.3.6. </span>Activation layer<a class="headerlink" href="#activation-layer" title="Link to this heading">#</a></h3>
<p>Finally, after the model runs through blocks of attention, linear
transformation, and normalization/dropout, it passes the data through an
<strong>activation layer</strong>. This layer introduces non-linearity in the model, which
in turn allows it to learn more complex patterns that cannot be approximated
through simple, linear relationships. You can think of linear layers as filters
of a sort: they use specially designed cutoffs to determine how input values
are transformed into outputs.</p>
<p>There are several different kinds of activation functions. We’ll demonstrate a
few below. First, we create a set of linear input values <span class="math notranslate nohighlight">\([-5, 5]\)</span> and
initialize a dictionary of our functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">activation_functions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;original&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">data</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span>
    <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
    <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="s2">&quot;ReLU&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="s2">&quot;GELU&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>We put our values through every activation function and format the results as a
DataFrame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activated</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">activation_functions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">activated</span><span class="o">.</span><span class="n">extend</span><span class="p">([(</span><span class="n">name</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yi</span><span class="p">)</span> <span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">yi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="p">)])</span>

<span class="n">activated</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">activated</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Now plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span>
    <span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span>
    <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span>
    <span class="n">hue</span> <span class="o">=</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span>
    <span class="n">style</span> <span class="o">=</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span>
    <span class="n">dashes</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">],</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">activated</span>
<span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Activation Functions&quot;</span><span class="p">,</span>
    <span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Input values&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;Output values&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/6cfd6aae413dc9b94b7f432a99bbebb5b44287ee3e3cb5e4c228066e972020a7.png" src="../_images/6cfd6aae413dc9b94b7f432a99bbebb5b44287ee3e3cb5e4c228066e972020a7.png" />
</div>
</div>
<p>GELU, or Gaussian Error Linear Unit, is a popular activation function for
Transformers.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">activation_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
<span class="n">activated</span> <span class="o">=</span> <span class="n">activation_layer</span><span class="p">(</span><span class="n">dropped</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="running-the-model">
<h2><span class="section-number">7.4. </span>Running the Model<a class="headerlink" href="#running-the-model" title="Link to this heading">#</a></h2>
<p>This is a lot of information and a lot of steps. Luckily, all of the above will
happen in a single call. But first, let’s move our model to a device (like a
GPU, represented below with <code class="docutils literal notranslate"><span class="pre">0</span></code>). The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library is pretty good at
doing this for us, but we can always do so explicitly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">bert</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Moved model to </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moved model to cpu
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can also set the model device when initializing it.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Time to process the inputs. First, put the model in evaluation mode. This
disables dropout, which can make outputs inconsistent (e.g. non-deterministic).</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bert</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0-11): 12 x BertLayer(
        (attention): BertAttention(
          (self): BertSdpaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
</pre></div>
</div>
</div>
</div>
<p>Then, wrap the process in a context manager. This context manager will keep the
model from collecting gradients when it processes. Unless you are training a
model or trying understand model internals, there’s no need for gradients. With
the context manager built, send the inputs to the model.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">bert</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="model-outputs">
<h3><span class="section-number">7.4.1. </span>Model outputs<a class="headerlink" href="#model-outputs" title="Link to this heading">#</a></h3>
<p>There are several components in this output:</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1813, -0.1627, -0.2402,  ..., -0.1174,  0.2389,  0.5933],
         [-0.1219,  0.2374, -0.8745,  ...,  0.3379,  0.4232, -0.2547],
         [ 0.3440,  0.2197, -0.0133,  ..., -0.1566,  0.2564,  0.2016],
         ...,
         [ 0.5548, -0.4396,  0.7075,  ...,  0.1718, -0.1337,  0.4442],
         [ 0.5042,  0.1461, -0.2642,  ...,  0.0728, -0.4193, -0.3139],
         [ 0.4306,  0.1996, -0.0055,  ...,  0.1924, -0.5685, -0.3190]]]), pooler_output=tensor([[-9.0215e-01, -3.6355e-01, -8.6364e-01,  6.3150e-01,  5.2959e-01,
         -2.3988e-01,  7.8954e-01,  1.9048e-01, -7.7929e-01, -9.9996e-01,
         -1.1218e-01,  9.3984e-01,  9.7191e-01,  2.6334e-01,  8.9467e-01,
         -6.9052e-01, -3.4147e-01, -5.5349e-01,  2.6398e-01, -4.4079e-01,
          6.8211e-01,  9.9988e-01,  6.3717e-02,  1.4847e-01,  4.0085e-01,
          9.8006e-01, -5.9317e-01,  8.9493e-01,  9.5012e-01,  6.7092e-01,
         -5.0414e-01,  1.7098e-01, -9.8875e-01,  9.4529e-03, -7.9112e-01,
         -9.8928e-01,  1.5902e-01, -6.4964e-01,  1.5020e-01,  1.5166e-01,
         -9.0359e-01,  2.1367e-01,  9.9995e-01, -4.1530e-01,  4.3635e-01,
         -1.1558e-01, -1.0000e+00,  2.2613e-01, -8.9463e-01,  8.5809e-01,
          8.1223e-01,  9.0436e-01,  8.7882e-02,  4.7012e-01,  4.0199e-01,
         -2.3712e-01, -1.9334e-01,  4.6882e-04, -2.2115e-01, -5.5566e-01,
         -5.7362e-01,  3.4552e-01, -7.4646e-01, -8.7389e-01,  8.9060e-01,
          6.6513e-01, -2.7116e-02, -1.4315e-01, -4.6456e-02, -1.1431e-01,
          8.5752e-01,  4.0294e-02,  1.1696e-02, -8.3643e-01,  4.3631e-01,
          1.6847e-01, -5.5569e-01,  1.0000e+00, -4.2451e-01, -9.7485e-01,
          7.4691e-01,  6.4409e-01,  4.5656e-01,  7.3656e-02,  1.3891e-01,
         -1.0000e+00,  6.1069e-01,  8.5232e-02, -9.8454e-01,  5.9957e-02,
          5.2395e-01, -2.2201e-01,  1.2185e-01,  5.0690e-01, -3.0269e-01,
         -4.4167e-01, -1.1198e-01, -7.8108e-01, -9.9268e-02, -3.5597e-01,
         -1.5475e-02,  6.0668e-02, -2.7171e-01, -1.8928e-01,  2.8009e-01,
         -4.4744e-01, -6.4973e-01,  2.8635e-01,  3.2423e-03,  5.7549e-01,
          3.8756e-01, -2.1564e-01,  2.9823e-01, -9.4773e-01,  5.1873e-01,
         -2.5598e-01, -9.8738e-01, -5.5347e-01, -9.8613e-01,  6.5802e-01,
         -1.9799e-01, -2.3325e-01,  9.4350e-01,  7.4329e-02,  2.8480e-01,
          1.1823e-01, -9.1459e-01, -1.0000e+00, -7.1362e-01, -3.3832e-01,
         -1.5427e-02, -2.2573e-01, -9.7104e-01, -9.5031e-01,  5.0464e-01,
          9.3296e-01,  1.1413e-01,  9.9982e-01, -1.4094e-01,  9.2251e-01,
         -1.1937e-02, -5.9505e-01,  6.0277e-01, -3.5788e-01,  6.4097e-01,
         -5.7841e-02, -2.5286e-01,  2.0187e-01, -2.5656e-01,  4.5353e-01,
         -6.9229e-01, -3.8542e-03, -6.3896e-01, -8.9819e-01, -2.3500e-01,
          9.3484e-01, -4.6395e-01, -8.6960e-01, -3.4996e-02, -7.0627e-02,
         -3.8399e-01,  7.3087e-01,  7.5051e-01,  2.9726e-01, -3.3074e-01,
          3.7799e-01, -1.4909e-01,  4.0023e-01, -7.6468e-01, -4.4099e-02,
          4.0490e-01, -2.2321e-01, -6.1502e-01, -9.8328e-01, -2.5075e-01,
          5.6157e-01,  9.8075e-01,  6.5358e-01,  5.6203e-02,  7.9783e-01,
         -1.8338e-01,  7.3604e-01, -9.2419e-01,  9.7970e-01,  2.9938e-02,
          5.1264e-02, -8.8993e-04,  5.2048e-01, -8.5517e-01, -2.0054e-01,
          7.7441e-01, -6.9283e-01, -8.3609e-01, -9.7109e-04, -3.6640e-01,
         -2.3179e-01, -6.8952e-01,  5.6256e-01, -1.9093e-01, -1.7197e-01,
          1.5008e-01,  9.0820e-01,  9.2894e-01,  7.8880e-01,  1.3647e-01,
          6.8747e-01, -8.5234e-01, -3.4889e-01,  1.4948e-02,  7.0052e-02,
          8.3297e-02,  9.9249e-01, -5.5713e-01,  1.2017e-02, -9.3871e-01,
         -9.8170e-01, -2.3915e-01, -8.9892e-01, -4.3212e-02, -5.4737e-01,
          5.8934e-01, -2.9702e-01,  3.1357e-01,  3.1863e-01, -9.4778e-01,
         -6.9678e-01,  3.0899e-01, -4.9348e-01,  3.4331e-01, -3.1521e-01,
          9.4813e-01,  8.7397e-01, -4.9162e-01,  4.0090e-01,  9.2305e-01,
         -8.6857e-01, -7.5446e-01,  6.5415e-01, -2.4450e-01,  8.2804e-01,
         -5.3416e-01,  9.8093e-01,  8.6154e-01,  8.7709e-01, -8.8134e-01,
         -5.8875e-01, -7.7638e-01, -4.9128e-01,  6.3842e-02, -3.1175e-01,
          8.1792e-01,  5.7528e-01,  3.3582e-01,  6.7806e-01, -5.1993e-01,
          9.9162e-01, -9.6824e-01, -9.4774e-01, -5.3149e-01,  2.6096e-02,
         -9.8804e-01,  8.2734e-01,  1.6417e-01,  3.2679e-01, -4.0681e-01,
         -4.9522e-01, -9.5393e-01,  7.7904e-01, -1.4957e-03,  9.6117e-01,
         -2.7639e-01, -8.6071e-01, -6.4372e-01, -9.0329e-01, -2.5810e-01,
         -1.1203e-01, -1.1593e-01, -2.3476e-01, -9.4845e-01,  3.6636e-01,
          5.2653e-01,  5.2235e-01, -6.2701e-01,  9.9611e-01,  1.0000e+00,
          9.7235e-01,  8.7527e-01,  8.2946e-01, -9.9941e-01, -6.6086e-01,
          9.9998e-01, -9.8423e-01, -1.0000e+00, -9.1387e-01, -6.1058e-01,
          9.1625e-02, -1.0000e+00, -1.6898e-01,  1.8335e-01, -9.1385e-01,
          6.8013e-01,  9.7530e-01,  9.8308e-01, -1.0000e+00,  8.5240e-01,
          9.2787e-01, -5.7181e-01,  9.1319e-01, -3.5524e-01,  9.6975e-01,
          3.2755e-01,  5.3928e-01, -2.5199e-02,  2.4171e-01, -8.6792e-01,
         -7.3762e-01, -3.5711e-01, -7.3350e-01,  9.9496e-01,  9.6279e-02,
         -7.7655e-01, -8.4988e-01,  6.1927e-01,  2.1021e-03, -3.2598e-01,
         -9.5913e-01, -1.0941e-01,  5.3905e-01,  7.7692e-01,  2.0210e-01,
          1.3288e-01, -5.2597e-01,  1.3350e-01, -3.0387e-01,  3.1106e-02,
          6.0131e-01, -9.2876e-01, -4.0126e-01,  9.3838e-02, -9.1770e-02,
         -2.1734e-01, -9.5718e-01,  9.5094e-01, -2.4999e-01,  8.7258e-01,
          1.0000e+00,  4.4012e-01, -8.2275e-01,  5.4966e-01,  1.5370e-01,
          2.0310e-01,  1.0000e+00,  7.8576e-01, -9.7345e-01, -5.6520e-01,
          5.5103e-01, -4.8463e-01, -6.1582e-01,  9.9890e-01, -1.2441e-01,
         -5.9766e-01, -4.3516e-01,  9.7372e-01, -9.8673e-01,  9.8594e-01,
         -8.5766e-01, -9.6840e-01,  9.5956e-01,  9.2108e-01, -6.8813e-01,
         -7.0525e-01,  3.6422e-02, -4.2375e-01,  1.7284e-01, -9.3253e-01,
          7.1364e-01,  3.9647e-01, -9.4511e-02,  8.9084e-01, -5.6835e-01,
         -5.2339e-01,  1.4913e-01, -6.5024e-01, -1.9193e-01,  9.0409e-01,
          4.0446e-01, -7.4188e-02, -5.9329e-02, -1.0553e-01, -8.4495e-01,
         -9.6772e-01,  6.0419e-01,  1.0000e+00,  1.2257e-02,  7.9661e-01,
         -2.5697e-01,  7.9121e-02, -2.7145e-01,  3.9955e-01,  3.5015e-01,
         -1.9779e-01, -8.1081e-01,  6.1581e-01, -9.3205e-01, -9.8435e-01,
          5.6242e-01,  2.5414e-02, -1.9855e-01,  9.9998e-01,  3.9147e-01,
          3.8831e-02,  3.6355e-01,  9.7193e-01, -1.5554e-01,  3.0005e-01,
          7.3116e-01,  9.7749e-01, -1.4626e-01,  5.5644e-01,  7.9268e-01,
         -8.0457e-01, -2.1986e-01, -5.8049e-01, -1.1498e-01, -9.2331e-01,
          2.5465e-01, -9.5982e-01,  9.4562e-01,  9.3056e-01,  2.6739e-01,
         -4.9384e-04,  5.2062e-01,  1.0000e+00, -8.0677e-01,  3.9905e-01,
          2.6592e-01,  5.3715e-01, -9.9927e-01, -7.9586e-01, -3.2750e-01,
         -5.8726e-02, -6.6198e-01, -2.9297e-01,  1.0346e-01, -9.6175e-01,
          5.6368e-01,  5.5213e-01, -9.7025e-01, -9.8716e-01, -3.4926e-01,
          7.4946e-01,  6.1641e-02, -9.7373e-01, -7.1220e-01, -3.7798e-01,
          5.8977e-01, -1.0241e-01, -9.3295e-01,  2.2246e-02, -1.3604e-01,
          5.2007e-01, -8.4998e-02,  5.1492e-01,  7.3342e-01,  8.4501e-01,
         -5.2785e-01, -2.8822e-01,  4.6259e-02, -6.9614e-01,  8.7093e-01,
         -7.8254e-01, -8.6091e-01, -4.9410e-03,  1.0000e+00, -4.8026e-01,
          8.4091e-01,  6.7065e-01,  7.7482e-01, -1.2159e-01,  1.1097e-01,
          7.9307e-01,  2.5259e-01, -4.3484e-01, -7.9768e-01, -5.9233e-03,
         -2.7596e-01,  6.4743e-01,  4.9924e-01,  4.2030e-01,  7.4892e-01,
          7.1720e-01,  2.1605e-01,  1.7675e-01, -7.7313e-02,  9.9814e-01,
         -1.3775e-01, -1.5530e-01, -3.0964e-01,  4.3301e-02, -2.4627e-01,
          2.7069e-01,  1.0000e+00,  2.0974e-01,  4.2502e-01, -9.8813e-01,
         -7.9993e-01, -7.9667e-01,  1.0000e+00,  8.3059e-01, -8.1765e-01,
          7.4333e-01,  6.1189e-01,  6.8243e-02,  7.5832e-01, -1.0380e-02,
          1.1044e-03,  1.9780e-01, -1.8199e-02,  9.3912e-01, -5.1335e-01,
         -9.6651e-01, -5.2125e-01,  3.9677e-01, -9.5898e-01,  9.9963e-01,
         -5.3292e-01, -2.3007e-01, -4.3810e-01, -7.4668e-02, -4.8650e-01,
         -1.8025e-01, -9.8233e-01, -1.9585e-01,  1.0636e-01,  9.5299e-01,
          1.4254e-01, -5.2442e-01, -8.6130e-01,  6.9175e-01,  7.5675e-01,
         -9.0013e-01, -9.0459e-01,  9.4746e-01, -9.7303e-01,  6.2423e-01,
          1.0000e+00,  3.3112e-01, -9.2328e-02,  1.7466e-01, -4.8845e-01,
          3.1759e-01, -3.8244e-01,  6.9155e-01, -9.5553e-01, -2.3247e-01,
         -1.3807e-01,  2.2340e-01,  3.9980e-02, -6.1394e-01,  5.1713e-01,
          6.2565e-02, -4.7686e-01, -5.7325e-01,  3.2512e-02,  3.9323e-01,
          8.0339e-01, -3.3118e-02, -1.3022e-01,  2.2383e-01, -2.0161e-02,
         -8.4427e-01, -4.3153e-01, -4.6155e-01, -9.9996e-01,  4.6373e-01,
         -1.0000e+00,  3.2713e-01, -1.4122e-01, -2.0265e-01,  8.0648e-01,
          7.0225e-01,  6.5395e-01, -5.9549e-01, -7.6726e-01,  6.8309e-01,
          6.5727e-01, -2.7250e-01, -5.2437e-01, -5.6740e-01,  2.4622e-01,
          1.5573e-01,  1.4188e-01, -5.6907e-01,  6.8004e-01, -2.5054e-01,
          1.0000e+00,  2.2881e-02, -7.2110e-01, -9.5469e-01,  4.3091e-02,
         -2.4881e-01,  1.0000e+00, -8.4721e-01, -9.5246e-01,  1.7552e-01,
         -6.6395e-01, -7.8544e-01,  3.4256e-01, -8.3179e-02, -7.1474e-01,
         -8.8283e-01,  9.0191e-01,  7.6542e-01, -5.9564e-01,  5.3151e-01,
         -1.7647e-01, -5.0729e-01, -1.2652e-01,  7.7420e-01,  9.8289e-01,
          1.4510e-01,  8.0867e-01, -1.6427e-01, -3.5557e-01,  9.6864e-01,
          2.1303e-01, -3.8065e-03, -9.2923e-02,  1.0000e+00,  1.9231e-01,
         -8.8242e-01,  7.1092e-02, -9.8139e-01, -2.3762e-02, -9.4682e-01,
          2.8557e-01,  5.2677e-02,  8.9981e-01, -1.9667e-01,  9.5172e-01,
         -6.6690e-01,  6.6642e-04, -6.0149e-01, -2.4802e-01,  3.8019e-01,
         -8.9955e-01, -9.7870e-01, -9.8522e-01,  5.4832e-01, -4.1989e-01,
         -2.7977e-02,  1.0936e-01, -1.4552e-01,  2.4088e-01,  2.6210e-01,
         -1.0000e+00,  9.2344e-01,  3.4815e-01,  7.4503e-01,  9.6188e-01,
          6.9634e-01,  6.4311e-01,  6.2565e-02, -9.7912e-01, -9.6776e-01,
         -1.6542e-01, -1.7331e-01,  4.8728e-01,  5.5929e-01,  8.0049e-01,
          3.6028e-01, -2.9847e-01, -5.4233e-01, -3.9048e-01, -9.2027e-01,
         -9.9083e-01,  2.3925e-01, -5.0385e-01, -9.1661e-01,  9.3613e-01,
         -6.3137e-01, -2.6656e-02, -7.6421e-03, -6.3848e-01,  8.7719e-01,
          8.0430e-01,  1.3750e-01, -4.6426e-02,  3.8011e-01,  8.8338e-01,
          8.8459e-01,  9.7661e-01, -7.2297e-01,  6.2925e-01, -7.2241e-01,
          3.1132e-01,  8.6522e-01, -9.2078e-01,  4.3722e-02,  2.3552e-01,
         -9.4707e-02,  1.9644e-01, -1.0795e-01, -9.3186e-01,  7.4395e-01,
         -2.4304e-01,  2.8119e-01, -2.1058e-01,  2.3263e-01, -3.1718e-01,
         -2.5258e-02, -7.1409e-01, -6.0906e-01,  6.1541e-01,  1.9725e-01,
          8.6647e-01,  7.9473e-01,  1.4623e-01, -7.4865e-01,  4.9832e-02,
         -6.5079e-01, -8.6864e-01,  8.7466e-01,  9.9015e-02,  3.3872e-02,
          5.8198e-01, -3.2675e-01,  7.9461e-01,  8.8223e-02, -3.0361e-01,
         -2.4622e-01, -6.1891e-01,  8.8182e-01, -7.5603e-01, -3.8631e-01,
         -3.5339e-01,  6.3820e-01,  2.1275e-01,  9.9991e-01, -6.3115e-01,
         -8.5991e-01, -6.4168e-01, -1.8362e-01,  2.6631e-01, -4.0186e-01,
         -1.0000e+00,  3.6668e-01, -5.3633e-01,  5.8175e-01, -5.9615e-01,
          7.6011e-01, -6.7364e-01, -9.5899e-01, -4.1586e-02,  6.7226e-01,
          6.5868e-01, -4.6331e-01, -7.2867e-01,  4.7537e-01, -4.6924e-01,
          9.4955e-01,  8.0151e-01,  6.2790e-02,  4.0838e-01,  6.3502e-01,
         -6.3827e-01, -6.5047e-01,  8.9680e-01]]), hidden_states=(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],
         [-0.2008,  0.1479,  0.1878,  ...,  0.9505,  0.9427,  0.1835],
         [-0.3319,  0.4860, -0.1578,  ...,  0.5669,  0.7301,  0.1399],
         ...,
         [-0.1509,  0.1222,  0.4894,  ...,  0.0128, -0.1437, -0.0780],
         [-0.3884,  0.6414,  0.0598,  ...,  0.6821,  0.3488,  0.7101],
         [-0.5870,  0.2658,  0.0439,  ..., -0.1067, -0.0729, -0.0851]]]), tensor([[[-0.0422,  0.0229, -0.2086,  ...,  0.1785, -0.0790, -0.0525],
         [-0.5901,  0.1755, -0.0278,  ...,  1.0815,  1.6212,  0.1523],
         [ 0.0323,  0.8927, -0.2348,  ...,  0.0032,  1.3259,  0.2274],
         ...,
         [ 0.6683,  0.2020, -0.0523,  ...,  0.0027, -0.2793,  0.1329],
         [-0.1310,  0.5102, -0.1028,  ...,  0.3445,  0.0718,  0.6305],
         [-0.3432,  0.2476, -0.0468,  ..., -0.1301,  0.1246,  0.0411]]]), tensor([[[-0.1382, -0.2264, -0.4627,  ...,  0.3514,  0.0516, -0.0463],
         [-0.8300,  0.4672, -0.2483,  ...,  1.2602,  1.2012, -0.1328],
         [ 0.7289,  0.6790, -0.3091,  ..., -0.1309,  0.9835, -0.2290],
         ...,
         [ 0.8956,  0.3428,  0.0079,  ...,  0.2997, -0.3415,  0.7970],
         [-0.1553,  0.2835,  0.2071,  ...,  0.0758, -0.0326,  0.6186],
         [-0.3426,  0.0535,  0.0638,  ...,  0.0197,  0.1122, -0.1884]]]), tensor([[[-0.0770, -0.3675, -0.2666,  ...,  0.3117,  0.2467,  0.1323],
         [-0.3731, -0.0286, -0.1670,  ...,  0.6970,  1.5362, -0.3529],
         [ 0.7061,  0.4618, -0.2415,  ..., -0.0807,  0.8768, -0.2854],
         ...,
         [ 1.3325,  0.1663, -0.0099,  ...,  0.1685, -0.1381,  0.6110],
         [-0.3374,  0.1269,  0.1817,  ..., -0.0198, -0.0905,  0.3292],
         [-0.0850, -0.0934,  0.1007,  ...,  0.0459,  0.0579, -0.0371]]]), tensor([[[ 0.0599, -0.7039, -0.8094,  ...,  0.4053,  0.2542,  0.5017],
         [-0.7397, -0.5218, -0.1666,  ...,  0.6768,  1.5843, -0.2920],
         [ 0.8869,  0.5469, -0.3197,  ..., -0.0870,  0.5288,  0.1315],
         ...,
         [ 1.5591,  0.2863,  0.2924,  ...,  0.4971, -0.0800,  0.7023],
         [-0.3145,  0.1553, -0.0974,  ..., -0.1852, -0.3847,  0.5292],
         [-0.0261, -0.0488,  0.0042,  ...,  0.0081,  0.0475, -0.0346]]]), tensor([[[-0.0289, -0.7001, -0.6573,  ..., -0.0254,  0.2115,  0.5060],
         [-0.9080, -0.4675, -0.2327,  ...,  0.2051,  1.5554, -0.3402],
         [ 1.0436,  0.5098, -0.4004,  ..., -0.4537,  0.3073,  0.5464],
         ...,
         [ 1.8741,  0.1041, -0.1578,  ...,  0.5090,  0.0933,  0.9344],
         [ 0.2248,  0.2398, -0.3275,  ..., -0.2687, -0.5662,  0.7646],
         [-0.0183, -0.0432,  0.0123,  ...,  0.0138,  0.0110, -0.0385]]]), tensor([[[ 0.1700, -0.9118, -0.5099,  ..., -0.2153,  0.4185,  0.3388],
         [-0.5750, -0.5454, -0.3029,  ..., -0.1316,  1.3756, -0.3223],
         [ 0.8847,  0.6076, -0.5053,  ..., -0.5245,  0.0685,  0.3392],
         ...,
         [ 1.8617, -0.1778,  0.0593,  ..., -0.1164,  0.1354,  1.5028],
         [ 0.3238,  0.6568, -0.6567,  ..., -0.6430, -0.4393,  0.4841],
         [ 0.0172, -0.0527, -0.0179,  ..., -0.0102, -0.0174, -0.0409]]]), tensor([[[ 0.3411, -0.8139, -0.7188,  ..., -0.6404,  0.2390,  0.1338],
         [-0.6435, -0.1589, -0.1621,  ..., -0.0504,  0.9217, -0.4096],
         [ 0.7229,  0.5266, -0.7379,  ..., -0.5187,  0.0021,  0.3104],
         ...,
         [ 1.7987,  0.0404,  0.1860,  ..., -0.3626,  0.4451,  1.3464],
         [ 0.1577, -0.0492, -1.1795,  ..., -0.8191, -0.4314,  0.3754],
         [ 0.0079, -0.0187, -0.0308,  ..., -0.0261,  0.0054, -0.0522]]]), tensor([[[ 0.2597, -0.5194, -0.8438,  ..., -0.6873, -0.1183,  0.4508],
         [-0.5360,  0.0884, -0.3540,  ..., -0.2608,  0.5271, -0.4311],
         [ 0.3990,  0.4642, -0.6246,  ..., -0.5714,  0.1685,  0.5618],
         ...,
         [ 1.3260, -0.1660,  0.4866,  ...,  0.1439,  0.5888,  0.9798],
         [-0.2248, -0.3549, -1.2145,  ..., -0.7236, -0.3995,  0.3148],
         [ 0.0038, -0.0030,  0.0181,  ..., -0.0527, -0.0362, -0.0885]]]), tensor([[[ 0.2711, -0.3491, -0.6618,  ..., -0.1569,  0.0043,  0.3841],
         [-0.4096,  0.3449, -0.8822,  ...,  0.2367,  0.2244, -0.4131],
         [ 0.4250,  0.4963, -0.3541,  ..., -0.4456,  0.2106,  0.3286],
         ...,
         [ 1.1249, -0.2633,  0.2771,  ...,  0.2688,  0.2323,  0.7970],
         [ 0.1102,  0.2645, -0.9370,  ..., -0.3904, -0.3523,  0.1010],
         [-0.0321, -0.0416,  0.0300,  ..., -0.0738, -0.0530, -0.0741]]]), tensor([[[-0.0167, -0.2538, -0.4799,  ..., -0.0870, -0.4391,  0.3460],
         [-0.2158,  0.3668, -0.8787,  ...,  0.1046, -0.1264, -0.5901],
         [ 0.4833,  0.1214,  0.0037,  ..., -0.4762,  0.0543,  0.2185],
         ...,
         [ 0.8555, -0.2857,  0.6263,  ...,  0.5248,  0.1679,  0.6346],
         [ 0.0267,  0.0116, -0.0948,  ..., -0.0126, -0.0193,  0.0141],
         [-0.0377, -0.0243,  0.1689,  ...,  0.2037, -0.1910, -0.1169]]]), tensor([[[ 0.0439, -0.2886, -0.5210,  ..., -0.0585,  0.0057,  0.3484],
         [ 0.2003,  0.1950, -0.8941,  ...,  0.2855,  0.3792, -0.4433],
         [ 0.6422,  0.2077, -0.0531,  ..., -0.2940,  0.1614,  0.3406],
         ...,
         [ 0.8555, -0.3486,  0.6021,  ...,  0.2175,  0.1230,  0.5547],
         [ 0.0507,  0.0111, -0.0194,  ...,  0.0255, -0.0229,  0.0141],
         [ 0.0348, -0.0095, -0.0097,  ...,  0.0583, -0.0379, -0.0241]]]), tensor([[[-0.1813, -0.1627, -0.2402,  ..., -0.1174,  0.2389,  0.5933],
         [-0.1219,  0.2374, -0.8745,  ...,  0.3379,  0.4232, -0.2547],
         [ 0.3440,  0.2197, -0.0133,  ..., -0.1566,  0.2564,  0.2016],
         ...,
         [ 0.5548, -0.4396,  0.7075,  ...,  0.1718, -0.1337,  0.4442],
         [ 0.5042,  0.1461, -0.2642,  ...,  0.0728, -0.4193, -0.3139],
         [ 0.4306,  0.1996, -0.0055,  ...,  0.1924, -0.5685, -0.3190]]])), past_key_values=None, attentions=None, cross_attentions=None)
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">last_hidden_state</span></code> tensor contains the hidden states for each token after
the final layer of the model. Every vector is a contextualized representation
of a token. The shape of this tensor is (batch size, sequence length, hidden
state size).</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-0.1813, -0.1627, -0.2402,  ..., -0.1174,  0.2389,  0.5933],
         [-0.1219,  0.2374, -0.8745,  ...,  0.3379,  0.4232, -0.2547],
         [ 0.3440,  0.2197, -0.0133,  ..., -0.1566,  0.2564,  0.2016],
         ...,
         [ 0.5548, -0.4396,  0.7075,  ...,  0.1718, -0.1337,  0.4442],
         [ 0.5042,  0.1461, -0.2642,  ...,  0.0728, -0.4193, -0.3139],
         [ 0.4306,  0.1996, -0.0055,  ...,  0.1924, -0.5685, -0.3190]]])
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">pooler_output</span></code> tensor is usually the one you want to use if you are
embedding text to use for some other purpose. It corresponds to the hidden
state of the <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> token. Remember that models use this as a summary
representation of the entire sequence. The shape of this tensor is (batch size,
hidden state size).</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span><span class="o">.</span><span class="n">pooler_output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-9.0215e-01, -3.6355e-01, -8.6364e-01,  6.3150e-01,  5.2959e-01,
         -2.3988e-01,  7.8954e-01,  1.9048e-01, -7.7929e-01, -9.9996e-01,
         -1.1218e-01,  9.3984e-01,  9.7191e-01,  2.6334e-01,  8.9467e-01,
         -6.9052e-01, -3.4147e-01, -5.5349e-01,  2.6398e-01, -4.4079e-01,
          6.8211e-01,  9.9988e-01,  6.3717e-02,  1.4847e-01,  4.0085e-01,
          9.8006e-01, -5.9317e-01,  8.9493e-01,  9.5012e-01,  6.7092e-01,
         -5.0414e-01,  1.7098e-01, -9.8875e-01,  9.4529e-03, -7.9112e-01,
         -9.8928e-01,  1.5902e-01, -6.4964e-01,  1.5020e-01,  1.5166e-01,
         -9.0359e-01,  2.1367e-01,  9.9995e-01, -4.1530e-01,  4.3635e-01,
         -1.1558e-01, -1.0000e+00,  2.2613e-01, -8.9463e-01,  8.5809e-01,
          8.1223e-01,  9.0436e-01,  8.7882e-02,  4.7012e-01,  4.0199e-01,
         -2.3712e-01, -1.9334e-01,  4.6882e-04, -2.2115e-01, -5.5566e-01,
         -5.7362e-01,  3.4552e-01, -7.4646e-01, -8.7389e-01,  8.9060e-01,
          6.6513e-01, -2.7116e-02, -1.4315e-01, -4.6456e-02, -1.1431e-01,
          8.5752e-01,  4.0294e-02,  1.1696e-02, -8.3643e-01,  4.3631e-01,
          1.6847e-01, -5.5569e-01,  1.0000e+00, -4.2451e-01, -9.7485e-01,
          7.4691e-01,  6.4409e-01,  4.5656e-01,  7.3656e-02,  1.3891e-01,
         -1.0000e+00,  6.1069e-01,  8.5232e-02, -9.8454e-01,  5.9957e-02,
          5.2395e-01, -2.2201e-01,  1.2185e-01,  5.0690e-01, -3.0269e-01,
         -4.4167e-01, -1.1198e-01, -7.8108e-01, -9.9268e-02, -3.5597e-01,
         -1.5475e-02,  6.0668e-02, -2.7171e-01, -1.8928e-01,  2.8009e-01,
         -4.4744e-01, -6.4973e-01,  2.8635e-01,  3.2423e-03,  5.7549e-01,
          3.8756e-01, -2.1564e-01,  2.9823e-01, -9.4773e-01,  5.1873e-01,
         -2.5598e-01, -9.8738e-01, -5.5347e-01, -9.8613e-01,  6.5802e-01,
         -1.9799e-01, -2.3325e-01,  9.4350e-01,  7.4329e-02,  2.8480e-01,
          1.1823e-01, -9.1459e-01, -1.0000e+00, -7.1362e-01, -3.3832e-01,
         -1.5427e-02, -2.2573e-01, -9.7104e-01, -9.5031e-01,  5.0464e-01,
          9.3296e-01,  1.1413e-01,  9.9982e-01, -1.4094e-01,  9.2251e-01,
         -1.1937e-02, -5.9505e-01,  6.0277e-01, -3.5788e-01,  6.4097e-01,
         -5.7841e-02, -2.5286e-01,  2.0187e-01, -2.5656e-01,  4.5353e-01,
         -6.9229e-01, -3.8542e-03, -6.3896e-01, -8.9819e-01, -2.3500e-01,
          9.3484e-01, -4.6395e-01, -8.6960e-01, -3.4996e-02, -7.0627e-02,
         -3.8399e-01,  7.3087e-01,  7.5051e-01,  2.9726e-01, -3.3074e-01,
          3.7799e-01, -1.4909e-01,  4.0023e-01, -7.6468e-01, -4.4099e-02,
          4.0490e-01, -2.2321e-01, -6.1502e-01, -9.8328e-01, -2.5075e-01,
          5.6157e-01,  9.8075e-01,  6.5358e-01,  5.6203e-02,  7.9783e-01,
         -1.8338e-01,  7.3604e-01, -9.2419e-01,  9.7970e-01,  2.9938e-02,
          5.1264e-02, -8.8993e-04,  5.2048e-01, -8.5517e-01, -2.0054e-01,
          7.7441e-01, -6.9283e-01, -8.3609e-01, -9.7109e-04, -3.6640e-01,
         -2.3179e-01, -6.8952e-01,  5.6256e-01, -1.9093e-01, -1.7197e-01,
          1.5008e-01,  9.0820e-01,  9.2894e-01,  7.8880e-01,  1.3647e-01,
          6.8747e-01, -8.5234e-01, -3.4889e-01,  1.4948e-02,  7.0052e-02,
          8.3297e-02,  9.9249e-01, -5.5713e-01,  1.2017e-02, -9.3871e-01,
         -9.8170e-01, -2.3915e-01, -8.9892e-01, -4.3212e-02, -5.4737e-01,
          5.8934e-01, -2.9702e-01,  3.1357e-01,  3.1863e-01, -9.4778e-01,
         -6.9678e-01,  3.0899e-01, -4.9348e-01,  3.4331e-01, -3.1521e-01,
          9.4813e-01,  8.7397e-01, -4.9162e-01,  4.0090e-01,  9.2305e-01,
         -8.6857e-01, -7.5446e-01,  6.5415e-01, -2.4450e-01,  8.2804e-01,
         -5.3416e-01,  9.8093e-01,  8.6154e-01,  8.7709e-01, -8.8134e-01,
         -5.8875e-01, -7.7638e-01, -4.9128e-01,  6.3842e-02, -3.1175e-01,
          8.1792e-01,  5.7528e-01,  3.3582e-01,  6.7806e-01, -5.1993e-01,
          9.9162e-01, -9.6824e-01, -9.4774e-01, -5.3149e-01,  2.6096e-02,
         -9.8804e-01,  8.2734e-01,  1.6417e-01,  3.2679e-01, -4.0681e-01,
         -4.9522e-01, -9.5393e-01,  7.7904e-01, -1.4957e-03,  9.6117e-01,
         -2.7639e-01, -8.6071e-01, -6.4372e-01, -9.0329e-01, -2.5810e-01,
         -1.1203e-01, -1.1593e-01, -2.3476e-01, -9.4845e-01,  3.6636e-01,
          5.2653e-01,  5.2235e-01, -6.2701e-01,  9.9611e-01,  1.0000e+00,
          9.7235e-01,  8.7527e-01,  8.2946e-01, -9.9941e-01, -6.6086e-01,
          9.9998e-01, -9.8423e-01, -1.0000e+00, -9.1387e-01, -6.1058e-01,
          9.1625e-02, -1.0000e+00, -1.6898e-01,  1.8335e-01, -9.1385e-01,
          6.8013e-01,  9.7530e-01,  9.8308e-01, -1.0000e+00,  8.5240e-01,
          9.2787e-01, -5.7181e-01,  9.1319e-01, -3.5524e-01,  9.6975e-01,
          3.2755e-01,  5.3928e-01, -2.5199e-02,  2.4171e-01, -8.6792e-01,
         -7.3762e-01, -3.5711e-01, -7.3350e-01,  9.9496e-01,  9.6279e-02,
         -7.7655e-01, -8.4988e-01,  6.1927e-01,  2.1021e-03, -3.2598e-01,
         -9.5913e-01, -1.0941e-01,  5.3905e-01,  7.7692e-01,  2.0210e-01,
          1.3288e-01, -5.2597e-01,  1.3350e-01, -3.0387e-01,  3.1106e-02,
          6.0131e-01, -9.2876e-01, -4.0126e-01,  9.3838e-02, -9.1770e-02,
         -2.1734e-01, -9.5718e-01,  9.5094e-01, -2.4999e-01,  8.7258e-01,
          1.0000e+00,  4.4012e-01, -8.2275e-01,  5.4966e-01,  1.5370e-01,
          2.0310e-01,  1.0000e+00,  7.8576e-01, -9.7345e-01, -5.6520e-01,
          5.5103e-01, -4.8463e-01, -6.1582e-01,  9.9890e-01, -1.2441e-01,
         -5.9766e-01, -4.3516e-01,  9.7372e-01, -9.8673e-01,  9.8594e-01,
         -8.5766e-01, -9.6840e-01,  9.5956e-01,  9.2108e-01, -6.8813e-01,
         -7.0525e-01,  3.6422e-02, -4.2375e-01,  1.7284e-01, -9.3253e-01,
          7.1364e-01,  3.9647e-01, -9.4511e-02,  8.9084e-01, -5.6835e-01,
         -5.2339e-01,  1.4913e-01, -6.5024e-01, -1.9193e-01,  9.0409e-01,
          4.0446e-01, -7.4188e-02, -5.9329e-02, -1.0553e-01, -8.4495e-01,
         -9.6772e-01,  6.0419e-01,  1.0000e+00,  1.2257e-02,  7.9661e-01,
         -2.5697e-01,  7.9121e-02, -2.7145e-01,  3.9955e-01,  3.5015e-01,
         -1.9779e-01, -8.1081e-01,  6.1581e-01, -9.3205e-01, -9.8435e-01,
          5.6242e-01,  2.5414e-02, -1.9855e-01,  9.9998e-01,  3.9147e-01,
          3.8831e-02,  3.6355e-01,  9.7193e-01, -1.5554e-01,  3.0005e-01,
          7.3116e-01,  9.7749e-01, -1.4626e-01,  5.5644e-01,  7.9268e-01,
         -8.0457e-01, -2.1986e-01, -5.8049e-01, -1.1498e-01, -9.2331e-01,
          2.5465e-01, -9.5982e-01,  9.4562e-01,  9.3056e-01,  2.6739e-01,
         -4.9384e-04,  5.2062e-01,  1.0000e+00, -8.0677e-01,  3.9905e-01,
          2.6592e-01,  5.3715e-01, -9.9927e-01, -7.9586e-01, -3.2750e-01,
         -5.8726e-02, -6.6198e-01, -2.9297e-01,  1.0346e-01, -9.6175e-01,
          5.6368e-01,  5.5213e-01, -9.7025e-01, -9.8716e-01, -3.4926e-01,
          7.4946e-01,  6.1641e-02, -9.7373e-01, -7.1220e-01, -3.7798e-01,
          5.8977e-01, -1.0241e-01, -9.3295e-01,  2.2246e-02, -1.3604e-01,
          5.2007e-01, -8.4998e-02,  5.1492e-01,  7.3342e-01,  8.4501e-01,
         -5.2785e-01, -2.8822e-01,  4.6259e-02, -6.9614e-01,  8.7093e-01,
         -7.8254e-01, -8.6091e-01, -4.9410e-03,  1.0000e+00, -4.8026e-01,
          8.4091e-01,  6.7065e-01,  7.7482e-01, -1.2159e-01,  1.1097e-01,
          7.9307e-01,  2.5259e-01, -4.3484e-01, -7.9768e-01, -5.9233e-03,
         -2.7596e-01,  6.4743e-01,  4.9924e-01,  4.2030e-01,  7.4892e-01,
          7.1720e-01,  2.1605e-01,  1.7675e-01, -7.7313e-02,  9.9814e-01,
         -1.3775e-01, -1.5530e-01, -3.0964e-01,  4.3301e-02, -2.4627e-01,
          2.7069e-01,  1.0000e+00,  2.0974e-01,  4.2502e-01, -9.8813e-01,
         -7.9993e-01, -7.9667e-01,  1.0000e+00,  8.3059e-01, -8.1765e-01,
          7.4333e-01,  6.1189e-01,  6.8243e-02,  7.5832e-01, -1.0380e-02,
          1.1044e-03,  1.9780e-01, -1.8199e-02,  9.3912e-01, -5.1335e-01,
         -9.6651e-01, -5.2125e-01,  3.9677e-01, -9.5898e-01,  9.9963e-01,
         -5.3292e-01, -2.3007e-01, -4.3810e-01, -7.4668e-02, -4.8650e-01,
         -1.8025e-01, -9.8233e-01, -1.9585e-01,  1.0636e-01,  9.5299e-01,
          1.4254e-01, -5.2442e-01, -8.6130e-01,  6.9175e-01,  7.5675e-01,
         -9.0013e-01, -9.0459e-01,  9.4746e-01, -9.7303e-01,  6.2423e-01,
          1.0000e+00,  3.3112e-01, -9.2328e-02,  1.7466e-01, -4.8845e-01,
          3.1759e-01, -3.8244e-01,  6.9155e-01, -9.5553e-01, -2.3247e-01,
         -1.3807e-01,  2.2340e-01,  3.9980e-02, -6.1394e-01,  5.1713e-01,
          6.2565e-02, -4.7686e-01, -5.7325e-01,  3.2512e-02,  3.9323e-01,
          8.0339e-01, -3.3118e-02, -1.3022e-01,  2.2383e-01, -2.0161e-02,
         -8.4427e-01, -4.3153e-01, -4.6155e-01, -9.9996e-01,  4.6373e-01,
         -1.0000e+00,  3.2713e-01, -1.4122e-01, -2.0265e-01,  8.0648e-01,
          7.0225e-01,  6.5395e-01, -5.9549e-01, -7.6726e-01,  6.8309e-01,
          6.5727e-01, -2.7250e-01, -5.2437e-01, -5.6740e-01,  2.4622e-01,
          1.5573e-01,  1.4188e-01, -5.6907e-01,  6.8004e-01, -2.5054e-01,
          1.0000e+00,  2.2881e-02, -7.2110e-01, -9.5469e-01,  4.3091e-02,
         -2.4881e-01,  1.0000e+00, -8.4721e-01, -9.5246e-01,  1.7552e-01,
         -6.6395e-01, -7.8544e-01,  3.4256e-01, -8.3179e-02, -7.1474e-01,
         -8.8283e-01,  9.0191e-01,  7.6542e-01, -5.9564e-01,  5.3151e-01,
         -1.7647e-01, -5.0729e-01, -1.2652e-01,  7.7420e-01,  9.8289e-01,
          1.4510e-01,  8.0867e-01, -1.6427e-01, -3.5557e-01,  9.6864e-01,
          2.1303e-01, -3.8065e-03, -9.2923e-02,  1.0000e+00,  1.9231e-01,
         -8.8242e-01,  7.1092e-02, -9.8139e-01, -2.3762e-02, -9.4682e-01,
          2.8557e-01,  5.2677e-02,  8.9981e-01, -1.9667e-01,  9.5172e-01,
         -6.6690e-01,  6.6642e-04, -6.0149e-01, -2.4802e-01,  3.8019e-01,
         -8.9955e-01, -9.7870e-01, -9.8522e-01,  5.4832e-01, -4.1989e-01,
         -2.7977e-02,  1.0936e-01, -1.4552e-01,  2.4088e-01,  2.6210e-01,
         -1.0000e+00,  9.2344e-01,  3.4815e-01,  7.4503e-01,  9.6188e-01,
          6.9634e-01,  6.4311e-01,  6.2565e-02, -9.7912e-01, -9.6776e-01,
         -1.6542e-01, -1.7331e-01,  4.8728e-01,  5.5929e-01,  8.0049e-01,
          3.6028e-01, -2.9847e-01, -5.4233e-01, -3.9048e-01, -9.2027e-01,
         -9.9083e-01,  2.3925e-01, -5.0385e-01, -9.1661e-01,  9.3613e-01,
         -6.3137e-01, -2.6656e-02, -7.6421e-03, -6.3848e-01,  8.7719e-01,
          8.0430e-01,  1.3750e-01, -4.6426e-02,  3.8011e-01,  8.8338e-01,
          8.8459e-01,  9.7661e-01, -7.2297e-01,  6.2925e-01, -7.2241e-01,
          3.1132e-01,  8.6522e-01, -9.2078e-01,  4.3722e-02,  2.3552e-01,
         -9.4707e-02,  1.9644e-01, -1.0795e-01, -9.3186e-01,  7.4395e-01,
         -2.4304e-01,  2.8119e-01, -2.1058e-01,  2.3263e-01, -3.1718e-01,
         -2.5258e-02, -7.1409e-01, -6.0906e-01,  6.1541e-01,  1.9725e-01,
          8.6647e-01,  7.9473e-01,  1.4623e-01, -7.4865e-01,  4.9832e-02,
         -6.5079e-01, -8.6864e-01,  8.7466e-01,  9.9015e-02,  3.3872e-02,
          5.8198e-01, -3.2675e-01,  7.9461e-01,  8.8223e-02, -3.0361e-01,
         -2.4622e-01, -6.1891e-01,  8.8182e-01, -7.5603e-01, -3.8631e-01,
         -3.5339e-01,  6.3820e-01,  2.1275e-01,  9.9991e-01, -6.3115e-01,
         -8.5991e-01, -6.4168e-01, -1.8362e-01,  2.6631e-01, -4.0186e-01,
         -1.0000e+00,  3.6668e-01, -5.3633e-01,  5.8175e-01, -5.9615e-01,
          7.6011e-01, -6.7364e-01, -9.5899e-01, -4.1586e-02,  6.7226e-01,
          6.5868e-01, -4.6331e-01, -7.2867e-01,  4.7537e-01, -4.6924e-01,
          9.4955e-01,  8.0151e-01,  6.2790e-02,  4.0838e-01,  6.3502e-01,
         -6.3827e-01, -6.5047e-01,  8.9680e-01]])
</pre></div>
</div>
</div>
</div>
<p>Setting <code class="docutils literal notranslate"><span class="pre">output_hidden_states</span> <span class="pre">=</span> <span class="pre">True</span></code> had the model return all of the hidden
states, from the first embedding layer to the very last layer. These are
accessible from <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code>. This is a tuple of tensors. Every tensor has
the shape (batch size, sequence length, hidden state size).</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],
          [-0.2008,  0.1479,  0.1878,  ...,  0.9505,  0.9427,  0.1835],
          [-0.3319,  0.4860, -0.1578,  ...,  0.5669,  0.7301,  0.1399],
          ...,
          [-0.1509,  0.1222,  0.4894,  ...,  0.0128, -0.1437, -0.0780],
          [-0.3884,  0.6414,  0.0598,  ...,  0.6821,  0.3488,  0.7101],
          [-0.5870,  0.2658,  0.0439,  ..., -0.1067, -0.0729, -0.0851]]]),
 tensor([[[-0.0422,  0.0229, -0.2086,  ...,  0.1785, -0.0790, -0.0525],
          [-0.5901,  0.1755, -0.0278,  ...,  1.0815,  1.6212,  0.1523],
          [ 0.0323,  0.8927, -0.2348,  ...,  0.0032,  1.3259,  0.2274],
          ...,
          [ 0.6683,  0.2020, -0.0523,  ...,  0.0027, -0.2793,  0.1329],
          [-0.1310,  0.5102, -0.1028,  ...,  0.3445,  0.0718,  0.6305],
          [-0.3432,  0.2476, -0.0468,  ..., -0.1301,  0.1246,  0.0411]]]),
 tensor([[[-0.1382, -0.2264, -0.4627,  ...,  0.3514,  0.0516, -0.0463],
          [-0.8300,  0.4672, -0.2483,  ...,  1.2602,  1.2012, -0.1328],
          [ 0.7289,  0.6790, -0.3091,  ..., -0.1309,  0.9835, -0.2290],
          ...,
          [ 0.8956,  0.3428,  0.0079,  ...,  0.2997, -0.3415,  0.7970],
          [-0.1553,  0.2835,  0.2071,  ...,  0.0758, -0.0326,  0.6186],
          [-0.3426,  0.0535,  0.0638,  ...,  0.0197,  0.1122, -0.1884]]]),
 tensor([[[-0.0770, -0.3675, -0.2666,  ...,  0.3117,  0.2467,  0.1323],
          [-0.3731, -0.0286, -0.1670,  ...,  0.6970,  1.5362, -0.3529],
          [ 0.7061,  0.4618, -0.2415,  ..., -0.0807,  0.8768, -0.2854],
          ...,
          [ 1.3325,  0.1663, -0.0099,  ...,  0.1685, -0.1381,  0.6110],
          [-0.3374,  0.1269,  0.1817,  ..., -0.0198, -0.0905,  0.3292],
          [-0.0850, -0.0934,  0.1007,  ...,  0.0459,  0.0579, -0.0371]]]),
 tensor([[[ 0.0599, -0.7039, -0.8094,  ...,  0.4053,  0.2542,  0.5017],
          [-0.7397, -0.5218, -0.1666,  ...,  0.6768,  1.5843, -0.2920],
          [ 0.8869,  0.5469, -0.3197,  ..., -0.0870,  0.5288,  0.1315],
          ...,
          [ 1.5591,  0.2863,  0.2924,  ...,  0.4971, -0.0800,  0.7023],
          [-0.3145,  0.1553, -0.0974,  ..., -0.1852, -0.3847,  0.5292],
          [-0.0261, -0.0488,  0.0042,  ...,  0.0081,  0.0475, -0.0346]]]),
 tensor([[[-0.0289, -0.7001, -0.6573,  ..., -0.0254,  0.2115,  0.5060],
          [-0.9080, -0.4675, -0.2327,  ...,  0.2051,  1.5554, -0.3402],
          [ 1.0436,  0.5098, -0.4004,  ..., -0.4537,  0.3073,  0.5464],
          ...,
          [ 1.8741,  0.1041, -0.1578,  ...,  0.5090,  0.0933,  0.9344],
          [ 0.2248,  0.2398, -0.3275,  ..., -0.2687, -0.5662,  0.7646],
          [-0.0183, -0.0432,  0.0123,  ...,  0.0138,  0.0110, -0.0385]]]),
 tensor([[[ 0.1700, -0.9118, -0.5099,  ..., -0.2153,  0.4185,  0.3388],
          [-0.5750, -0.5454, -0.3029,  ..., -0.1316,  1.3756, -0.3223],
          [ 0.8847,  0.6076, -0.5053,  ..., -0.5245,  0.0685,  0.3392],
          ...,
          [ 1.8617, -0.1778,  0.0593,  ..., -0.1164,  0.1354,  1.5028],
          [ 0.3238,  0.6568, -0.6567,  ..., -0.6430, -0.4393,  0.4841],
          [ 0.0172, -0.0527, -0.0179,  ..., -0.0102, -0.0174, -0.0409]]]),
 tensor([[[ 0.3411, -0.8139, -0.7188,  ..., -0.6404,  0.2390,  0.1338],
          [-0.6435, -0.1589, -0.1621,  ..., -0.0504,  0.9217, -0.4096],
          [ 0.7229,  0.5266, -0.7379,  ..., -0.5187,  0.0021,  0.3104],
          ...,
          [ 1.7987,  0.0404,  0.1860,  ..., -0.3626,  0.4451,  1.3464],
          [ 0.1577, -0.0492, -1.1795,  ..., -0.8191, -0.4314,  0.3754],
          [ 0.0079, -0.0187, -0.0308,  ..., -0.0261,  0.0054, -0.0522]]]),
 tensor([[[ 0.2597, -0.5194, -0.8438,  ..., -0.6873, -0.1183,  0.4508],
          [-0.5360,  0.0884, -0.3540,  ..., -0.2608,  0.5271, -0.4311],
          [ 0.3990,  0.4642, -0.6246,  ..., -0.5714,  0.1685,  0.5618],
          ...,
          [ 1.3260, -0.1660,  0.4866,  ...,  0.1439,  0.5888,  0.9798],
          [-0.2248, -0.3549, -1.2145,  ..., -0.7236, -0.3995,  0.3148],
          [ 0.0038, -0.0030,  0.0181,  ..., -0.0527, -0.0362, -0.0885]]]),
 tensor([[[ 0.2711, -0.3491, -0.6618,  ..., -0.1569,  0.0043,  0.3841],
          [-0.4096,  0.3449, -0.8822,  ...,  0.2367,  0.2244, -0.4131],
          [ 0.4250,  0.4963, -0.3541,  ..., -0.4456,  0.2106,  0.3286],
          ...,
          [ 1.1249, -0.2633,  0.2771,  ...,  0.2688,  0.2323,  0.7970],
          [ 0.1102,  0.2645, -0.9370,  ..., -0.3904, -0.3523,  0.1010],
          [-0.0321, -0.0416,  0.0300,  ..., -0.0738, -0.0530, -0.0741]]]),
 tensor([[[-0.0167, -0.2538, -0.4799,  ..., -0.0870, -0.4391,  0.3460],
          [-0.2158,  0.3668, -0.8787,  ...,  0.1046, -0.1264, -0.5901],
          [ 0.4833,  0.1214,  0.0037,  ..., -0.4762,  0.0543,  0.2185],
          ...,
          [ 0.8555, -0.2857,  0.6263,  ...,  0.5248,  0.1679,  0.6346],
          [ 0.0267,  0.0116, -0.0948,  ..., -0.0126, -0.0193,  0.0141],
          [-0.0377, -0.0243,  0.1689,  ...,  0.2037, -0.1910, -0.1169]]]),
 tensor([[[ 0.0439, -0.2886, -0.5210,  ..., -0.0585,  0.0057,  0.3484],
          [ 0.2003,  0.1950, -0.8941,  ...,  0.2855,  0.3792, -0.4433],
          [ 0.6422,  0.2077, -0.0531,  ..., -0.2940,  0.1614,  0.3406],
          ...,
          [ 0.8555, -0.3486,  0.6021,  ...,  0.2175,  0.1230,  0.5547],
          [ 0.0507,  0.0111, -0.0194,  ...,  0.0255, -0.0229,  0.0141],
          [ 0.0348, -0.0095, -0.0097,  ...,  0.0583, -0.0379, -0.0241]]]),
 tensor([[[-0.1813, -0.1627, -0.2402,  ..., -0.1174,  0.2389,  0.5933],
          [-0.1219,  0.2374, -0.8745,  ...,  0.3379,  0.4232, -0.2547],
          [ 0.3440,  0.2197, -0.0133,  ..., -0.1566,  0.2564,  0.2016],
          ...,
          [ 0.5548, -0.4396,  0.7075,  ...,  0.1718, -0.1337,  0.4442],
          [ 0.5042,  0.1461, -0.2642,  ...,  0.0728, -0.4193, -0.3139],
          [ 0.4306,  0.1996, -0.0055,  ...,  0.1924, -0.5685, -0.3190]]]))
</pre></div>
</div>
</div>
</div>
<p>Above, we pulled the embeddings from <code class="docutils literal notranslate"><span class="pre">bert.embeddings.word_embeddings()</span></code>, but
we can also access them from the <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hs_embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="o">==</span> <span class="n">hs_embeddings</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;Embeddings aren&#39;t the same!&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>Other optional outputs, which we don’t have here, include the following:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">past_key_values</span></code>: previously computed key and value matrices, which
generative models can draw on to speed up computation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">attentions</span></code>: attention weights for every layer in the model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_attentions</span></code>: layer-by-layer attention weights for models that work by
attending to tokens across input pairs</p></li>
</ul>
</section>
<section id="which-layer-which-token">
<h3><span class="section-number">7.4.2. </span>Which layer? Which token?<a class="headerlink" href="#which-layer-which-token" title="Link to this heading">#</a></h3>
<p>The next chapter demonstrates a classification task with BERT. This involves
modifying the network layers to output one of a set of labels for input. All
this will happen inside the model itself, but you can also generate embeddings
with a model and to use those embeddings for some other task that has nothing
to do with a LLM.</p>
<p>People often use the last hidden state embeddings for other tasks, though
there’s no hard and fast rule saying that this is necessary. The
<a class="reference external" href="https://aclanthology.org/2020.tacl-1.54/">BERTology paper</a> tells us that different layers in BERT do
different things: earlier ones capture syntactic features, while later ones
capture more semantic features. If you’re studying syntax, you might choose an
earlier layer, or set of layers.</p>
<p>For general document embeddings, there are a number of options:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Strategy</p></th>
<th class="head"><p>Token(s)</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Effect</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mean pooling</p></td>
<td><p>All</p></td>
<td><p>Mean of the last hidden layer</p></td>
<td><p>Smoothes out noise</p></td>
</tr>
<tr class="row-odd"><td><p>Max pooling</p></td>
<td><p>All</p></td>
<td><p>Max of the last hidden layer</p></td>
<td><p>Boosts salient features</p></td>
</tr>
<tr class="row-even"><td><p>Last four layer mean</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[CLS]</span></code></p></td>
<td><p>Mean of last four hidden layers</p></td>
<td><p>Smoothing with more information</p></td>
</tr>
<tr class="row-odd"><td><p>Last four layer max</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[CLS]</span></code></p></td>
<td><p>Max of last four hidden layers</p></td>
<td><p>Saliency with more information</p></td>
</tr>
<tr class="row-even"><td><p>Concatenate last four layers</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[CLS]</span></code></p></td>
<td><p>Append the last four layers</p></td>
<td><p>Combine levels of abstraction</p></td>
</tr>
</tbody>
</table>
</div>
<p>Finally, while using <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> is customary, it’s not necessary for all purposes
and you can select another token if you feel it would be better. You can even
train a classification model to learn from a different token, but be warned:
one of the reasons <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> is customary is because this token is in every input
sequence. The same cannot always be said of other tokens.</p>
</section>
</section>
<section id="examining-attention">
<h2><span class="section-number">7.5. </span>Examining Attention<a class="headerlink" href="#examining-attention" title="Link to this heading">#</a></h2>
<p>The rest of this chapter will demonstrate how all the above layers transform
data over the course of model processing. We won’t do analysis per se, just
some looking around.</p>
<p>First: attention. Let’s re-run our inputs through the model and ask it to
return attention scores.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">bert</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output_attentions</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Stored in the <code class="docutils literal notranslate"><span class="pre">.attentions</span></code> attribute are attention weights for each layer in
the network. The shape of each weight matrix is as follows: batch size, number
of heads, number of tokens, number of tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of weight matrices:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of a weight matrix:&quot;</span><span class="p">,</span> <span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of weight matrices: 12
Shape of a weight matrix: torch.Size([1, 12, 17, 17])
</pre></div>
</div>
</div>
</div>
<section id="visualizing-attention-weights">
<h3><span class="section-number">7.5.1. </span>Visualizing attention weights<a class="headerlink" href="#visualizing-attention-weights" title="Link to this heading">#</a></h3>
<p>Below, we extract the matrices from the model outputs, squeeze out the batch
dimension, and convert the PyTorch matrices to NumPy ones.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">attentions</span> <span class="o">=</span> <span class="p">[</span><span class="n">attn</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">attn</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Use <code class="docutils literal notranslate"><span class="pre">circuitsvis</span></code> to render a heatmap for every head in a layer. Use labels
from the tokenizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokid</span><span class="p">)</span> <span class="k">for</span> <span class="n">tokid</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
<span class="n">cv</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_heads</span><span class="p">(</span>
    <span class="n">attentions</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">negative_color</span> <span class="o">=</span> <span class="s2">&quot;#1f78b4&quot;</span><span class="p">,</span>
    <span class="n">positive_color</span> <span class="o">=</span> <span class="s2">&quot;#e31a1c&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div id="circuits-vis-ad516a97-a438" style="margin: 15px 0;"/>
    <script crossorigin type="module">
    import { render, AttentionHeads } from "https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js";
    render(
      "circuits-vis-ad516a97-a438",
      AttentionHeads,
      {"attention": [[[0.040366340428590775, 0.06489410251379013, 0.030769625678658485, 0.03937108814716339, 0.10098583996295929, 0.02540595270693302, 0.07179750502109528, 0.037355564534664154, 0.05822756513953209, 0.05554118752479553, 0.04726696386933327, 0.06025823950767517, 0.03080206736922264, 0.027247672900557518, 0.024133482947945595, 0.08059891313314438, 0.2049778699874878], [0.07777003198862076, 0.039379023015499115, 0.060007013380527496, 0.09192187339067459, 0.07213697582483292, 0.07653458416461945, 0.04503144696354866, 0.028457237407565117, 0.04775780066847801, 0.05034760758280754, 0.060071349143981934, 0.03345257788896561, 0.0410660021007061, 0.037478722631931305, 0.06861355155706406, 0.11478967219591141, 0.05518455058336258], [0.08497639000415802, 0.029455581679940224, 0.09540452063083649, 0.10967101901769638, 0.03422827646136284, 0.05421304702758789, 0.025201618671417236, 0.04368503764271736, 0.0247691310942173, 0.06504297256469727, 0.1699700504541397, 0.03482849895954132, 0.05568799749016762, 0.029983436688780785, 0.05306759104132652, 0.04000160098075867, 0.04981320723891258], [0.03460358828306198, 0.048829980194568634, 0.06951747834682465, 0.06410208344459534, 0.06334435194730759, 0.07161479443311691, 0.03439074754714966, 0.02930254116654396, 0.056518323719501495, 0.050341080874204636, 0.08819333463907242, 0.04977701976895332, 0.05110762268304825, 0.034919947385787964, 0.053310420364141464, 0.083308644592762, 0.11681797355413437], [0.07272599637508392, 0.05881401151418686, 0.044426098465919495, 0.07101428508758545, 0.10939844697713852, 0.05208270996809006, 0.04137587919831276, 0.0344521626830101, 0.05122208222746849, 0.08955877274274826, 0.039367761462926865, 0.056534480303525925, 0.07179926335811615, 0.026797598227858543, 0.059924229979515076, 0.06815313547849655, 0.05235305055975914], [0.05264585465192795, 0.0524466335773468, 0.12589870393276215, 0.11564656347036362, 0.04524800181388855, 0.0652669221162796, 0.026575203984975815, 0.04012272506952286, 0.034438371658325195, 0.06128692999482155, 0.08746808022260666, 0.054870061576366425, 0.05282682552933693, 0.02610066346824169, 0.05245231091976166, 0.04440777748823166, 0.062298454344272614], [0.044658511877059937, 0.034949492663145065, 0.12274754047393799, 0.09044651687145233, 0.07352441549301147, 0.047214582562446594, 0.05266665667295456, 0.0234721340239048, 0.06963781267404556, 0.06044389680027962, 0.10290268063545227, 0.03993726149201393, 0.05704127624630928, 0.03839555010199547, 0.03745810687541962, 0.062163449823856354, 0.04234011843800545], [0.06205875799059868, 0.03258373588323593, 0.16742034256458282, 0.05329447612166405, 0.06803828477859497, 0.033654384315013885, 0.024351313710212708, 0.013333718292415142, 0.04133697971701622, 0.08768152445554733, 0.0807264894247055, 0.044943030923604965, 0.04407404363155365, 0.035559650510549545, 0.055123649537563324, 0.05612906441092491, 0.09969066083431244], [0.060986269265413284, 0.04356071352958679, 0.04931895434856415, 0.05760672688484192, 0.11914543807506561, 0.05065375566482544, 0.046872805804014206, 0.029522821307182312, 0.06837879866361618, 0.07579740881919861, 0.06379861384630203, 0.058043450117111206, 0.06942591071128845, 0.04119157791137695, 0.058996107429265976, 0.05141817778348923, 0.05528242141008377], [0.027387917041778564, 0.0360020287334919, 0.16343317925930023, 0.06507086753845215, 0.03256683051586151, 0.07797317206859589, 0.04359551519155502, 0.028578074648976326, 0.0412597581744194, 0.06569547206163406, 0.0846182331442833, 0.07521190494298935, 0.04594451189041138, 0.03887626901268959, 0.06918033212423325, 0.06090916693210602, 0.04369671270251274], [0.05775798484683037, 0.0439654216170311, 0.17649929225444794, 0.06863462179899216, 0.041848454624414444, 0.046406518667936325, 0.031671181321144104, 0.026018377393484116, 0.022844886407256126, 0.06464064866304398, 0.10718260705471039, 0.05611633509397507, 0.05369586870074272, 0.01739128679037094, 0.043188925832509995, 0.06658937782049179, 0.07554829120635986], [0.0514645054936409, 0.05540596321225166, 0.09700870513916016, 0.05816665291786194, 0.04478514939546585, 0.07160636782646179, 0.01956777088344097, 0.027720803394913673, 0.03089056722819805, 0.08034292608499527, 0.11550043523311615, 0.04428046941757202, 0.11315104365348816, 0.02530500665307045, 0.0831713154911995, 0.029970211908221245, 0.051662009209394455], [0.058930765837430954, 0.03616516664624214, 0.07250328361988068, 0.07003621011972427, 0.08472890406847, 0.07030022144317627, 0.0354921817779541, 0.025703702121973038, 0.06824184954166412, 0.06229052692651749, 0.08599314093589783, 0.050210658460855484, 0.025187835097312927, 0.036178745329380035, 0.05889810249209404, 0.09008319675922394, 0.06905544549226761], [0.05209294706583023, 0.048060342669487, 0.09273643046617508, 0.07496950775384903, 0.06463237851858139, 0.08293759822845459, 0.023187728598713875, 0.027867566794157028, 0.04420696943998337, 0.07896542549133301, 0.08655327558517456, 0.06783853471279144, 0.042834531515836716, 0.022300198674201965, 0.06135603040456772, 0.08304460346698761, 0.04641595482826233], [0.03896625339984894, 0.0530947707593441, 0.0624099001288414, 0.0796666368842125, 0.044302817434072495, 0.06427424401044846, 0.034527771174907684, 0.025213995948433876, 0.02792493999004364, 0.0851895809173584, 0.14523343741893768, 0.06527266651391983, 0.05962174013257027, 0.03233262151479721, 0.038106635212898254, 0.0662037655711174, 0.07765823602676392], [0.04266800358891487, 0.04854888468980789, 0.043463531881570816, 0.06460607796907425, 0.10598824173212051, 0.07372406125068665, 0.06178578361868858, 0.03152286261320114, 0.04882797598838806, 0.07007457315921783, 0.043068379163742065, 0.0689905509352684, 0.04438987001776695, 0.05123814195394516, 0.05678070709109306, 0.09464948624372482, 0.04967288300395012], [0.047091368585824966, 0.03627440333366394, 0.05558536946773529, 0.03836105763912201, 0.08180876821279526, 0.06129312887787819, 0.09454991668462753, 0.02435026317834854, 0.050947919487953186, 0.0705677717924118, 0.08566901832818985, 0.05799805372953415, 0.03550048545002937, 0.03966129198670387, 0.0384892076253891, 0.10589408874511719, 0.07595793902873993]], [[0.29457584023475647, 0.003627731930464506, 0.00617615832015872, 0.0010261839488521218, 0.27666908502578735, 0.0026771347038447857, 0.004546976648271084, 0.002755562076345086, 0.23536470532417297, 0.012982930988073349, 0.0038075074553489685, 0.00269554671831429, 0.12764087319374084, 0.006070129107683897, 0.0012745083076879382, 0.015047297812998295, 0.0030618845485150814], [0.002056814031675458, 0.05931824445724487, 0.03187933564186096, 0.211695596575737, 0.01736515946686268, 0.12935423851013184, 0.1175031065940857, 0.08375051617622375, 0.016400089487433434, 0.07173699140548706, 0.024023503065109253, 0.044051602482795715, 0.014477108605206013, 0.03348959982395172, 0.10901835560798645, 0.018435362726449966, 0.015444439835846424], [0.0038746751379221678, 0.0186601635068655, 0.01189667172729969, 0.12457318603992462, 0.004833804909139872, 0.09474187344312668, 0.04184018447995186, 0.08912927657365799, 0.007623607292771339, 0.10401652008295059, 0.027561837807297707, 0.2573806941509247, 0.008283075876533985, 0.023977210745215416, 0.15769661962985992, 0.010112485848367214, 0.013798070140182972], [0.005467109382152557, 0.007990436628460884, 0.016196269541978836, 0.08325783908367157, 0.007564082276076078, 0.08164913207292557, 0.04680721089243889, 0.05788324773311615, 0.00835471972823143, 0.1063922792673111, 0.027238665148615837, 0.235885888338089, 0.01584477536380291, 0.016665106639266014, 0.26095443964004517, 0.005456389859318733, 0.0163924191147089], [0.009139214642345905, 0.10117918252944946, 0.04337960481643677, 0.16007636487483978, 0.017613761126995087, 0.10132443159818649, 0.17090006172657013, 0.06023634597659111, 0.017627684399485588, 0.0616193488240242, 0.06035323068499565, 0.04870421066880226, 0.012275940738618374, 0.023465828970074654, 0.06830044835805893, 0.026662534102797508, 0.017141791060566902], [0.010598878376185894, 0.008358865045011044, 0.007520277518779039, 0.15771792829036713, 0.0061492957174777985, 0.027769790962338448, 0.035617396235466, 0.05093085765838623, 0.00440999586135149, 0.1562189906835556, 0.01609227806329727, 0.09171362966299057, 0.0031749503687024117, 0.00929594598710537, 0.39852985739707947, 0.005072424188256264, 0.01082876417785883], [0.0028833160176873207, 0.07023760676383972, 0.030077436938881874, 0.17989741265773773, 0.021253423765301704, 0.08645954728126526, 0.06685880571603775, 0.09576836973428726, 0.012323257513344288, 0.10423743724822998, 0.04068758338689804, 0.1488703340291977, 0.010031689889729023, 0.028695208951830864, 0.06781959533691406, 0.01736520044505596, 0.01653374545276165], [0.003992090467363596, 0.016185337677598, 0.0083212461322546, 0.14559343457221985, 0.004102383274585009, 0.1297518014907837, 0.030392814427614212, 0.051662128418684006, 0.0051368423737585545, 0.18170411884784698, 0.04659644141793251, 0.1833973526954651, 0.006837980356067419, 0.01780976541340351, 0.14393246173858643, 0.0038217457477003336, 0.020761996507644653], [0.017841458320617676, 0.059623803943395615, 0.05600764602422714, 0.13198907673358917, 0.029788324609398842, 0.08120033890008926, 0.13680963218212128, 0.045746974647045135, 0.021958814933896065, 0.08469226211309433, 0.05985196307301521, 0.07983113080263138, 0.024158384650945663, 0.0283843781799078, 0.09017966687679291, 0.030597854405641556, 0.02133830077946186], [0.022123252972960472, 0.023976456373929977, 0.036124397069215775, 0.07433260232210159, 0.028175879269838333, 0.03609968721866608, 0.04730411246418953, 0.10670427232980728, 0.024231906980276108, 0.06913311779499054, 0.024505825713276863, 0.12553390860557556, 0.06009122356772423, 0.08132576942443848, 0.13130038976669312, 0.014213667251169682, 0.09482347220182419], [0.005607731640338898, 0.008883587084710598, 0.0026915466878563166, 0.05243263766169548, 0.004290824756026268, 0.0544794425368309, 0.030261896550655365, 0.026659203693270683, 0.005259776022285223, 0.19069305062294006, 0.010110576637089252, 0.3714807629585266, 0.005653735250234604, 0.010323051363229752, 0.2070106565952301, 0.0039879255928099155, 0.010173525661230087], [0.012885325588285923, 0.015531844459474087, 0.019208313897252083, 0.06471756100654602, 0.006446885410696268, 0.06689199060201645, 0.04504822939634323, 0.08477379381656647, 0.013755885884165764, 0.13900908827781677, 0.018993878737092018, 0.18449655175209045, 0.018618648871779442, 0.035334233194589615, 0.22818514704704285, 0.006972027942538261, 0.03913052752614021], [0.010992341674864292, 0.07479042559862137, 0.03357856348156929, 0.10009703040122986, 0.013631830923259258, 0.07283667474985123, 0.13496443629264832, 0.06712464243173599, 0.01113462820649147, 0.08743559569120407, 0.05260263755917549, 0.06873025000095367, 0.007453628815710545, 0.03388408571481705, 0.11726170778274536, 0.09018457680940628, 0.023296978324651718], [0.00738230487331748, 0.05287284776568413, 0.02748834528028965, 0.13072171807289124, 0.010176889598369598, 0.09104453772306442, 0.08391224592924118, 0.06560830026865005, 0.013354847207665443, 0.08792359381914139, 0.03670091554522514, 0.12332535535097122, 0.0088042626157403, 0.022486677393317223, 0.13316065073013306, 0.05986065790057182, 0.04517587646842003], [0.012695718556642532, 0.026055196300148964, 0.022329291328787804, 0.0450638011097908, 0.010577467270195484, 0.04616229608654976, 0.04701787233352661, 0.05067073926329613, 0.024871688336133957, 0.1816597878932953, 0.051409438252449036, 0.38859912753105164, 0.01676558330655098, 0.02167905680835247, 0.018797725439071655, 0.013953532092273235, 0.02169165574014187], [0.0008386684930883348, 0.1866893768310547, 0.13078397512435913, 0.03159390762448311, 0.029805121943354607, 0.025462638586759567, 0.15396931767463684, 0.0144027816131711, 0.01846894435584545, 0.012513899244368076, 0.05690573528409004, 0.02096882276237011, 0.010582726448774338, 0.02610178478062153, 0.0223299041390419, 0.25093212723731995, 0.00765022961422801], [0.02020050771534443, 0.06334458291530609, 0.08447235077619553, 0.015379440039396286, 0.15486246347427368, 0.021524718031287193, 0.061193086206912994, 0.012539378367364407, 0.10347498953342438, 0.02432064339518547, 0.04243594408035278, 0.010748522356152534, 0.07315993309020996, 0.033932194113731384, 0.00964222103357315, 0.25528743863105774, 0.013481702655553818]], [[0.6981646418571472, 0.017532868310809135, 0.02515825442969799, 0.011747701093554497, 0.021112678572535515, 0.008474407717585564, 0.01647743582725525, 0.010445646941661835, 0.01668182946741581, 0.006616213358938694, 0.021473603323101997, 0.011342601850628853, 0.03163883835077286, 0.017641965299844742, 0.014207329601049423, 0.026977982372045517, 0.044306088238954544], [0.743219792842865, 0.046988725662231445, 0.07547420263290405, 0.014134873636066914, 0.021618137136101723, 0.006666907109320164, 0.015244631096720695, 0.00360606936737895, 0.00689518079161644, 0.001770606031641364, 0.011878532357513905, 0.007537756115198135, 0.0010100464569404721, 0.001865018275566399, 0.010379449464380741, 0.007081437390297651, 0.02462853491306305], [0.5618336796760559, 0.23118290305137634, 0.05389007553458214, 0.005549961235374212, 0.02720073238015175, 0.0029118580278009176, 0.014177513308823109, 0.004730903543531895, 0.001608863822184503, 0.0016424180939793587, 0.003812688635662198, 0.00589982932433486, 0.04124623164534569, 0.0025717774406075478, 0.0011364860692992806, 0.023988336324691772, 0.016615601256489754], [0.24665851891040802, 0.02425065077841282, 0.6186596751213074, 0.011464553885161877, 0.014050992205739021, 0.006904889363795519, 0.004195770714432001, 0.0051613119430840015, 0.006043924018740654, 8.216913556680083e-05, 0.0010108398273587227, 0.001133922254666686, 0.0022016428411006927, 0.0034354086965322495, 0.001155381789430976, 0.007974069565534592, 0.04561637341976166], [0.058185528963804245, 0.03266623616218567, 0.08290761709213257, 0.6362035870552063, 0.04153342545032501, 0.010160445235669613, 0.024082010611891747, 0.002802683971822262, 0.05398959666490555, 0.0020444763358682394, 0.000531044031959027, 0.0016400441527366638, 0.0029571247287094593, 0.0028468607924878597, 0.024667948484420776, 0.004405901301652193, 0.018375514075160027], [0.106356680393219, 0.01575089991092682, 0.0256501454859972, 0.023008134216070175, 0.7605218291282654, 0.008339278399944305, 0.012814763002097607, 0.016660498455166817, 0.007954075932502747, 0.003976548556238413, 0.005010015331208706, 9.242049418389797e-05, 0.00048307646648027003, 0.0006139388424344361, 0.002496210392564535, 0.009025774896144867, 0.0012457683915272355], [0.3990272283554077, 0.011024841107428074, 0.018008319661021233, 0.008825073018670082, 0.10585073381662369, 0.23410636186599731, 0.06593618541955948, 0.03982527181506157, 0.03595086187124252, 0.002401669742539525, 0.029575955122709274, 0.0015361907426267862, 0.00034144928213208914, 0.0006710857851430774, 0.002313911681994796, 0.01147629413753748, 0.03312861919403076], [0.02276361919939518, 0.02942824736237526, 0.00771111948415637, 0.006385409738868475, 0.03148825466632843, 0.04751600697636604, 0.758979320526123, 0.02367834374308586, 0.023261262103915215, 0.004415938165038824, 0.02129259705543518, 0.0033432114869356155, 0.0031042639166116714, 0.00020125923037994653, 0.0051873051561415195, 0.003158487845212221, 0.008085339330136776], [0.0451827310025692, 0.0060783508233726025, 0.009466823190450668, 0.0027949402574449778, 0.024109914898872375, 0.006771950051188469, 0.13470622897148132, 0.6424131393432617, 0.0816274881362915, 0.006352249067276716, 0.019605666399002075, 0.002419887576252222, 0.008261087350547314, 0.0014702962944284081, 0.00035362940980121493, 0.003217697609215975, 0.005167857278138399], [0.13024534285068512, 0.0006528122466988862, 0.0027962629683315754, 0.007059287745505571, 0.006864625029265881, 0.007990780286490917, 0.026330862194299698, 0.15840338170528412, 0.5981725454330444, 0.008382939733564854, 0.030433496460318565, 0.011673147790133953, 0.003414490260183811, 0.0035967533476650715, 0.0005762578221037984, 0.0003518436860758811, 0.0030550991650670767], [0.397002637386322, 0.004386902321130037, 0.0008940395782701671, 0.0013608788140118122, 0.014978621155023575, 0.0023901849053800106, 0.013145443983376026, 0.024187956005334854, 0.0726446807384491, 0.2098972499370575, 0.057409148663282394, 0.005679212510585785, 0.1553308665752411, 0.023164235055446625, 0.007766354363411665, 0.008352799341082573, 0.0014086647424846888], [0.17123720049858093, 0.006619974039494991, 0.0038048699498176575, 0.00040024062036536634, 0.007854538038372993, 0.0028457168955355883, 0.0035384646616876125, 0.006226320285350084, 0.008929059840738773, 0.029295645654201508, 0.6858198046684265, 0.030245959758758545, 0.008804689161479473, 0.009216874837875366, 0.006260135676711798, 0.016587479040026665, 0.0023130110930651426], [0.018574148416519165, 0.006898283492773771, 0.012527020648121834, 0.0018294466426596045, 0.0006088177906349301, 0.001040903152897954, 0.005291468929499388, 0.005561737809330225, 0.012987921014428139, 0.0058511230163276196, 0.03289174288511276, 0.7656614184379578, 0.03584258630871773, 0.03673243895173073, 0.01766328513622284, 0.010965516790747643, 0.029072148725390434], [0.007601868361234665, 0.0018443099688738585, 0.001479113008826971, 0.00019323822925798595, 0.00023222093295771629, 3.372315768501721e-05, 0.0001567626022733748, 0.0004469529667403549, 0.0011709548998624086, 0.001066762488335371, 0.0018628158140927553, 0.0034696150105446577, 0.9211670756340027, 0.04542645812034607, 0.0010545032564550638, 0.009366531856358051, 0.003427111776545644], [0.4860518276691437, 0.004630083218216896, 0.027307290583848953, 0.001770538161508739, 0.005590984597802162, 0.0008004836272448301, 0.00032046056003309786, 0.0049631232395768166, 0.007840623147785664, 0.003965197131037712, 0.03400776535272598, 0.05107063427567482, 0.10046487301588058, 0.11817830801010132, 0.035498861223459244, 0.052459076046943665, 0.0650799423456192], [0.09877566248178482, 0.0038432697765529156, 0.005425885785371065, 0.006688122171908617, 0.005964457523077726, 0.0009697769419290125, 0.0009907787898555398, 0.000439800089225173, 0.004264394752681255, 0.006452901754528284, 0.004362103063613176, 0.005959067493677139, 0.09817367792129517, 0.23738066852092743, 0.298074334859848, 0.15234777331352234, 0.06988733261823654], [0.27389881014823914, 0.007820652797818184, 0.0032323419582098722, 0.0002451504406053573, 0.003887212835252285, 0.0010435415897518396, 0.0010374835692346096, 0.00043369538616389036, 0.00022426736541092396, 0.0006224105600267649, 0.013201112858951092, 0.002147105522453785, 0.014154828153550625, 0.0051426333375275135, 0.04183376207947731, 0.5042781233787537, 0.12679696083068848]], [[0.6095179915428162, 0.033848267048597336, 0.02666194923222065, 0.02015094831585884, 0.029490862041711807, 0.01462053693830967, 0.02132117934525013, 0.024882351979613304, 0.018006157130002975, 0.037560537457466125, 0.029475564137101173, 0.022273823618888855, 0.01792338117957115, 0.02019413188099861, 0.010060019791126251, 0.020841870456933975, 0.0431705042719841], [0.300358384847641, 0.1910104751586914, 0.08971308916807175, 0.1432417333126068, 0.0548425130546093, 0.023374661803245544, 0.017754990607500076, 0.004790300037711859, 0.01844322867691517, 0.01781892031431198, 0.022536171600222588, 0.007201068568974733, 0.008182727731764317, 0.03238292783498764, 0.023403212428092957, 0.025323623791337013, 0.019621992483735085], [0.0983300656080246, 0.5228375196456909, 0.18831881880760193, 0.0747106671333313, 0.02196674793958664, 0.009508688002824783, 0.008893896825611591, 0.0036608967930078506, 0.002787283156067133, 0.008092561736702919, 0.01006473507732153, 0.010022943839430809, 0.004950097296386957, 0.00296883936971426, 0.005383281968533993, 0.018165552988648415, 0.009337428025901318], [0.06665215641260147, 0.38390636444091797, 0.33358848094940186, 0.07670566439628601, 0.042726390063762665, 0.027636973187327385, 0.006791647057980299, 0.0074564749374985695, 0.0018224639352411032, 0.0010415030410513282, 0.004739062394946814, 0.0017280926695093513, 0.0010366354836151004, 0.002621916588395834, 0.0010116924531757832, 0.025630053132772446, 0.01490440871566534], [0.0003601640637498349, 0.01669863425195217, 0.037370581179857254, 0.9312489628791809, 0.0030021443963050842, 0.0047836615704, 0.0012536576250568032, 0.0009321000543422997, 0.0008163574384525418, 0.000506165437400341, 7.88886463851668e-05, 0.0003140818153042346, 0.00034381140721961856, 0.0002568929339759052, 0.0008445307612419128, 0.00042399021913297474, 0.0007653085049241781], [0.0025076738093048334, 0.006355005782097578, 0.016251780092716217, 0.793350338935852, 0.16255725920200348, 0.008314080536365509, 0.00247995276004076, 0.0035596222151070833, 0.0006026549381203949, 0.00110574287828058, 0.0004614660865627229, 0.00010098978964379057, 0.00010997541539836675, 0.00026242839521728456, 0.0003019747673533857, 0.0012852346990257502, 0.0003938179579563439], [0.004395531956106424, 0.01092948205769062, 0.01924915611743927, 0.09978891164064407, 0.14385658502578735, 0.6395168900489807, 0.05413706600666046, 0.009034610353410244, 0.004596170037984848, 0.002334059914574027, 0.0037204772233963013, 0.0008860807283781469, 0.00024292727175634354, 0.0007560233352705836, 0.0010521074291318655, 0.004061451181769371, 0.001442443230189383], [0.0005633618566207588, 0.004056201316416264, 0.0036959967110306025, 0.013823693618178368, 0.014390992000699043, 0.5209199786186218, 0.4031563997268677, 0.013200167566537857, 0.016560399904847145, 0.0016621409449726343, 0.001582384924404323, 0.0016871931729838252, 0.0004727114283014089, 0.0004075246979482472, 0.0007731044315733016, 0.0017792724538594484, 0.001268445630557835], [0.001898949733003974, 0.00952473096549511, 0.007496705278754234, 0.008701283484697342, 0.018916185945272446, 0.16734585165977478, 0.5714948177337646, 0.1588928997516632, 0.02313939295709133, 0.012212037108838558, 0.005942282732576132, 0.003736619371920824, 0.004253944382071495, 0.0014276745496317744, 0.0006454708054661751, 0.0028664933051913977, 0.001504692598246038], [0.04078364744782448, 0.0029030428268015385, 0.006877072621136904, 0.020722754299640656, 0.008620087057352066, 0.06541537493467331, 0.18097272515296936, 0.263065904378891, 0.3060530424118042, 0.06048516184091568, 0.017289919778704643, 0.012707325629889965, 0.0022925755474716425, 0.005750555079430342, 0.001991923898458481, 0.001368250697851181, 0.0027006445452570915], [0.03128692880272865, 0.002367935376241803, 0.016335677355527878, 0.013359267264604568, 0.006891955155879259, 0.011816994287073612, 0.021818500012159348, 0.062243182212114334, 0.21260851621627808, 0.4842340350151062, 0.1085713729262352, 0.007550774607807398, 0.00906718336045742, 0.003478087717667222, 0.001493234420195222, 0.005987126380205154, 0.0008891951874829829], [0.016566341742873192, 0.0008439947268925607, 0.0007339381845667958, 0.0012966007925570011, 0.0034405372571200132, 0.009938591159880161, 0.0049998401664197445, 0.009881018660962582, 0.03883107006549835, 0.19331921637058258, 0.6661722660064697, 0.030333636328577995, 0.0068085165694355965, 0.003833927446976304, 0.0014827705454081297, 0.010783662088215351, 0.0007339325966313481], [0.0018408496398478746, 0.0020773885771632195, 0.0024408712051808834, 0.0024280184879899025, 0.0013236489612609148, 0.01004121359437704, 0.02576003596186638, 0.010759763419628143, 0.020512264221906662, 0.056503456085920334, 0.2669135332107544, 0.4497784972190857, 0.12443725764751434, 0.01243523508310318, 0.0024720081128180027, 0.004003015346825123, 0.006273000035434961], [0.001346359378658235, 0.002645148430019617, 0.00031380250584334135, 0.0006513993721455336, 0.0010452968999743462, 0.0019809056539088488, 0.007841495797038078, 0.011361343786120415, 0.01081921998411417, 0.03519783169031143, 0.028326358646154404, 0.8413611650466919, 0.021771766245365143, 0.005846907384693623, 0.017481788992881775, 0.008134804666042328, 0.0038743987679481506], [0.0472383052110672, 0.0010948795825242996, 0.0027835164219141006, 0.0020345773082226515, 0.0009380871779285371, 0.0019082458456978202, 0.0006998960743658245, 0.00754022691398859, 0.0164000503718853, 0.016982970759272575, 0.047624554485082626, 0.08625802397727966, 0.3916708528995514, 0.316429078578949, 0.029013510793447495, 0.023127209395170212, 0.008256020024418831], [0.0012728635920211673, 0.002062007552012801, 0.0013791770907118917, 0.0064827073365449905, 0.002220867434516549, 0.0011999655980616808, 0.003637714544311166, 0.0017992303473874927, 0.012128143571317196, 0.024097967892885208, 0.008868847042322159, 0.04021504148840904, 0.15253278613090515, 0.23546327650547028, 0.41227707266807556, 0.06914027780294418, 0.025222092866897583], [0.1294708400964737, 0.007246014196425676, 0.004714522510766983, 0.002335128141567111, 0.0035983745474368334, 0.0026396987959742546, 0.0009173242142423987, 0.0006152213318273425, 0.0008882706169970334, 0.007077471818774939, 0.013188973069190979, 0.004149242769926786, 0.012255246751010418, 0.10204090923070908, 0.21213997900485992, 0.4234587550163269, 0.07326406240463257]], [[0.5106123685836792, 0.0343390554189682, 0.03299872204661369, 0.03329678997397423, 0.03024992346763611, 0.03570824861526489, 0.01878446340560913, 0.035144053399562836, 0.017931796610355377, 0.03697741776704788, 0.016949601471424103, 0.02918037213385105, 0.04620984196662903, 0.020001288503408432, 0.028305070474743843, 0.025426644831895828, 0.04788432642817497], [0.043345630168914795, 0.05810007452964783, 0.026114869862794876, 0.11872027069330215, 0.015282434411346912, 0.10349524021148682, 0.07708796113729477, 0.07912831753492355, 0.021529074758291245, 0.06547169387340546, 0.033743541687726974, 0.05692742019891739, 0.09107817709445953, 0.06978460401296616, 0.04087148606777191, 0.06127748265862465, 0.038041748106479645], [0.03439297527074814, 0.07056642323732376, 0.010636383667588234, 0.2039961814880371, 0.010993517935276031, 0.08490883558988571, 0.11702527850866318, 0.07878046482801437, 0.02309763804078102, 0.049507126212120056, 0.031230861321091652, 0.03652365133166313, 0.04887809231877327, 0.05831743776798248, 0.044403623789548874, 0.05873538926243782, 0.03800603374838829], [0.2698756754398346, 0.04902661219239235, 0.030721403658390045, 0.03595681115984917, 0.01684589684009552, 0.055038999766111374, 0.028725668787956238, 0.07084697484970093, 0.028334075585007668, 0.10903869569301605, 0.04113535210490227, 0.059824906289577484, 0.042311325669288635, 0.024501631036400795, 0.04588811844587326, 0.047335803508758545, 0.044592034071683884], [0.04341587424278259, 0.05772056803107262, 0.02399502880871296, 0.08451473712921143, 0.046166494488716125, 0.06869754940271378, 0.08880815654993057, 0.06093619018793106, 0.048023197799921036, 0.05958857014775276, 0.045326244086027145, 0.05202973634004593, 0.06334182620048523, 0.04878496751189232, 0.06662832200527191, 0.062369175255298615, 0.0796533077955246], [0.16111992299556732, 0.05331747233867645, 0.03352297097444534, 0.11892972886562347, 0.03274722397327423, 0.028544582426548004, 0.06176907941699028, 0.058300167322158813, 0.06290493160486221, 0.05607999861240387, 0.035651031881570816, 0.06201845407485962, 0.02487761527299881, 0.04153115302324295, 0.06584954261779785, 0.07345853000879288, 0.029377644881606102], [0.013078388758003712, 0.08941976726055145, 0.02166817896068096, 0.07494733482599258, 0.0418592132627964, 0.04408188536763191, 0.08855380117893219, 0.06361407786607742, 0.05728723108768463, 0.06930068135261536, 0.0884874239563942, 0.04149995744228363, 0.02055768482387066, 0.04444774612784386, 0.11139383167028427, 0.07273712754249573, 0.05706563591957092], [0.10773666948080063, 0.04040073603391647, 0.01241106167435646, 0.07387170195579529, 0.020089488476514816, 0.09322803467512131, 0.10772567242383957, 0.03152098134160042, 0.027430403977632523, 0.06807353347539902, 0.024945013225078583, 0.061256371438503265, 0.05724923312664032, 0.06529974937438965, 0.0572073869407177, 0.11358832567930222, 0.03796562924981117], [0.04520883038640022, 0.050408732146024704, 0.03524542972445488, 0.03842662647366524, 0.059153515845537186, 0.06779550015926361, 0.07169337570667267, 0.09376968443393707, 0.05698185786604881, 0.06154352053999901, 0.06284598261117935, 0.06749647110700607, 0.04173070564866066, 0.032615840435028076, 0.08016731590032578, 0.07503456622362137, 0.05988215282559395], [0.41193127632141113, 0.018110234290361404, 0.028078027069568634, 0.03488859906792641, 0.015307269990444183, 0.02133905328810215, 0.040430016815662384, 0.0645824745297432, 0.04591675475239754, 0.027514133602380753, 0.026146532967686653, 0.030418075621128082, 0.008004904724657536, 0.023096956312656403, 0.06015728786587715, 0.06029234826564789, 0.0837860032916069], [0.05842137336730957, 0.07064276933670044, 0.018139807507395744, 0.15832780301570892, 0.024250412359833717, 0.07659993320703506, 0.11219293624162674, 0.08175171911716461, 0.03225475549697876, 0.06979995965957642, 0.031108513474464417, 0.03160635754466057, 0.0449267253279686, 0.060439225286245346, 0.044529397040605545, 0.046270184218883514, 0.038738083094358444], [0.09876051545143127, 0.020123884081840515, 0.031848788261413574, 0.09721429646015167, 0.010213497094810009, 0.08794964104890823, 0.07877292484045029, 0.14904463291168213, 0.018623562529683113, 0.045155517756938934, 0.04141278564929962, 0.008869609795510769, 0.037262819707393646, 0.11768042296171188, 0.06954445689916611, 0.04162031412124634, 0.045902401208877563], [0.06511834263801575, 0.06423753499984741, 0.035320017486810684, 0.060128748416900635, 0.02854452282190323, 0.08097797632217407, 0.05587947741150856, 0.08999010175466537, 0.032405056059360504, 0.06406847387552261, 0.041012659668922424, 0.055922746658325195, 0.07420828193426132, 0.06991880387067795, 0.0502328984439373, 0.06419118493795395, 0.06784316897392273], [0.02744654007256031, 0.08072438091039658, 0.037641141563653946, 0.07387154549360275, 0.05488560348749161, 0.046270325779914856, 0.0933903306722641, 0.03768577426671982, 0.05144555866718292, 0.06979628652334213, 0.07933833450078964, 0.07703371345996857, 0.03447379916906357, 0.04035176336765289, 0.08558652549982071, 0.04911292716860771, 0.060945458710193634], [0.22935988008975983, 0.031191561371088028, 0.053513411432504654, 0.060256604105234146, 0.015326060354709625, 0.04702044650912285, 0.03751911595463753, 0.07349270582199097, 0.04510248079895973, 0.07432780414819717, 0.04624123126268387, 0.11470093578100204, 0.023976268246769905, 0.055349402129650116, 0.016223996877670288, 0.05299048498272896, 0.02340776100754738], [0.025984017178416252, 0.05068673938512802, 0.029490966349840164, 0.11159612238407135, 0.026731353253126144, 0.08411295711994171, 0.06656941026449203, 0.07346147298812866, 0.03709869459271431, 0.05950511246919632, 0.046784061938524246, 0.07655828446149826, 0.052201706916093826, 0.050458215177059174, 0.06365986913442612, 0.05218794196844101, 0.09291308373212814], [0.04739044979214668, 0.017780177295207977, 0.008561485446989536, 0.03431307151913643, 0.010713869705796242, 0.01945509761571884, 0.01968236453831196, 0.014571063220500946, 0.011402755975723267, 0.011895307339727879, 0.01683126948773861, 0.0031400343868881464, 0.01500327792018652, 0.0063332668505609035, 0.004146274644881487, 0.005089582875370979, 0.7536906003952026]], [[0.027245422825217247, 0.009652664884924889, 0.019773537293076515, 0.0034909993410110474, 0.005776853766292334, 0.0028757955878973007, 0.018553411588072777, 0.003064687829464674, 0.004121046978980303, 0.0030350901652127504, 0.009411784820258617, 0.010976319201290607, 0.005356391426175833, 0.008505566976964474, 0.003282791469246149, 0.8505065441131592, 0.014371123164892197], [0.007139197085052729, 0.05842677876353264, 0.01636257953941822, 0.30022478103637695, 0.0332835353910923, 0.06560911238193512, 0.06403123587369919, 0.10644181817770004, 0.026356475427746773, 0.05177002400159836, 0.03630815073847771, 0.04909592494368553, 0.04166300967335701, 0.01937471330165863, 0.02708474174141884, 0.03187069296836853, 0.06495725363492966], [0.07241269946098328, 0.04854238033294678, 0.03905031457543373, 0.06077088788151741, 0.06812848895788193, 0.026454703882336617, 0.02062210626900196, 0.051576606929302216, 0.13908691704273224, 0.14967991411685944, 0.06996451318264008, 0.033937785774469376, 0.0878283753991127, 0.04587830230593681, 0.01716502010822296, 0.010422666557133198, 0.05847831070423126], [0.025338008999824524, 0.04873888939619064, 0.02770266868174076, 0.14909838140010834, 0.03795381262898445, 0.1383567452430725, 0.038006942719221115, 0.10958662629127502, 0.042737215757369995, 0.08592811971902847, 0.024029238149523735, 0.061002060770988464, 0.01994210295379162, 0.030588120222091675, 0.09197111427783966, 0.01585681363940239, 0.053163133561611176], [0.02740716002881527, 0.07848784327507019, 0.020125292241573334, 0.3334597051143646, 0.01060887798666954, 0.11717429012060165, 0.06894893944263458, 0.08837439864873886, 0.014497978612780571, 0.017384396865963936, 0.02400857023894787, 0.037268348038196564, 0.023684227839112282, 0.012433912605047226, 0.03684786334633827, 0.03969566151499748, 0.04959248751401901], [0.018748415634036064, 0.02631635218858719, 0.01595119759440422, 0.24896784126758575, 0.01373999658972025, 0.04265798255801201, 0.026486216112971306, 0.1280650645494461, 0.017709393054246902, 0.03716505318880081, 0.008461049757897854, 0.012929067946970463, 0.012891830876469612, 0.015129907988011837, 0.3469735085964203, 0.01278013363480568, 0.015026976354420185], [0.024317525327205658, 0.11530566960573196, 0.01826501451432705, 0.21210558712482452, 0.01990969106554985, 0.046474725008010864, 0.06298547983169556, 0.1605076640844345, 0.0404600091278553, 0.07395518571138382, 0.015871750190854073, 0.016528939828276634, 0.013839434832334518, 0.024547120556235313, 0.10726357996463776, 0.02324565127491951, 0.0244169682264328], [0.006123150233179331, 0.05072992667555809, 0.0030820698011666536, 0.2416767030954361, 0.024167519062757492, 0.15025828778743744, 0.06546127051115036, 0.0627114474773407, 0.01937922276556492, 0.08276720345020294, 0.0191244687885046, 0.0330677404999733, 0.019504263997077942, 0.0036090307403355837, 0.16122853755950928, 0.020328974351286888, 0.036780182272195816], [0.10739029198884964, 0.06544465571641922, 0.028070515021681786, 0.10164906084537506, 0.024267535656690598, 0.05753900855779648, 0.15114779770374298, 0.06120315566658974, 0.018382478505373, 0.0276706013828516, 0.048554934561252594, 0.07635796815156937, 0.018390869721770287, 0.019755924120545387, 0.02999069169163704, 0.07400660961866379, 0.09017796814441681], [0.030173422768712044, 0.053881604224443436, 0.013534785248339176, 0.08658469468355179, 0.021206196397542953, 0.07312166690826416, 0.1279664784669876, 0.08197761327028275, 0.020685335621237755, 0.06545647978782654, 0.01886621303856373, 0.1106625571846962, 0.037991538643836975, 0.017151135951280594, 0.04910702630877495, 0.05815785378217697, 0.1334753930568695], [0.09564624726772308, 0.04178570210933685, 0.027252940461039543, 0.05303410068154335, 0.03156319633126259, 0.03099931590259075, 0.02472941018640995, 0.04680508375167847, 0.07547566294670105, 0.25672054290771484, 0.026277001947164536, 0.08303015679121017, 0.038403742015361786, 0.04498806968331337, 0.04343334957957268, 0.014224211685359478, 0.06563124805688858], [0.017575260251760483, 0.029759161174297333, 0.005513886921107769, 0.08486614376306534, 0.021370677277445793, 0.08849819004535675, 0.04027296602725983, 0.09057433903217316, 0.02073938027024269, 0.18647170066833496, 0.03768850117921829, 0.14150957763195038, 0.015262497588992119, 0.00810402911156416, 0.08584599196910858, 0.013933577574789524, 0.11201417446136475], [0.20857228338718414, 0.02617901749908924, 0.028042422607541084, 0.0714377611875534, 0.03597474470734596, 0.035515427589416504, 0.03724202513694763, 0.058901555836200714, 0.0655384287238121, 0.11267702281475067, 0.07095120847225189, 0.0525100901722908, 0.012408148497343063, 0.02986113540828228, 0.03531016781926155, 0.054539624601602554, 0.06433891505002975], [0.05912736430764198, 0.025943264365196228, 0.02975938655436039, 0.02471187710762024, 0.04471781104803085, 0.023753106594085693, 0.02027692086994648, 0.042416203767061234, 0.1145978793501854, 0.15211686491966248, 0.041244927793741226, 0.10069666802883148, 0.06725933402776718, 0.02625381574034691, 0.009166743606328964, 0.029036153107881546, 0.18892167508602142], [0.04748935624957085, 0.04888178035616875, 0.02543170377612114, 0.019401265308260918, 0.026573684066534042, 0.1458994448184967, 0.05913887917995453, 0.027143310755491257, 0.0259183868765831, 0.13487929105758667, 0.06328412890434265, 0.0782892033457756, 0.020502524450421333, 0.01990128867328167, 0.09101128578186035, 0.02140221744775772, 0.1448521912097931], [0.09679004549980164, 0.022587180137634277, 0.050536803901195526, 0.034339919686317444, 0.030113518238067627, 0.023773889988660812, 0.02895384654402733, 0.026247048750519753, 0.03368492051959038, 0.03296772763133049, 0.034373749047517776, 0.06708604097366333, 0.01757235638797283, 0.036698076874017715, 0.020637039095163345, 0.38564276695251465, 0.057994965463876724], [0.18190215528011322, 0.021957412362098694, 0.03663105145096779, 0.018070615828037262, 0.030422136187553406, 0.012921164743602276, 0.02626078389585018, 0.011635229922831059, 0.04215249791741371, 0.025525055825710297, 0.029096411541104317, 0.057484835386276245, 0.009278490208089352, 0.02831307053565979, 0.013276591897010803, 0.4027590751647949, 0.052313413470983505]], [[0.6808972358703613, 0.02303452230989933, 0.0048103067092597485, 0.007378086913377047, 0.06036517024040222, 0.0062837000004947186, 0.03632303699851036, 0.0053563350811600685, 0.006563545670360327, 0.014235997572541237, 0.016257504001259804, 0.009321448393166065, 0.008754882961511612, 0.07340975850820541, 0.005885816179215908, 0.032737720757722855, 0.008385008201003075], [0.03165728971362114, 0.04870730638504028, 0.06157243251800537, 0.11710342764854431, 0.01409297063946724, 0.16734659671783447, 0.02792307920753956, 0.06761620938777924, 0.02832932025194168, 0.10213813930749893, 0.0461818091571331, 0.07944958657026291, 0.03835873678326607, 0.02256450243294239, 0.08888854831457138, 0.024286087602376938, 0.033784035593271255], [0.020634006708860397, 0.04508265107870102, 0.1075766384601593, 0.12789003551006317, 0.0051635936833918095, 0.0959831103682518, 0.027152901515364647, 0.06269185245037079, 0.005249459762126207, 0.06237063556909561, 0.18838602304458618, 0.09163869917392731, 0.024424578994512558, 0.0366106852889061, 0.04332117736339569, 0.02538222260773182, 0.03044172003865242], [0.06311099976301193, 0.04634597897529602, 0.05179616063833237, 0.12267306447029114, 0.01741677336394787, 0.11959631741046906, 0.0240617822855711, 0.08119269460439682, 0.017764870077371597, 0.11225082725286484, 0.02082524262368679, 0.09084486961364746, 0.03250780701637268, 0.029709763824939728, 0.1038203239440918, 0.02307511493563652, 0.04300736263394356], [0.08853810280561447, 0.06369393318891525, 0.03858011215925217, 0.0734643042087555, 0.0716438814997673, 0.08580296486616135, 0.04965078830718994, 0.051593679934740067, 0.05396497622132301, 0.08444810658693314, 0.03797284886240959, 0.0608951561152935, 0.03721179813146591, 0.05193696171045303, 0.05448503419756889, 0.04000372439622879, 0.05611352622509003], [0.051742300391197205, 0.06375698000192642, 0.11949510127305984, 0.11491476744413376, 0.03230704367160797, 0.12636180222034454, 0.03297262266278267, 0.06111579388380051, 0.014400439336895943, 0.10092483460903168, 0.04627049341797829, 0.06932331621646881, 0.025301678106188774, 0.03387904167175293, 0.06962928175926208, 0.01248360238969326, 0.025120850652456284], [0.017336783930659294, 0.14596058428287506, 0.07667501270771027, 0.1484251618385315, 0.03318384289741516, 0.12449251115322113, 0.05067687854170799, 0.08565463870763779, 0.022138627246022224, 0.06166972219944, 0.031119907274842262, 0.07830923050642014, 0.013897757977247238, 0.017898762598633766, 0.053925298154354095, 0.01564982160925865, 0.02298543229699135], [0.033145468682050705, 0.07064005732536316, 0.05579521507024765, 0.0792713463306427, 0.0417402908205986, 0.11400794982910156, 0.0445113480091095, 0.06265442073345184, 0.030926210805773735, 0.06975185871124268, 0.03495069220662117, 0.10834810882806778, 0.045095425099134445, 0.05294658616185188, 0.08554033190011978, 0.047723449766635895, 0.022951209917664528], [0.041073769330978394, 0.056090544909238815, 0.05970007926225662, 0.0591801181435585, 0.08787253499031067, 0.0754726380109787, 0.0393814817070961, 0.05220937728881836, 0.13477586209774017, 0.06947183609008789, 0.0381026454269886, 0.0726870521903038, 0.03133830800652504, 0.036014895886182785, 0.05325615033507347, 0.03232542797923088, 0.06104721501469612], [0.18340782821178436, 0.062499143183231354, 0.033869240432977676, 0.10079173743724823, 0.041114356368780136, 0.1261482685804367, 0.026711691170930862, 0.02428399585187435, 0.008982867002487183, 0.08212747424840927, 0.01926242746412754, 0.09043843299150467, 0.010507299564778805, 0.08830158412456512, 0.07816650718450546, 0.011477765627205372, 0.011909328401088715], [0.02713599242269993, 0.0763126090168953, 0.15547412633895874, 0.1075967401266098, 0.017560690641403198, 0.07968809455633163, 0.039396174252033234, 0.044230397790670395, 0.007827543653547764, 0.05660819634795189, 0.07023848593235016, 0.14559224247932434, 0.01611635833978653, 0.02364197000861168, 0.06898224353790283, 0.01904396526515484, 0.044554151594638824], [0.03591451421380043, 0.08405455946922302, 0.07375317066907883, 0.09386659413576126, 0.03315418213605881, 0.09665559977293015, 0.04932751879096031, 0.03412634879350662, 0.009452035650610924, 0.04696030542254448, 0.0753072127699852, 0.2122953087091446, 0.011302785947918892, 0.019952965900301933, 0.09469534456729889, 0.008773792535066605, 0.02040768787264824], [0.021764028817415237, 0.04754798859357834, 0.061505455523729324, 0.046631112694740295, 0.02607930265367031, 0.07688483595848083, 0.02617909014225006, 0.07395435124635696, 0.026766065508127213, 0.09588088095188141, 0.04692777246236801, 0.06113690510392189, 0.22104771435260773, 0.06925967335700989, 0.03956495597958565, 0.03150678053498268, 0.027363071218132973], [0.03664644807577133, 0.05749697610735893, 0.09781962633132935, 0.11739431321620941, 0.027092793956398964, 0.09792878478765488, 0.02529994212090969, 0.08744780719280243, 0.047355666756629944, 0.07243123650550842, 0.049279212951660156, 0.08996787667274475, 0.03696748614311218, 0.03526579216122627, 0.05480954796075821, 0.026746246963739395, 0.040050212293863297], [0.037938203662633896, 0.03840680792927742, 0.07261300086975098, 0.1355466991662979, 0.010900803841650486, 0.07454685866832733, 0.030478045344352722, 0.0590989850461483, 0.009292845614254475, 0.06488724797964096, 0.04004501551389694, 0.17888964712619781, 0.01916460320353508, 0.014949051663279533, 0.17887945473194122, 0.011538081802427769, 0.022824585437774658], [0.01858188770711422, 0.0810588002204895, 0.1551932841539383, 0.06204324960708618, 0.03824789077043533, 0.04408992454409599, 0.06849044561386108, 0.0719783678650856, 0.03231367468833923, 0.06567485630512238, 0.10928002744913101, 0.04528854414820671, 0.054420262575149536, 0.0285357479006052, 0.03359611704945564, 0.0468820258975029, 0.044324878603219986], [0.05370408296585083, 0.04899904131889343, 0.128506138920784, 0.036063238978385925, 0.025709904730319977, 0.03261617198586464, 0.0361577533185482, 0.09792215377092361, 0.054795846343040466, 0.05550818890333176, 0.06395389139652252, 0.039506033062934875, 0.05164289101958275, 0.02029506303369999, 0.026831241324543953, 0.11153098940849304, 0.11625754833221436]], [[0.5219010710716248, 0.015685513615608215, 0.030501944944262505, 0.006844689603894949, 0.01735885627567768, 0.008699242025613785, 0.0006905119516886771, 0.011820762418210506, 0.02500423975288868, 0.009461328387260437, 0.0025287815369665623, 0.00604661600664258, 0.041752103716135025, 0.005021998658776283, 0.006988897919654846, 0.04879109561443329, 0.24090220034122467], [0.31844037771224976, 0.04411846771836281, 0.05656815320253372, 0.021898582577705383, 0.03348499909043312, 0.0336831733584404, 0.04189567640423775, 0.034760888665914536, 0.04734227433800697, 0.03267835080623627, 0.03763609007000923, 0.0144393565133214, 0.04047499969601631, 0.04302544146776199, 0.027119163423776627, 0.0017156056128442287, 0.1707184910774231], [0.10861588269472122, 0.05482431501150131, 0.1529117226600647, 0.032656993716955185, 0.035015810281038284, 0.025514887645840645, 0.04537046328186989, 0.03744431957602501, 0.05817858502268791, 0.031226275488734245, 0.06264421343803406, 0.03311933949589729, 0.01670723222196102, 0.038671087473630905, 0.029511943459510803, 0.0007485223468393087, 0.2368384301662445], [0.11985556036233902, 0.09082277119159698, 0.1125139370560646, 0.04694903641939163, 0.02158772572875023, 0.09959063678979874, 0.04011088237166405, 0.07445371896028519, 0.01932588405907154, 0.03909192606806755, 0.04527682811021805, 0.05973982438445091, 0.03881366178393364, 0.03638797253370285, 0.09554339200258255, 0.0022785793989896774, 0.0576576367020607], [0.22936208546161652, 0.03397461771965027, 0.07394571602344513, 0.018517250195145607, 0.021348580718040466, 0.02351047471165657, 0.038543786853551865, 0.0324644036591053, 0.14437969028949738, 0.021797413006424904, 0.07701782882213593, 0.010636253282427788, 0.029891645535826683, 0.06768979877233505, 0.018326038494706154, 0.002235394436866045, 0.15635904669761658], [0.09866377711296082, 0.05252418294548988, 0.0639881119132042, 0.08884220570325851, 0.024492142722010612, 0.07423693686723709, 0.047958243638277054, 0.10858552157878876, 0.04253662750124931, 0.08647001534700394, 0.05542140081524849, 0.048013754189014435, 0.027360225096344948, 0.024045884609222412, 0.11388301104307175, 0.0030229315161705017, 0.03995499759912491], [0.21274890005588531, 0.06614330410957336, 0.06345600634813309, 0.033561669290065765, 0.07704339921474457, 0.037603989243507385, 0.033880334347486496, 0.056390292942523956, 0.07459709793329239, 0.051115065813064575, 0.0625758171081543, 0.018782339990139008, 0.02710084803402424, 0.029663192108273506, 0.06403077393770218, 0.0036775015760213137, 0.08762950450181961], [0.15646961331367493, 0.08776260912418365, 0.07974721491336823, 0.0411919541656971, 0.07188419252634048, 0.11965487897396088, 0.03451933339238167, 0.05384588986635208, 0.03301876783370972, 0.06536418944597244, 0.03362317383289337, 0.026401661336421967, 0.04644794389605522, 0.01831713318824768, 0.07314130663871765, 0.003515407908707857, 0.05509469285607338], [0.21211197972297668, 0.04382183030247688, 0.050990987569093704, 0.033024292439222336, 0.025789812207221985, 0.02970501035451889, 0.04834725335240364, 0.03579633682966232, 0.09620000422000885, 0.0335482694208622, 0.05352738872170448, 0.013111952692270279, 0.04897868633270264, 0.07383595407009125, 0.023559490218758583, 0.0019043009961023927, 0.17574654519557953], [0.3819066882133484, 0.08176442235708237, 0.03418957069516182, 0.06456034630537033, 0.005222178529947996, 0.041482314467430115, 0.011875848285853863, 0.029762711375951767, 0.04397178068757057, 0.03319787606596947, 0.012191012501716614, 0.03511623293161392, 0.13491596281528473, 0.015290050767362118, 0.0372476652264595, 0.0019692552741616964, 0.035336099565029144], [0.09620901197195053, 0.041578229516744614, 0.31169283390045166, 0.03668414056301117, 0.03822452202439308, 0.021066371351480484, 0.022275405004620552, 0.033473435789346695, 0.04219589754939079, 0.018906567245721817, 0.037108659744262695, 0.04062177240848541, 0.009884788654744625, 0.02744896151125431, 0.02885505184531212, 0.001958162523806095, 0.19181619584560394], [0.07354753464460373, 0.031746044754981995, 0.03370855748653412, 0.09180406481027603, 0.026845872402191162, 0.0830005556344986, 0.03937176987528801, 0.06520972400903702, 0.0715404599905014, 0.12851223349571228, 0.05710763856768608, 0.09867215901613235, 0.010774699039757252, 0.0170897264033556, 0.11321812123060226, 0.001450088107958436, 0.05640070140361786], [0.1939561367034912, 0.045106787234544754, 0.0802108570933342, 0.02228483185172081, 0.021224133670330048, 0.028737127780914307, 0.020517027005553246, 0.028441190719604492, 0.04733382910490036, 0.030314233154058456, 0.032439153641462326, 0.013478711247444153, 0.07927366346120834, 0.0559023953974247, 0.030072104185819626, 0.0008272157283499837, 0.2698805332183838], [0.1932566910982132, 0.05782658979296684, 0.10653842985630035, 0.05435830354690552, 0.023610200732946396, 0.05593056604266167, 0.038798633962869644, 0.053458474576473236, 0.04146949201822281, 0.06763003766536713, 0.046373311430215836, 0.032822828739881516, 0.036033131182193756, 0.019495736807584763, 0.052954256534576416, 0.0010276242392137647, 0.11841575801372528], [0.019862614572048187, 0.046093281358480453, 0.029258329421281815, 0.2113860845565796, 0.015482166782021523, 0.09965220838785172, 0.04415053129196167, 0.07724253833293915, 0.02706577628850937, 0.12980502843856812, 0.04290798306465149, 0.07443234324455261, 0.033330392092466354, 0.025529488921165466, 0.05104747414588928, 0.005796774756163359, 0.06695714592933655], [0.14302337169647217, 0.03921914100646973, 0.03209078311920166, 0.02483999729156494, 0.015109666623175144, 0.030239220708608627, 0.06036478281021118, 0.027004806324839592, 0.05528153479099274, 0.017851125448942184, 0.056019049137830734, 0.013829522766172886, 0.10232318192720413, 0.1335519552230835, 0.02189900353550911, 0.010814216919243336, 0.21653865277767181], [0.2494320422410965, 0.059944603592157364, 0.10032748430967331, 0.01274609100073576, 0.013893581926822662, 0.03304459527134895, 0.053322672843933105, 0.02009705640375614, 0.0634428933262825, 0.027313612401485443, 0.0968189612030983, 0.018844792619347572, 0.10093210637569427, 0.07190103828907013, 0.029793620109558105, 0.008931629359722137, 0.039213210344314575]], [[0.671194851398468, 0.01915689930319786, 0.011907553300261497, 0.03478606790304184, 0.028649643063545227, 0.04680236428976059, 0.024658257141709328, 0.00968124344944954, 0.01591922715306282, 0.017978355288505554, 0.028600750491023064, 0.012868168763816357, 0.013397172093391418, 0.025381172075867653, 0.017668498679995537, 0.008846667595207691, 0.012503070756793022], [0.017089108005166054, 0.03312318027019501, 0.04983589053153992, 0.11128441244363785, 0.05250997468829155, 0.14693203568458557, 0.11687581241130829, 0.059757061302661896, 0.02713387832045555, 0.17518752813339233, 0.040342431515455246, 0.05408574640750885, 0.011423339135944843, 0.013457762077450752, 0.028423406183719635, 0.027330221608281136, 0.03520827367901802], [0.005015794653445482, 0.04534822329878807, 0.04315515235066414, 0.12247348576784134, 0.05823676660656929, 0.09419041872024536, 0.07065196335315704, 0.08553071320056915, 0.025845950469374657, 0.07467449456453323, 0.11059527844190598, 0.07315310090780258, 0.026247458532452583, 0.01049153320491314, 0.024695413187146187, 0.07305620610713959, 0.056637998670339584], [0.010763397440314293, 0.05362556129693985, 0.033220190554857254, 0.05517978593707085, 0.023186223581433296, 0.15995487570762634, 0.17351284623146057, 0.09160010516643524, 0.01148084457963705, 0.10215006023645401, 0.04624297097325325, 0.08991813659667969, 0.015357783064246178, 0.019870886579155922, 0.07278956472873688, 0.017688769847154617, 0.02345803566277027], [0.0029674333054572344, 0.07579675316810608, 0.06865131109952927, 0.10314149409532547, 0.02762426808476448, 0.12933757901191711, 0.10375092923641205, 0.09408161044120789, 0.024810174480080605, 0.07882919162511826, 0.03387993574142456, 0.08331193029880524, 0.023419402539730072, 0.029721466824412346, 0.038078486919403076, 0.03426215797662735, 0.04833570867776871], [0.005297343246638775, 0.06490714848041534, 0.06320276856422424, 0.15338999032974243, 0.02217305265367031, 0.05062092840671539, 0.07952085882425308, 0.09842997044324875, 0.03227349743247032, 0.11125883460044861, 0.07230368256568909, 0.08749931305646896, 0.02181393653154373, 0.03750718757510185, 0.04703756421804428, 0.025906287133693695, 0.026857640594244003], [0.0012337651569396257, 0.06750914454460144, 0.13214774429798126, 0.07038485258817673, 0.01255433727055788, 0.06384250521659851, 0.03692352771759033, 0.07273422181606293, 0.03256643936038017, 0.09908278286457062, 0.09461317211389542, 0.07932521402835846, 0.03678416460752487, 0.06723562628030777, 0.06731041520833969, 0.027951523661613464, 0.0378006212413311], [0.004508120007812977, 0.07648370414972305, 0.03366478905081749, 0.07647383958101273, 0.02482370100915432, 0.07820677012205124, 0.062093645334243774, 0.04182733967900276, 0.024308523163199425, 0.1374339610338211, 0.06206170469522476, 0.0959426760673523, 0.03489410877227783, 0.0352131687104702, 0.09585284441709518, 0.04907927289605141, 0.06713183224201202], [0.002947599161416292, 0.05902518332004547, 0.02898063324391842, 0.05435969680547714, 0.034995388239622116, 0.05824629217386246, 0.05814948305487633, 0.08742277324199677, 0.02454894408583641, 0.10512379556894302, 0.06550242006778717, 0.11271978169679642, 0.05958529934287071, 0.045559123158454895, 0.08800606429576874, 0.038062553852796555, 0.0767650455236435], [0.01074732095003128, 0.05469321087002754, 0.012058568187057972, 0.08738283067941666, 0.02686689980328083, 0.04273240268230438, 0.04493338614702225, 0.05517585948109627, 0.024320121854543686, 0.029983900487422943, 0.07900651544332504, 0.2319403737783432, 0.02683134749531746, 0.06231360882520676, 0.14187926054000854, 0.019327478483319283, 0.04980694130063057], [0.002987390151247382, 0.029629472643136978, 0.04926900938153267, 0.06659319251775742, 0.019704820588231087, 0.031201273202896118, 0.028046200051903725, 0.05741332843899727, 0.029895326122641563, 0.06387273222208023, 0.044326018542051315, 0.14858096837997437, 0.11631489545106888, 0.05389934033155441, 0.10993817448616028, 0.044632405042648315, 0.10369552671909332], [0.001977337524294853, 0.044142622500658035, 0.019095830619335175, 0.054626524448394775, 0.023638250306248665, 0.029188821092247963, 0.046918995678424835, 0.042353227734565735, 0.019718648865818977, 0.09401752054691315, 0.05046665668487549, 0.052264370024204254, 0.06172715127468109, 0.10640604048967361, 0.2342640906572342, 0.05195016786456108, 0.06724380701780319], [0.0062879519537091255, 0.04567281901836395, 0.020817242562770844, 0.03572683036327362, 0.02095838077366352, 0.03473515063524246, 0.025478718802332878, 0.039598483592271805, 0.024183569476008415, 0.07430484145879745, 0.04863784834742546, 0.08400267362594604, 0.11526330560445786, 0.03831319510936737, 0.12842756509780884, 0.11388657987117767, 0.1437048465013504], [0.0034787384793162346, 0.027433713898062706, 0.030677685514092445, 0.037292055785655975, 0.007931438274681568, 0.018535934388637543, 0.025689680129289627, 0.03938944265246391, 0.010638811625540257, 0.05754554644227028, 0.07062377780675888, 0.14103244245052338, 0.058240339159965515, 0.022433685138821602, 0.15888355672359467, 0.10568378865718842, 0.18448929488658905], [0.0045853545889258385, 0.05456317961215973, 0.047861918807029724, 0.04501941427588463, 0.021089700981974602, 0.028721049427986145, 0.0444602444767952, 0.07679926604032516, 0.033816225826740265, 0.05134221166372299, 0.10343495011329651, 0.1384839117527008, 0.03478261083364487, 0.046661630272865295, 0.06622140854597092, 0.07720151543617249, 0.1249554306268692], [0.010323612950742245, 0.04529334604740143, 0.021447692066431046, 0.04741401970386505, 0.0491061732172966, 0.03374853357672691, 0.037580959498882294, 0.035552579909563065, 0.03899684548377991, 0.041925858706235886, 0.02629224956035614, 0.04956913739442825, 0.08727940171957016, 0.03874996677041054, 0.05833202600479126, 0.19846384227275848, 0.17992380261421204], [0.11402101069688797, 0.05104634910821915, 0.020451245829463005, 0.06762772053480148, 0.06010618805885315, 0.046797432005405426, 0.056588590145111084, 0.028770504519343376, 0.03096618689596653, 0.03973765671253204, 0.08339827507734299, 0.03702156990766525, 0.05456780269742012, 0.10786979645490646, 0.05396602675318718, 0.08300936967134476, 0.06405428797006607]], [[0.047720424830913544, 0.21376334130764008, 0.03867128863930702, 0.027641823515295982, 0.05007321760058403, 0.02121145650744438, 0.06661080569028854, 0.09859900921583176, 0.10551216453313828, 0.05921408534049988, 0.054283905774354935, 0.022203760221600533, 0.1241607666015625, 0.02077336795628071, 0.012322681955993176, 0.02584298886358738, 0.011394944041967392], [0.2520570158958435, 0.0517495833337307, 0.026011744514107704, 0.06807569414377213, 0.01620563119649887, 0.06611252576112747, 0.06601668149232864, 0.09829271584749222, 0.02084486372768879, 0.05114300549030304, 0.03105522319674492, 0.04610902816057205, 0.02043185383081436, 0.01692543365061283, 0.10181885212659836, 0.01524619571864605, 0.051903944462537766], [0.13388089835643768, 0.01656069979071617, 0.18057742714881897, 0.013417931273579597, 0.01298569981008768, 0.014339668676257133, 0.02814226970076561, 0.01884855516254902, 0.014448742382228374, 0.03874737396836281, 0.32232794165611267, 0.044969744980335236, 0.008164004422724247, 0.03352636098861694, 0.04258730262517929, 0.023696687072515488, 0.052778806537389755], [0.29217568039894104, 0.023780545219779015, 0.01885560154914856, 0.03341545909643173, 0.01919577457010746, 0.06741513311862946, 0.0391358844935894, 0.06950320303440094, 0.007121453061699867, 0.11888252198696136, 0.042787160724401474, 0.039387546479701996, 0.02087760716676712, 0.019379161298274994, 0.09960120171308517, 0.03990485146641731, 0.04858126863837242], [0.2758451998233795, 0.056308090686798096, 0.011259142309427261, 0.04851212725043297, 0.026065856218338013, 0.042715493589639664, 0.034671857953071594, 0.08086242526769638, 0.09144258499145508, 0.05610806494951248, 0.015038984827697277, 0.05720318481326103, 0.03334139287471771, 0.024223724380135536, 0.033906254917383194, 0.03266318887472153, 0.07983236759901047], [0.21537479758262634, 0.020727980881929398, 0.01798917166888714, 0.08803402632474899, 0.01894954778254032, 0.16457180678844452, 0.024972395971417427, 0.028943784534931183, 0.0185102391988039, 0.07982711493968964, 0.023053158074617386, 0.04025224596261978, 0.02458604983985424, 0.013143204152584076, 0.15471328794956207, 0.027720056474208832, 0.03863124921917915], [0.3537801206111908, 0.03765600547194481, 0.021091055124998093, 0.05258705094456673, 0.02025272138416767, 0.04134095460176468, 0.026862334460020065, 0.03661241754889488, 0.020059609785676003, 0.05652809143066406, 0.03512025997042656, 0.0420529842376709, 0.022461846470832825, 0.039187029004096985, 0.08862718939781189, 0.026111150160431862, 0.0796690583229065], [0.23573748767375946, 0.04648103192448616, 0.012020515277981758, 0.069282166659832, 0.017465822398662567, 0.0877501517534256, 0.03273525834083557, 0.03490997478365898, 0.008869386278092861, 0.09054484218358994, 0.02792906016111374, 0.04459512233734131, 0.01607193611562252, 0.008928489871323109, 0.18128827214241028, 0.020376505330204964, 0.06501399725675583], [0.4998197555541992, 0.06757567077875137, 0.008977184072136879, 0.03996840491890907, 0.029659148305654526, 0.05262676253914833, 0.023801613599061966, 0.03640222176909447, 0.014457739889621735, 0.02986329421401024, 0.010611130855977535, 0.02447100169956684, 0.013120518997311592, 0.011220666579902172, 0.024865351617336273, 0.028882406651973724, 0.08367718756198883], [0.34405216574668884, 0.02201986312866211, 0.010789177380502224, 0.02378341183066368, 0.011644510552287102, 0.05227043107151985, 0.11469002813100815, 0.03607926517724991, 0.005636635236442089, 0.1730417162179947, 0.012823197990655899, 0.022495318204164505, 0.023535504937171936, 0.012834875844419003, 0.090457022190094, 0.01970032975077629, 0.024146543815732002], [0.06573358178138733, 0.037767961621284485, 0.29237082600593567, 0.029895417392253876, 0.015804169699549675, 0.024588918313384056, 0.06532561033964157, 0.024639593437314034, 0.012162655591964722, 0.02923431433737278, 0.17928092181682587, 0.03455723449587822, 0.023707227781414986, 0.04260903224349022, 0.05037829279899597, 0.024851500988006592, 0.04709279164671898], [0.23235562443733215, 0.009995768778026104, 0.009226943366229534, 0.022343061864376068, 0.014650436118245125, 0.016018694266676903, 0.023521065711975098, 0.045161452144384384, 0.01624043844640255, 0.05668651685118675, 0.03963998705148697, 0.1859220415353775, 0.008798979222774506, 0.013333666138350964, 0.22742222249507904, 0.02071806602180004, 0.05796492472290993], [0.32189422845840454, 0.05246077477931976, 0.018380697816610336, 0.07428880035877228, 0.03875477984547615, 0.04613209515810013, 0.01673899218440056, 0.06871698051691055, 0.051288485527038574, 0.03152643144130707, 0.024572761729359627, 0.01737002097070217, 0.009173892438411713, 0.009756525978446007, 0.04741721227765083, 0.0748094990849495, 0.09671778976917267], [0.47579216957092285, 0.03341686353087425, 0.011561528779566288, 0.03405674919486046, 0.02153155580163002, 0.04972793534398079, 0.03433798626065254, 0.03388983756303787, 0.018575400114059448, 0.05293137952685356, 0.021394869312644005, 0.018421201035380363, 0.007510688155889511, 0.013412791304290295, 0.03986223042011261, 0.03251711279153824, 0.10105966776609421], [0.23688752949237823, 0.0108243552967906, 0.01316611748188734, 0.018121730536222458, 0.024788521230220795, 0.029742639511823654, 0.022875141352415085, 0.03661922737956047, 0.01866363175213337, 0.040579747408628464, 0.03550487011671066, 0.039216846227645874, 0.007806350011378527, 0.006634450983256102, 0.32208991050720215, 0.02652631141245365, 0.10995261371135712], [0.5090511441230774, 0.03934098035097122, 0.012598547153174877, 0.029301531612873077, 0.027334826067090034, 0.01857813633978367, 0.014025499112904072, 0.040016673505306244, 0.07644248753786087, 0.03588859736919403, 0.008823784999549389, 0.03644595667719841, 0.015649840235710144, 0.009137430228292942, 0.020502522587776184, 0.018193110823631287, 0.08866895735263824], [0.44825446605682373, 0.06365934759378433, 0.023191332817077637, 0.024077363312244415, 0.034970011562108994, 0.019658615812659264, 0.026009412482380867, 0.05865873396396637, 0.047118041664361954, 0.07019788771867752, 0.01972860097885132, 0.01687077060341835, 0.024124758318066597, 0.014678318053483963, 0.042866211384534836, 0.03839792683720589, 0.027538230642676353]], [[0.7451545000076294, 0.025153405964374542, 0.02552115172147751, 0.009643497876822948, 0.020314043387770653, 0.006562510970979929, 0.012321277521550655, 0.01946498639881611, 0.016965145245194435, 0.018122106790542603, 0.028326988220214844, 0.004769502207636833, 0.010744098573923111, 0.011007807217538357, 0.006158906035125256, 0.018966948613524437, 0.020803159102797508], [0.0031572214793413877, 0.062183670699596405, 0.2994574308395386, 0.44102942943573, 0.07474800199270248, 0.05210312828421593, 0.00819951482117176, 0.007802718784660101, 0.010957518592476845, 0.0009434346575289965, 0.0008871643804013729, 0.0033626225776970387, 0.009844167158007622, 0.0052651530131697655, 0.00157138891518116, 0.00542127899825573, 0.013066091574728489], [0.00046144123189151287, 0.00588524155318737, 0.005040581338107586, 0.9464016556739807, 0.01687931828200817, 0.013949829153716564, 0.0018351994222030044, 0.0009218106861226261, 0.0016436539590358734, 0.002824763534590602, 6.466472405008972e-05, 0.000759272777941078, 0.0006252228049561381, 0.0008957663085311651, 0.000957638316322118, 0.00021812962950207293, 0.000635715143289417], [0.0037412280216813087, 0.0069127860479056835, 0.007033566012978554, 0.027235163375735283, 0.6280022263526917, 0.28661036491394043, 0.007580637466162443, 0.017715128138661385, 0.001745722722262144, 0.0055137318558990955, 0.00018717542116064578, 0.00013205768482293934, 0.0002754918532446027, 9.299442172050476e-05, 0.000722126045729965, 0.005871881730854511, 0.000627747387625277], [0.0001619409304112196, 0.0035754884593188763, 0.0027697747573256493, 0.02128903940320015, 0.011731527745723724, 0.930479109287262, 0.017311060801148415, 0.0024907062761485577, 0.004442631267011166, 0.0012081891763955355, 0.000741250638384372, 0.0006815084489062428, 0.0001771737152012065, 0.00019034430442843586, 0.00017986273451242596, 0.0009743485716171563, 0.001595988986082375], [0.012732340954244137, 0.00844080001115799, 0.003018890507519245, 0.02174896001815796, 0.0326155386865139, 0.004862046334892511, 0.326991468667984, 0.5106053948402405, 0.025972122326493263, 0.0434732660651207, 0.0006534121348522604, 0.004471790511161089, 0.0017902579857036471, 5.996192339807749e-05, 0.0003675948246382177, 0.0011276681907474995, 0.0010685266461223364], [0.0015753676416352391, 0.0011321118799969554, 0.0038611660711467266, 0.005421122070401907, 0.03204385191202164, 0.06555818021297455, 0.031181486323475838, 0.6302411556243896, 0.18330204486846924, 0.021275192499160767, 0.009456195868551731, 0.001655089552514255, 0.008835040032863617, 0.0012219016207382083, 0.0003658908244688064, 0.0012725957203656435, 0.0016016027657315135], [0.002360299928113818, 0.00018324308621231467, 0.0006266485434025526, 0.00541038578376174, 0.004645211156457663, 0.05752834305167198, 0.019447309896349907, 0.026201605796813965, 0.7960488200187683, 0.057861361652612686, 0.018578924238681793, 0.0024988565128296614, 0.004858550615608692, 0.0013572156894952059, 0.0018950582016259432, 0.00012335630890447646, 0.000374768947949633], [0.0015419498085975647, 0.0003694420156534761, 0.0001021947173285298, 0.0011258574668318033, 0.0073355818167328835, 0.0028249016031622887, 0.009090878069400787, 0.07049073278903961, 0.028716307133436203, 0.7683790922164917, 0.04319748282432556, 0.04147972911596298, 0.015691429376602173, 0.0028167746495455503, 0.005019667092710733, 0.0016903961077332497, 0.0001276173716178164], [0.28313905000686646, 0.0007142736576497555, 0.0010427264496684074, 0.00013848088565282524, 0.0017456668429076672, 0.006640119012445211, 0.0021166368387639523, 0.007536482065916061, 0.058983445167541504, 0.018611617386341095, 0.5192825794219971, 0.03475469350814819, 0.0354338176548481, 0.014006159268319607, 0.0038725638296455145, 0.007994521409273148, 0.003987184725701809], [0.001502974540926516, 0.00028729723999276757, 0.00011328402615617961, 0.0003170001145917922, 9.271392627852038e-05, 0.0009002865408547223, 0.0011026617139577866, 0.0024558042641729116, 0.00798447523266077, 0.02750794216990471, 0.01859916001558304, 0.8619107007980347, 0.0439099483191967, 0.008507990278303623, 0.021430544555187225, 0.001345031545497477, 0.0020321577321738005], [0.06529746204614639, 0.001925544929690659, 0.001975933089852333, 0.0006543104536831379, 0.0006761232507415116, 4.74435837531928e-05, 0.0006524022319354117, 0.006825409829616547, 0.005318712443113327, 0.01423073559999466, 0.05236036330461502, 0.016635922715067863, 0.6952612400054932, 0.06741685420274734, 0.04272660240530968, 0.021553918719291687, 0.006441008765250444], [0.0010131513699889183, 0.00024425669107586145, 0.0006995332660153508, 0.0004960407386533916, 0.00021113731781952083, 9.808436880121008e-05, 2.5766841645236127e-05, 0.0002360682701691985, 0.00221252185292542, 0.001196482451632619, 0.004790224600583315, 0.055146560072898865, 0.019162459298968315, 0.8349206447601318, 0.05011487752199173, 0.017872877418994904, 0.011559313163161278], [0.0012443653540685773, 0.0017997882096096873, 0.0002811395388562232, 0.0026242986787110567, 0.0013681675773113966, 0.00047414368600584567, 0.00027355836937204003, 0.00012265467375982553, 0.00043697329238057137, 0.007795353885740042, 0.0030619092285633087, 0.041976891458034515, 0.01186008844524622, 0.01584525965154171, 0.783201277256012, 0.10042428970336914, 0.027209846302866936], [0.027631185948848724, 0.002444065874442458, 0.0010895362356677651, 0.00024119912995956838, 0.0017292182892560959, 0.00033898581750690937, 0.00017138129624072462, 0.0012223366647958755, 0.00012846679601352662, 0.0004957642522640526, 0.0027861965354532003, 0.0040708607994019985, 0.013752028346061707, 0.05852700024843216, 0.04828763008117676, 0.6877867579460144, 0.14929737150669098], [0.004256123211234808, 0.000997748109512031, 0.0015630173729732633, 0.0031726760789752007, 0.00024683072115294635, 0.0006816654349677265, 0.0006762493867427111, 0.00030320766381919384, 0.0002907389134634286, 9.27673390833661e-05, 0.0002009517338592559, 0.004235886503010988, 0.003210105001926422, 0.025369442999362946, 0.0122502651065588, 0.037502437829971313, 0.9049498438835144], [0.11767386645078659, 0.0022549768909811974, 0.0028091715648770332, 0.003984193783253431, 0.004113744478672743, 0.0005177510902285576, 0.0026409714482724667, 0.006197361275553703, 0.0011508730240166187, 0.0013756620464846492, 0.0003287947329226881, 0.00046096972073428333, 0.022435227409005165, 0.01867634989321232, 0.05194510146975517, 0.37662944197654724, 0.38680559396743774]], [[0.8526833653450012, 0.0032695296686142683, 0.011551490984857082, 0.00544710922986269, 0.01598731055855751, 0.008282527327537537, 0.012744382955133915, 0.02599107287824154, 0.0027186654042452574, 0.006457486189901829, 0.011347560212016106, 0.0031270631588995457, 0.0018809998873621225, 0.005654733162373304, 0.001028232742100954, 3.0735142786397773e-07, 0.031828176230192184], [0.21743902564048767, 0.05294176563620567, 0.053285591304302216, 0.228081613779068, 0.06002851575613022, 0.06309117376804352, 0.06675437837839127, 0.022872641682624817, 0.03824923187494278, 0.030409112572669983, 0.014735689386725426, 0.02496393769979477, 0.02708890475332737, 0.012282434850931168, 0.03079081140458584, 0.014529098756611347, 0.04245603457093239], [0.1958322376012802, 0.21364295482635498, 0.041592516005039215, 0.11042574048042297, 0.027908073738217354, 0.025414178147912025, 0.023031063377857208, 0.023872224614024162, 0.01713087595999241, 0.08998372405767441, 0.01110448781400919, 0.030580000951886177, 0.05691128224134445, 0.03128471598029137, 0.012936118058860302, 0.026119444519281387, 0.062230370938777924], [0.24989135563373566, 0.16116948425769806, 0.17640462517738342, 0.034966710954904556, 0.07457013428211212, 0.10098152607679367, 0.034733209758996964, 0.03284424543380737, 0.014094722457230091, 0.030791591852903366, 0.01806594617664814, 0.009875042364001274, 0.014007732272148132, 0.012307008728384972, 0.0055858478881418705, 0.004703985061496496, 0.025006795302033424], [0.2814714014530182, 0.07654623687267303, 0.02688213624060154, 0.24088506400585175, 0.041733238846063614, 0.11249542236328125, 0.06446604430675507, 0.020678898319602013, 0.018066365271806717, 0.03987482190132141, 0.006786905694752932, 0.014736246317625046, 0.009687816724181175, 0.0042737992480397224, 0.008956446312367916, 0.003673667088150978, 0.02878551557660103], [0.15839692950248718, 0.09116111695766449, 0.063274085521698, 0.37216219305992126, 0.037483301013708115, 0.016058018431067467, 0.041790835559368134, 0.06045303866267204, 0.01645219326019287, 0.09039048850536346, 0.006611851043999195, 0.006202797871083021, 0.012223483063280582, 0.004292212892323732, 0.01122099906206131, 0.003108011791482568, 0.00871854368597269], [0.20248258113861084, 0.06758779287338257, 0.017807967960834503, 0.09203970432281494, 0.06438888609409332, 0.06395702064037323, 0.024538835510611534, 0.10689102113246918, 0.03681620955467224, 0.1755915880203247, 0.02841677889227867, 0.022966507822275162, 0.029557237401604652, 0.00709288427606225, 0.011642256751656532, 0.027509616687893867, 0.020713042467832565], [0.12243456393480301, 0.04504697024822235, 0.055201660841703415, 0.0676695704460144, 0.05157007277011871, 0.13124461472034454, 0.17468780279159546, 0.007502433378249407, 0.04826091602444649, 0.10847876965999603, 0.06332867592573166, 0.03764553740620613, 0.02980096824467182, 0.009176823310554028, 0.009559781290590763, 0.017622025683522224, 0.02076887898147106], [0.22852833569049835, 0.02745443396270275, 0.00859803892672062, 0.04794522002339363, 0.035842105746269226, 0.07351494580507278, 0.0488804429769516, 0.16561509668827057, 0.01487303338944912, 0.2089562714099884, 0.03720799833536148, 0.028911396861076355, 0.022371811792254448, 0.007055867929011583, 0.021924098953604698, 0.0010880491463467479, 0.02123287133872509], [0.4060465097427368, 0.007750153541564941, 0.021108923479914665, 0.020073672756552696, 0.007817709818482399, 0.06683580577373505, 0.04070812463760376, 0.04328356683254242, 0.024571934714913368, 0.007637559436261654, 0.1835530549287796, 0.05996112897992134, 0.004648547153919935, 0.011250710114836693, 0.007630395703017712, 0.0007467112736776471, 0.08637542277574539], [0.3115960955619812, 0.03992041200399399, 0.009764373302459717, 0.012409734539687634, 0.02524895966053009, 0.014462258666753769, 0.041565265506505966, 0.044773880392313004, 0.05029427632689476, 0.16736742854118347, 0.04535451903939247, 0.06109927222132683, 0.06396802514791489, 0.013995361514389515, 0.01633756048977375, 0.010246503166854382, 0.0715961679816246], [0.09199883788824081, 0.01644899882376194, 0.02511855587363243, 0.02418057806789875, 0.018300030380487442, 0.02892596460878849, 0.022990504279732704, 0.19662997126579285, 0.014462399296462536, 0.03665975481271744, 0.34093764424324036, 0.02906440570950508, 0.03363645449280739, 0.025327419862151146, 0.05579908937215805, 0.007506872061640024, 0.032012492418289185], [0.23324275016784668, 0.03440278023481369, 0.017600366845726967, 0.03532235324382782, 0.014574307948350906, 0.03373589739203453, 0.016016129404306412, 0.02998761460185051, 0.03929803892970085, 0.09923556447029114, 0.03196278586983681, 0.07978761941194534, 0.041951924562454224, 0.05798477306962013, 0.04025726020336151, 0.04079589992761612, 0.15384390950202942], [0.14965450763702393, 0.03464731574058533, 0.02138260006904602, 0.024138372391462326, 0.014683496206998825, 0.011360610835254192, 0.012107504531741142, 0.052744779735803604, 0.012923222966492176, 0.14199969172477722, 0.054228778928518295, 0.052751462906599045, 0.1201290413737297, 0.020280517637729645, 0.09806517511606216, 0.03675633668899536, 0.14214664697647095], [0.0674249678850174, 0.04381589591503143, 0.022828198969364166, 0.01010636705905199, 0.01850370690226555, 0.04712536558508873, 0.0359027162194252, 0.06740608811378479, 0.025029823184013367, 0.11066380888223648, 0.10756460577249527, 0.10832084715366364, 0.015578920021653175, 0.044414177536964417, 0.055759139358997345, 0.037745412439107895, 0.18181002140045166], [0.41598665714263916, 0.03077869862318039, 0.0105829406529665, 0.027557121589779854, 0.01741187833249569, 0.0207478329539299, 0.01622147299349308, 0.016475584357976913, 0.008725565858185291, 0.045961376279592514, 0.02180391177535057, 0.02867053635418415, 0.02540312148630619, 0.037083230912685394, 0.04485275223851204, 0.0016573858447372913, 0.23007996380329132], [0.5602381229400635, 0.018121130764484406, 0.02596162259578705, 0.011367041617631912, 0.022935617715120316, 0.01138344220817089, 0.010130239650607109, 0.013834542594850063, 0.004972664173692465, 0.015121434815227985, 0.021982580423355103, 0.005542328115552664, 0.03403451293706894, 0.03163924813270569, 0.02081630378961563, 0.0015401361742988229, 0.19037893414497375]]], "negativeColor": "#1f78b4", "positiveColor": "#e31a1c", "tokens": ["[CLS]", "then", "i", "tried", "to", "find", "some", "way", "of", "embracing", "my", "mother", "'", "s", "ghost", ".", "[SEP]"]}
    )
    </script></div></div>
</div>
<p>One thing you’ll see immediately is that earlier layers have much more diffuse
attention weightings than later layers. Here’s the sixth layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_heads</span><span class="p">(</span>
    <span class="n">attentions</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">negative_color</span> <span class="o">=</span> <span class="s2">&quot;#1f78b4&quot;</span><span class="p">,</span>
    <span class="n">positive_color</span> <span class="o">=</span> <span class="s2">&quot;#e31a1c&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div id="circuits-vis-5e219b35-057e" style="margin: 15px 0;"/>
    <script crossorigin type="module">
    import { render, AttentionHeads } from "https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js";
    render(
      "circuits-vis-5e219b35-057e",
      AttentionHeads,
      {"attention": [[[0.03676910325884819, 0.06472685188055038, 0.016886908560991287, 0.04940935969352722, 0.004113558679819107, 0.01746641844511032, 0.0007284281309694052, 0.0017033853800967336, 0.0034568472765386105, 0.05391843244433403, 0.0036936351098120213, 0.0063156369142234325, 0.002796588931232691, 0.006340304389595985, 0.014724387787282467, 0.011027662083506584, 0.705922544002533], [0.049127671867609024, 0.025012878701090813, 0.09353750199079514, 0.26737311482429504, 0.057731129229068756, 0.055555928498506546, 0.012865970842540264, 0.016017986461520195, 0.003271885449066758, 0.010765600018203259, 0.0010140250669792295, 0.0026343795470893383, 0.0006501415628008544, 0.0018409858457744122, 0.010713838040828705, 0.011275283992290497, 0.38061174750328064], [0.044185977429151535, 0.015712961554527283, 0.03545418381690979, 0.11674605309963226, 0.056964073330163956, 0.0892740860581398, 0.005858971271663904, 0.012479164637625217, 0.011744368821382523, 0.01165612880140543, 0.0035189969930797815, 0.0012632731813937426, 0.0006744438433088362, 0.0009356923401355743, 0.002800361718982458, 0.012507828883826733, 0.5782233476638794], [0.06037050113081932, 0.017415156587958336, 0.020016668364405632, 0.0589069202542305, 0.04503471776843071, 0.15299351513385773, 0.018287397921085358, 0.05554817244410515, 0.03836182504892349, 0.038715630769729614, 0.00460547860711813, 0.0019850502721965313, 0.00044183694990351796, 0.0008903426351025701, 0.007115656975656748, 0.026100553572177887, 0.45321059226989746], [0.04023945704102516, 0.0044235908426344395, 0.02047189511358738, 0.10080278664827347, 0.03345954418182373, 0.30317771434783936, 0.06696361303329468, 0.08782470971345901, 0.05267254263162613, 0.04671265184879303, 0.006423069164156914, 0.0009124112548306584, 0.0001469668495701626, 0.00028930424014106393, 0.004409386310726404, 0.02592708170413971, 0.20514322817325592], [0.028409717604517937, 0.0016385461203753948, 0.005073850508779287, 0.01872413232922554, 0.008071526885032654, 0.04737634211778641, 0.04798872396349907, 0.2530107796192169, 0.12126719951629639, 0.24893046915531158, 0.021791275590658188, 0.0060250423848629, 0.002149268053472042, 0.00307928747497499, 0.06633910536766052, 0.02905573882162571, 0.09106896072626114], [0.020218389108777046, 0.004390103276818991, 0.026418544352054596, 0.02977745234966278, 0.007157936226576567, 0.05326962098479271, 0.01853766106069088, 0.36411571502685547, 0.10859019309282303, 0.1403423696756363, 0.031778715550899506, 0.012999020516872406, 0.0008777398033998907, 0.0010395615827292204, 0.00784018263220787, 0.015529840253293514, 0.15711699426174164], [0.028415773063898087, 0.0012784225400537252, 0.011041318997740746, 0.013589323498308659, 0.00160962191876024, 0.02981262095272541, 0.010970721952617168, 0.030234141275286674, 0.01824708841741085, 0.5400567054748535, 0.05353716015815735, 0.011655007489025593, 0.002451601205393672, 0.0024601558689028025, 0.009512783028185368, 0.009779945947229862, 0.22534769773483276], [0.04447575658559799, 0.004693987779319286, 0.0066595072858035564, 0.008301373571157455, 0.0010056279134005308, 0.017685266211628914, 0.00951339490711689, 0.018228160217404366, 0.03127991408109665, 0.2599210739135742, 0.056202806532382965, 0.012042722664773464, 0.0030997165013104677, 0.003495587036013603, 0.018327929079532623, 0.06330931186676025, 0.44175785779953003], [0.03860833868384361, 0.0022334468085318804, 0.0007337811402976513, 0.0005805652472190559, 0.00024294778995681554, 0.0026299564633518457, 0.0019848202355206013, 0.008961024694144726, 0.009877613745629787, 0.039771683514118195, 0.01345683541148901, 0.022535551339387894, 0.02403390221297741, 0.026869451627135277, 0.4952962100505829, 0.03594478219747543, 0.27623915672302246], [0.03807888180017471, 0.0017314045690000057, 0.0012151250848546624, 0.00040309299947693944, 0.00014604881289415061, 0.000641621882095933, 7.759302388876677e-05, 0.00041245302418246865, 0.0023681558668613434, 0.010004125535488129, 0.011532618664205074, 0.028041353449225426, 0.03964854031801224, 0.038068145513534546, 0.10658295452594757, 0.04230190068483353, 0.6787459850311279], [0.02006477303802967, 0.002342195948585868, 0.004004083573818207, 0.0013656985247507691, 0.00037080576294101775, 0.0007804710767231882, 0.00023981898266356438, 0.0010658403625711799, 0.0014711921103298664, 0.009610909037292004, 0.019247444346547127, 0.01791643723845482, 0.01858820579946041, 0.020201383158564568, 0.16038501262664795, 0.05988828465342522, 0.6624574661254883], [0.04149475321173668, 0.00462212273851037, 0.005395497661083937, 0.0019388480577617884, 0.00031306015443988144, 0.0009737981017678976, 0.0006824175361543894, 0.0005427359719760716, 0.000865705544129014, 0.01632201299071312, 0.009838534519076347, 0.03217010945081711, 0.026581263169646263, 0.040797073394060135, 0.43591272830963135, 0.1080404594540596, 0.27350887656211853], [0.04086973890662193, 0.001999668311327696, 0.0036441609263420105, 0.0014122016727924347, 0.0002686128718778491, 0.0008672124240547419, 0.0009782810229808092, 0.0004202176642138511, 0.0006666819099336863, 0.009879937395453453, 0.0063440874218940735, 0.020478475838899612, 0.030530493706464767, 0.04370102658867836, 0.3551550805568695, 0.12264201045036316, 0.3601420223712921], [0.028706369921565056, 0.0037704079877585173, 0.0007016416639089584, 0.0011523240245878696, 0.00023232809326145798, 0.0009398619877174497, 0.0012014633975923061, 0.0005258918972685933, 0.00036295378231443465, 0.0023278507869690657, 0.001704631606116891, 0.0024387941230088472, 0.0020372604485601187, 0.003710036864504218, 0.012667245231568813, 0.04469791799783707, 0.8928229212760925], [0.06171545386314392, 0.07929686456918716, 0.025927677750587463, 0.047267474234104156, 0.005612162407487631, 0.019487828016281128, 0.004369939211755991, 0.00736456410959363, 0.004694994539022446, 0.03570219874382019, 0.002996201626956463, 0.0059514036402106285, 0.0020723161287605762, 0.005694316700100899, 0.0301684457808733, 0.07046367228031158, 0.5912144780158997], [0.021593479439616203, 0.002722941106185317, 0.0022374102845788, 0.004777875728905201, 0.00141732522752136, 0.005884293466806412, 0.002309058792889118, 0.0030895923264324665, 0.002028935356065631, 0.010783783160150051, 0.0016566907288506627, 0.0015799843240529299, 0.0010193244088441133, 0.0015993170673027635, 0.01115385815501213, 0.015532758086919785, 0.9106131792068481]], [[0.017225932329893112, 0.006487834267318249, 0.016398457810282707, 0.002309079747647047, 0.004101407248526812, 0.002047728281468153, 0.004513372667133808, 0.0026580451522022486, 0.010105986148118973, 0.003519551595672965, 0.0019883476197719574, 0.0032198424451053143, 0.004153679125010967, 0.0023428527638316154, 0.008916817605495453, 0.019794879481196404, 0.8902161121368408], [0.0031496246811002493, 0.006424119230359793, 0.0020419799257069826, 0.009569025598466396, 0.0007200564723461866, 0.00035994380596093833, 0.004157603718340397, 0.0003147590614389628, 0.0003359538677614182, 0.004461570642888546, 0.0006995406001806259, 0.000193963831407018, 0.00022171760792843997, 0.0005739564658142626, 0.00010933391604339704, 0.0033466422464698553, 0.963320255279541], [0.011524942703545094, 0.011195703409612179, 0.008355985395610332, 0.006719597149640322, 0.005454082041978836, 0.0016882000491023064, 0.00417852820828557, 0.00047919058124534786, 0.00027025461895391345, 0.0004930226132273674, 0.0008741689962334931, 0.00043689465383067727, 0.00013640880933962762, 0.00021480511350091547, 0.0007171945762820542, 0.006461606360971928, 0.9407995343208313], [0.004082158673554659, 0.011503053829073906, 0.00703110545873642, 0.0013861667830497026, 0.008381939493119717, 0.0001819196913857013, 0.00092268583830446, 0.00011155186803080142, 0.00035578865208663046, 0.00010270202619722113, 7.049088162602857e-05, 4.56244697488728e-06, 2.2025458747521043e-05, 9.04504704521969e-05, 7.987076241988689e-05, 0.0035308152437210083, 0.9621427655220032], [0.10865513235330582, 0.019764376804232597, 0.01611042395234108, 0.22814083099365234, 0.05284956842660904, 0.04103696346282959, 0.04705829918384552, 0.007833168841898441, 0.001188851660117507, 0.0007029218249954283, 0.000293094664812088, 6.174908048706129e-05, 3.56053642462939e-05, 1.5008205082267523e-05, 0.0004787523648701608, 0.06419683247804642, 0.41157856583595276], [0.021487630903720856, 0.0018852025968953967, 0.003333165543153882, 0.00775213772431016, 0.017302347347140312, 0.007207270245999098, 0.02982337586581707, 0.0011184235336259007, 0.0011861063539981842, 0.00047244864981621504, 0.0006539491005241871, 5.061316187493503e-06, 1.2414524462656118e-05, 4.4977161451242864e-05, 0.00016387338109780103, 0.009695455431938171, 0.8978562355041504], [0.018063977360725403, 0.00390843627974391, 0.002426562365144491, 0.0022948021069169044, 0.005913326051086187, 0.010369034484028816, 0.08856410533189774, 0.0010785709600895643, 0.001271324115805328, 0.00120149040594697, 0.0007403389317914844, 0.00014543096767738461, 2.053562093351502e-05, 2.1953692339593545e-05, 0.00012475866242311895, 0.009044451639056206, 0.8548109531402588], [0.009934858419001102, 0.000814935308881104, 0.0009235747274942696, 0.006427542772144079, 0.005923707969486713, 0.03417877107858658, 0.31966936588287354, 0.001984555972740054, 0.0023364892695099115, 0.00090157421072945, 0.0003949061210732907, 1.552806315885391e-05, 5.694820629287278e-06, 1.9376086129341274e-05, 0.0013925043167546391, 0.004922737833112478, 0.6101537942886353], [0.046647809445858, 0.0010415285360068083, 0.0002798606874421239, 0.016892366111278534, 0.0007576468051411211, 0.029668491333723068, 0.008247698657214642, 0.8736469745635986, 0.0013509825803339481, 0.0026536721270531416, 7.336770795518532e-05, 1.9439487005001865e-05, 6.80233642924577e-06, 3.5101020330330357e-06, 0.0007971086306497455, 0.004671694710850716, 0.013240956701338291], [0.008628598414361477, 0.00044835565495304763, 0.0009022390004247427, 0.0007282526348717511, 0.0023009604774415493, 0.0020969645120203495, 0.006558516528457403, 0.000782812072429806, 0.0027700515929609537, 0.012350116856396198, 0.0011476235231384635, 6.911560194566846e-05, 0.0001042219009832479, 0.00023962750856298953, 0.0002673197304829955, 0.004213836044073105, 0.9563913345336914], [0.01768861711025238, 0.0014143090229481459, 0.0020101817790418863, 0.00032167320023290813, 0.0004245385935064405, 0.0043497695587575436, 0.0029960002284497023, 0.0026422140654176474, 0.002254507737234235, 0.01154506579041481, 0.015698891133069992, 0.0038789098616689444, 0.001581536722369492, 0.0009085660567507148, 0.001993787009268999, 0.00781704019755125, 0.9224745631217957], [0.005425724666565657, 0.0004538605862762779, 0.0006957416771911085, 0.0015028766356408596, 0.0002466878213454038, 0.001134552527219057, 0.0018134861020371318, 0.000978027586825192, 0.00026416065520606935, 0.0011059266980737448, 0.039139989763498306, 0.0007918451447039843, 0.0007952188025228679, 0.0006988105014897883, 0.0008523104479536414, 0.003496724646538496, 0.940604031085968], [0.047431230545043945, 0.0012793493224307895, 0.0006292531033977866, 0.0001361525064567104, 8.107753819786012e-05, 7.916751928860322e-05, 0.0005220448947511613, 0.021003318950533867, 0.0017642892198637128, 0.01565876044332981, 0.016149330884218216, 0.14782394468784332, 0.0047242529690265656, 0.004765194375067949, 0.016045495867729187, 0.02968258410692215, 0.692224383354187], [0.033952150493860245, 0.002367120236158371, 0.000985482125543058, 0.00016568849969189614, 5.318260082276538e-05, 0.0001394637074554339, 0.0002558841079007834, 0.03990189731121063, 0.0006934039993211627, 0.009066080674529076, 0.008187179453670979, 0.12034118920564651, 0.005051787942647934, 0.004575046245008707, 0.01539868488907814, 0.023102032020688057, 0.7357637286186218], [0.01269316766411066, 0.004091951530426741, 0.0025063527282327414, 0.0006410895148292184, 0.000964683829806745, 0.0011179194552823901, 0.0019865708891302347, 0.015290994197130203, 0.0030941974837332964, 0.014803426340222359, 0.01581587642431259, 0.01480854395776987, 0.009902499616146088, 0.012700650840997696, 0.02644132636487484, 0.012488259933888912, 0.8506525158882141], [0.016958506777882576, 0.001975604332983494, 0.00353025714866817, 0.0011180895380675793, 0.001217311597429216, 0.0010958651546388865, 0.0009871148504316807, 0.0005303201032802463, 0.002328022848814726, 0.006020095199346542, 0.0016593231121078134, 0.001997958403080702, 0.005908010061830282, 0.004259025678038597, 0.0053214021027088165, 0.022626560181379318, 0.9224666357040405], [0.006640237756073475, 0.0012084315530955791, 0.0009942386532202363, 0.0003932415565941483, 0.0008112337091006339, 0.0003785072185564786, 0.0016307526966556907, 0.0004168075101915747, 0.0006948229274712503, 0.001735540572553873, 0.0008073322824202478, 0.00012054239050485194, 0.0003382846189197153, 0.0005022197146899998, 0.0003949840320274234, 0.004825481679290533, 0.9781073927879333]], [[0.022427665069699287, 0.012447577901184559, 0.013234925456345081, 0.004011484328657389, 0.004265489522367716, 0.0018464154563844204, 0.0025821758899837732, 0.0029379462357610464, 0.008816137909889221, 0.007627632934600115, 0.006613138131797314, 0.0016032682033255696, 0.001422200002707541, 0.00168175611179322, 0.006260282825678587, 0.019566809758543968, 0.8826549649238586], [0.04120699316263199, 0.04041425138711929, 0.03936295583844185, 0.03743930906057358, 0.044412773102521896, 0.03977981582283974, 0.034220919013023376, 0.024530192837119102, 0.040089499205350876, 0.0430622361600399, 0.02258465066552162, 0.0054954844526946545, 0.011848120018839836, 0.012529794126749039, 0.0165915098041296, 0.10352791845798492, 0.4429035782814026], [0.06521783769130707, 0.043909162282943726, 0.06970421969890594, 0.021852193400263786, 0.030409881845116615, 0.028393812477588654, 0.0049901544116437435, 0.02624465338885784, 0.025359736755490303, 0.1041940376162529, 0.08534304797649384, 0.04526209831237793, 0.03437767177820206, 0.038779497146606445, 0.023100584745407104, 0.09707958251237869, 0.2557818591594696], [0.03571430221199989, 0.0828508660197258, 0.04570300132036209, 0.03921739012002945, 0.03364314138889313, 0.02253386564552784, 0.03033287636935711, 0.027646595612168312, 0.03646349161863327, 0.04879330098628998, 0.028412356972694397, 0.011568920686841011, 0.013997489586472511, 0.016749314963817596, 0.013706325553357601, 0.07209687680006027, 0.44056981801986694], [0.029171930626034737, 0.0996374860405922, 0.06927979737520218, 0.06494878232479095, 0.027003949508070946, 0.033109135925769806, 0.017504345625638962, 0.015607879497110844, 0.01704498939216137, 0.01955888234078884, 0.01930355094373226, 0.0062445118092000484, 0.006997294258326292, 0.008982313796877861, 0.008795257657766342, 0.0507819801568985, 0.5060279369354248], [0.016094064339995384, 0.03666364401578903, 0.03977375477552414, 0.01770561933517456, 0.014545617625117302, 0.006699463352560997, 0.004177153576165438, 0.0029057501815259457, 0.009261474013328552, 0.013484170660376549, 0.014317759312689304, 0.010417246259748936, 0.0032879849895834923, 0.004422708414494991, 0.005667489022016525, 0.04453809931874275, 0.7560379505157471], [0.018873056396842003, 0.0716753900051117, 0.05183400586247444, 0.053395893424749374, 0.01833443157374859, 0.01683894544839859, 0.030968092381954193, 0.008800401352345943, 0.01725475862622261, 0.008006435818970203, 0.01514776237308979, 0.0036374619230628014, 0.006051808129996061, 0.006170306820422411, 0.005172867327928543, 0.03715217858552933, 0.6306862831115723], [0.02599792554974556, 0.17655740678310394, 0.030116146430373192, 0.01980554312467575, 0.015426388941705227, 0.03787305951118469, 0.020897790789604187, 0.03153865784406662, 0.05241283401846886, 0.03687255084514618, 0.056853242218494415, 0.029989566653966904, 0.04485074058175087, 0.02606736682355404, 0.0070906272158026695, 0.021717466413974762, 0.365932822227478], [0.03431634604930878, 0.05556163191795349, 0.07163490355014801, 0.07029546052217484, 0.016558289527893066, 0.05233830213546753, 0.04101813957095146, 0.026935823261737823, 0.008684995584189892, 0.01092368271201849, 0.023183519020676613, 0.0062931296415627, 0.006864659488201141, 0.007039198186248541, 0.006247748155146837, 0.053781259804964066, 0.5083228945732117], [0.029613839462399483, 0.05540242791175842, 0.0660008043050766, 0.05772107094526291, 0.023652857169508934, 0.0777725800871849, 0.03656509891152382, 0.052191197872161865, 0.013744505122303963, 0.031876299530267715, 0.04710539057850838, 0.04005354642868042, 0.0037508236709982157, 0.007390294689685106, 0.021049607545137405, 0.022793911397457123, 0.41331568360328674], [0.041567761451005936, 0.05129259079694748, 0.14187051355838776, 0.0560111328959465, 0.03179680183529854, 0.06048136204481125, 0.013183429837226868, 0.07472885400056839, 0.014581938274204731, 0.09394200146198273, 0.06836501508951187, 0.03105100616812706, 0.01314762607216835, 0.01998325064778328, 0.032977938652038574, 0.07105780392885208, 0.1839609444141388], [0.053900863975286484, 0.028429390862584114, 0.05088641867041588, 0.0381590761244297, 0.016227426007390022, 0.07477802783250809, 0.02643825113773346, 0.06998466700315475, 0.011521824635565281, 0.052203960716724396, 0.05579984560608864, 0.0403546467423439, 0.02422838658094406, 0.043906211853027344, 0.029562169685959816, 0.029780203476548195, 0.35383859276771545], [0.02200528420507908, 0.05659103766083717, 0.10320565104484558, 0.05072096362709999, 0.0182181429117918, 0.17211852967739105, 0.06293609738349915, 0.08959076553583145, 0.023673374205827713, 0.06863025575876236, 0.08442157506942749, 0.04789077863097191, 0.004873360972851515, 0.006008991971611977, 0.03173220902681351, 0.024375904351472855, 0.13300710916519165], [0.02123306319117546, 0.063551165163517, 0.09678447246551514, 0.053678978234529495, 0.018152212724089622, 0.13121914863586426, 0.04638311639428139, 0.11420755833387375, 0.027736011892557144, 0.09052091091871262, 0.09166401624679565, 0.043385524302721024, 0.005499906372278929, 0.007334219291806221, 0.03830857202410698, 0.02462056092917919, 0.12572047114372253], [0.03514368459582329, 0.023009907454252243, 0.02643643505871296, 0.011273779906332493, 0.00720567163079977, 0.031556837260723114, 0.008721408434212208, 0.028566084802150726, 0.01454853918403387, 0.010056945495307446, 0.019488153979182243, 0.013480275869369507, 0.015707017853856087, 0.018352728337049484, 0.02026318944990635, 0.03107132576406002, 0.6851180791854858], [0.25682422518730164, 0.03630559891462326, 0.05109313502907753, 0.02072760835289955, 0.053750209510326385, 0.010532963089644909, 0.009461650624871254, 0.006697004195302725, 0.014816287904977798, 0.020643120631575584, 0.014234934002161026, 0.027687078341841698, 0.008236423134803772, 0.01165857631713152, 0.03159512206912041, 0.15337184071540833, 0.2723642587661743], [0.022490572184324265, 0.007662976626306772, 0.00102411734405905, 0.0012438647681847215, 0.0011835283366963267, 0.0007753626559861004, 0.0013164639240130782, 0.001310243271291256, 0.0016207923181355, 0.0002620916347950697, 0.0011791852302849293, 0.00015888067719060928, 0.0009701623348519206, 0.0005520037957467139, 0.00038026305264793336, 0.005131955724209547, 0.9527375102043152]], [[0.011956890113651752, 0.0544363372027874, 0.05017249658703804, 0.04349902272224426, 0.042242780327796936, 0.024568721652030945, 0.028982294723391533, 0.018299616873264313, 0.010297940112650394, 0.07177771627902985, 0.027923649176955223, 0.007817749865353107, 0.005029377061873674, 0.008127913810312748, 0.010977456346154213, 0.029670467600226402, 0.5542195439338684], [0.019303278997540474, 0.06252916157245636, 0.05740953981876373, 0.08517393469810486, 0.026013588532805443, 0.019055284559726715, 0.00943166483193636, 0.006647266913205385, 0.008106702007353306, 0.013702237978577614, 0.004825133830308914, 0.004291014280170202, 0.0021120356395840645, 0.0037423218600451946, 0.012271837331354618, 0.028871987015008926, 0.6365129947662354], [0.013729261234402657, 0.10521730035543442, 0.06582774966955185, 0.03526334464550018, 0.0055340188555419445, 0.013488421216607094, 0.0019742355216294527, 0.002001672750338912, 0.0028802435845136642, 0.0023170451167970896, 0.0008477540104649961, 0.0014345018425956368, 0.00018159821047447622, 0.0003894625115208328, 0.005835122428834438, 0.01947474107146263, 0.7236034274101257], [0.018054060637950897, 0.3757050633430481, 0.033720068633556366, 0.029603872448205948, 0.01945611461997032, 0.0034295287914574146, 0.0049318005330860615, 0.0030053495429456234, 0.00502429436892271, 0.002991252578794956, 0.0009350570617243648, 0.002026413334533572, 0.0003860065480694175, 0.0009687324636615813, 0.0032094409689307213, 0.019155237823724747, 0.4773976504802704], [0.029861191287636757, 0.34802302718162537, 0.01984313875436783, 0.03178481385111809, 0.00703114690259099, 0.0042320541106164455, 0.0025851321406662464, 0.002335977740585804, 0.0011353258742019534, 0.0010039866901934147, 0.0002528813201934099, 0.0005107453907839954, 0.00013827670773025602, 0.0002720617048908025, 0.0016879483591765165, 0.020385578274726868, 0.5289167761802673], [0.025247668847441673, 0.2840604782104492, 0.026410499587655067, 0.1004270538687706, 0.028077883645892143, 0.020960776135325432, 0.01266159676015377, 0.005195895675569773, 0.0025292469654232264, 0.007885342463850975, 0.0006219808128662407, 0.0005147079355083406, 0.000150435182149522, 0.00027034213417209685, 0.0016226344741880894, 0.02779761701822281, 0.4555658996105194], [0.023438792675733566, 0.16295751929283142, 0.05139912664890289, 0.169523686170578, 0.09287708252668381, 0.06993232667446136, 0.026276715099811554, 0.009324158541858196, 0.012022987008094788, 0.005865682847797871, 0.0007193065830506384, 0.0003647095581982285, 6.317455699900165e-05, 0.00014253711560741067, 0.0008224674384109676, 0.01832028105854988, 0.3559495210647583], [0.023426970466971397, 0.053457774221897125, 0.05315445736050606, 0.12974275648593903, 0.10858739167451859, 0.11296240240335464, 0.025821059942245483, 0.026089364662766457, 0.013045801781117916, 0.025009075179696083, 0.0019778592977672815, 0.000603051739744842, 0.00013174384366720915, 0.00023123309074435383, 0.0009939480805769563, 0.010296001099050045, 0.41446903347969055], [0.04121462255716324, 0.0573955699801445, 0.06352151930332184, 0.04478219524025917, 0.08347055315971375, 0.03352612629532814, 0.0613141730427742, 0.15167616307735443, 0.0164711382240057, 0.011726746335625648, 0.0023931916803121567, 0.0011039285454899073, 0.0007389571401290596, 0.0015254217432811856, 0.004785766825079918, 0.014222529716789722, 0.41013145446777344], [0.032246991991996765, 0.04227559641003609, 0.05803697183728218, 0.02568940818309784, 0.022422129288315773, 0.023565182462334633, 0.0467088483273983, 0.08344809710979462, 0.02035200223326683, 0.03996841236948967, 0.015987349674105644, 0.010448819026350975, 0.0024929160717874765, 0.004265228286385536, 0.017572982236742973, 0.01307995617389679, 0.5414390563964844], [0.028465144336223602, 0.0764285996556282, 0.5363528728485107, 0.03298339992761612, 0.02433030679821968, 0.0427246019244194, 0.01686996966600418, 0.028286930173635483, 0.026009827852249146, 0.01785857602953911, 0.013259724713861942, 0.0012810011394321918, 0.00031710811890661716, 0.00030151050304993987, 0.0041522253304719925, 0.014633985236287117, 0.13574433326721191], [0.07599161565303802, 0.047760698944330215, 0.035446394234895706, 0.004454989917576313, 0.009671704843640327, 0.006794788409024477, 0.00827043503522873, 0.007677210494875908, 0.02555611915886402, 0.0352679118514061, 0.03723664581775665, 0.01788448542356491, 0.002962172031402588, 0.003732864512130618, 0.0025328428018838167, 0.015479770489037037, 0.6632793545722961], [0.06568397581577301, 0.01644721068441868, 0.032189760357141495, 0.02240760065615177, 0.03250863775610924, 0.03444265201687813, 0.024913955479860306, 0.024511003866791725, 0.06818081438541412, 0.2796573042869568, 0.12259523570537567, 0.015285110101103783, 0.012968027964234352, 0.013820384629070759, 0.014654915779829025, 0.01866845227777958, 0.20106498897075653], [0.0945427343249321, 0.020607415586709976, 0.02729041315615177, 0.024023525416851044, 0.031264595687389374, 0.052027639001607895, 0.026096222922205925, 0.02288353443145752, 0.06785845011472702, 0.2124357372522354, 0.1241324245929718, 0.020257066935300827, 0.014452694915235043, 0.016141850501298904, 0.01666458509862423, 0.024092240259051323, 0.20522892475128174], [0.11032956838607788, 0.021199503913521767, 0.016198666766285896, 0.0407964363694191, 0.0686962902545929, 0.11362148076295853, 0.03999660164117813, 0.018260803073644638, 0.06737536936998367, 0.13325434923171997, 0.017304614186286926, 0.015257464721798897, 0.004395172465592623, 0.008065560832619667, 0.010283559560775757, 0.023708820343017578, 0.29125577211380005], [0.017563482746481895, 0.07741579413414001, 0.12819324433803558, 0.11248078942298889, 0.08885432034730911, 0.07466156780719757, 0.07167084515094757, 0.04526375234127045, 0.04703253507614136, 0.0916154608130455, 0.08238327503204346, 0.03741798549890518, 0.009446140378713608, 0.011991356499493122, 0.01946300081908703, 0.03095809556543827, 0.05358833447098732], [0.008010409772396088, 0.005434765014797449, 0.0017642256570979953, 0.003243668470531702, 0.0014975802041590214, 0.002078127581626177, 0.002282958012074232, 0.0011471433099359274, 0.0014546453021466732, 0.003197574755176902, 0.0005945409648120403, 0.0006893218960613012, 0.0002713846042752266, 0.0005063345306552947, 0.0011426486307755113, 0.008612038567662239, 0.9580725431442261]], [[0.011667268350720406, 0.1226755678653717, 0.06107298284769058, 0.09506870061159134, 0.036707088351249695, 0.04480241984128952, 0.07225480675697327, 0.05138117074966431, 0.021132998168468475, 0.018946386873722076, 0.012209859676659107, 0.011138513684272766, 0.0035161208361387253, 0.006508504040539265, 0.03275034576654434, 0.03316515311598778, 0.3650020956993103], [0.028194895014166832, 0.05090092867612839, 0.011825486086308956, 0.1651517003774643, 0.019038120284676552, 0.01810719631612301, 0.007801310624927282, 0.023983675986528397, 0.016009196639060974, 0.01056849304586649, 0.00179405661765486, 0.0014015239430591464, 0.0006637921906076372, 0.0007585032726638019, 0.004909696988761425, 0.06491146236658096, 0.573979914188385], [0.10102340579032898, 0.04694003984332085, 0.05643133819103241, 0.05693651735782623, 0.014035779051482677, 0.032677631825208664, 0.01908964104950428, 0.009736373089253902, 0.003706354880705476, 0.003105748910456896, 0.024273725226521492, 0.01919640228152275, 0.0019106018589809537, 0.00288300309330225, 0.011810976080596447, 0.08428891003131866, 0.5119535326957703], [0.054911743849515915, 0.1074647381901741, 0.04596702381968498, 0.01890680193901062, 0.05075555294752121, 0.016137314960360527, 0.01876004785299301, 0.0034538910258561373, 0.01781928353011608, 0.02759701944887638, 0.008908016607165337, 0.0019355311524122953, 0.0017713053384795785, 0.0018365891883149743, 0.009676569141447544, 0.09602218866348267, 0.5180763006210327], [0.061634305864572525, 0.10675731301307678, 0.025264425203204155, 0.04029142111539841, 0.04413758963346481, 0.056012339890003204, 0.03163307532668114, 0.006779605057090521, 0.02859174646437168, 0.013283108361065388, 0.005399364046752453, 0.0011851052986457944, 0.0006456396076828241, 0.0009302801336161792, 0.0058250934816896915, 0.08958294242620468, 0.4820466637611389], [0.05412422493100166, 0.03766248747706413, 0.025053508579730988, 0.012403341010212898, 0.02235867828130722, 0.006667145527899265, 0.014276566915214062, 0.0053869448602199554, 0.01658540777862072, 0.05274574086070061, 0.010600974783301353, 0.0042205145582556725, 0.001996002160012722, 0.003089034231379628, 0.0025977930054068565, 0.05847785994410515, 0.6717537641525269], [0.01884884387254715, 0.04018126428127289, 0.011279930360615253, 0.017815612256526947, 0.02292720414698124, 0.010280787944793701, 0.027672255411744118, 0.021389547735452652, 0.046680789440870285, 0.02916376106441021, 0.005928958300501108, 0.0034337223041802645, 0.006379987113177776, 0.008917540311813354, 0.0012667353730648756, 0.03881237283349037, 0.689020574092865], [0.018005652353167534, 0.03583923727273941, 0.001434594509191811, 0.00874084047973156, 0.0242118202149868, 0.04090869799256325, 0.059373170137405396, 0.01436672918498516, 0.024431340396404266, 0.10021238774061203, 0.0007803519256412983, 0.0018823574064299464, 0.0018718545325100422, 0.00287316064350307, 0.03487164527177811, 0.05925753340125084, 0.5709387063980103], [0.05291244387626648, 0.049161285161972046, 0.01281666848808527, 0.016159361228346825, 0.04851512610912323, 0.05383045971393585, 0.03211478888988495, 0.018857909366488457, 0.05063983425498009, 0.047892291098833084, 0.0042002443224191666, 0.001932647661305964, 0.0019831617828458548, 0.0027488090563565493, 0.008898457512259483, 0.104847751557827, 0.4924887418746948], [0.09336942434310913, 0.022747794166207314, 0.006859472021460533, 0.022975433617830276, 0.011297369375824928, 0.06477133929729462, 0.030933059751987457, 0.040463097393512726, 0.02687745913863182, 0.04470662400126457, 0.0204616729170084, 0.014060437679290771, 0.005049305036664009, 0.010110978968441486, 0.051957134157419205, 0.12351858615875244, 0.40984082221984863], [0.11269287019968033, 0.023035995662212372, 0.028270021080970764, 0.009384561330080032, 0.0022403933107852936, 0.006059026811271906, 0.014039864763617516, 0.007835295982658863, 0.005887723062187433, 0.019089996814727783, 0.14137285947799683, 0.05174447223544121, 0.03761383891105652, 0.06671300530433655, 0.015192243270576, 0.11855959892272949, 0.34026819467544556], [0.09559796005487442, 0.015783270820975304, 0.01329733245074749, 0.010057990439236164, 0.001607959740795195, 0.017338959500193596, 0.021626321598887444, 0.044764623045921326, 0.007046113256365061, 0.03406176343560219, 0.034279897809028625, 0.04034828022122383, 0.029351288452744484, 0.026628678664565086, 0.07577681541442871, 0.0842413380742073, 0.44819143414497375], [0.06545116007328033, 0.013022158294916153, 0.06350653618574142, 0.009337776340544224, 0.0043181162327528, 0.03206697106361389, 0.015918564051389694, 0.029000844806432724, 0.010467257350683212, 0.05702068656682968, 0.2758451998233795, 0.044694676995277405, 0.031310029327869415, 0.046210069209337234, 0.0187534112483263, 0.056214042007923126, 0.22686241567134857], [0.0969502180814743, 0.016999738290905952, 0.09464599937200546, 0.010607318952679634, 0.007638194598257542, 0.031038355082273483, 0.012121767736971378, 0.027463708072900772, 0.013930022716522217, 0.05576165020465851, 0.17348751425743103, 0.02646675705909729, 0.029618961736559868, 0.03800560534000397, 0.017363879829645157, 0.06735008955001831, 0.2805502116680145], [0.06170444190502167, 0.05426853522658348, 0.009812941774725914, 0.019630318507552147, 0.006237282883375883, 0.014134041965007782, 0.00345831667073071, 0.039390843361616135, 0.007334457710385323, 0.10221631824970245, 0.01306973211467266, 0.02603822760283947, 0.004996587987989187, 0.004296394996345043, 0.0004057630430907011, 0.05885830894112587, 0.5741475224494934], [0.03730117157101631, 0.2796667516231537, 0.07642219215631485, 0.06161777675151825, 0.02976403199136257, 0.03409497067332268, 0.08841941505670547, 0.01865929737687111, 0.016292639076709747, 0.02327072247862816, 0.03839801996946335, 0.009860058315098286, 0.0037339108530431986, 0.006976078264415264, 0.01552005484700203, 0.08225033432245255, 0.1777525097131729], [0.012251179665327072, 0.007640731055289507, 0.0014619240537285805, 0.00477474182844162, 0.002811201848089695, 0.003352283500134945, 0.006621579639613628, 0.006520786788314581, 0.002293126890435815, 0.003980346489697695, 0.0012895652325823903, 0.002714140573516488, 0.002215428277850151, 0.002633927622810006, 0.007767722941935062, 0.018487824127078056, 0.9131834506988525]], [[0.01910785213112831, 0.014903120696544647, 0.009978157468140125, 0.024113671854138374, 0.00205283984541893, 0.006661504041403532, 0.009913294576108456, 0.004307406023144722, 0.002456758636981249, 0.002478254958987236, 0.00926230289041996, 0.0027283290401101112, 0.0017236829735338688, 0.004491059109568596, 0.0018986423965543509, 0.050370603799819946, 0.8335524797439575], [0.016675662249326706, 0.18045781552791595, 0.016104063019156456, 0.05727766454219818, 0.0058751716278493404, 0.004987158812582493, 0.003080957103520632, 0.015702079981565475, 0.007521453313529491, 0.004602997098118067, 0.020047631114721298, 0.0019946217071264982, 0.0009485677583143115, 0.0009617232135497034, 0.008039307780563831, 0.06314633786678314, 0.592576801776886], [0.014171338640153408, 0.006352042313665152, 0.12322666496038437, 0.026653364300727844, 0.0038528884761035442, 0.0015813910868018866, 0.0031278401147574186, 0.01573323644697666, 0.0024345722049474716, 0.0027626545634120703, 0.15337450802326202, 0.004429296590387821, 0.0011479536769911647, 0.0013523329980671406, 0.002383055165410042, 0.041559599339962006, 0.5958570837974548], [0.01676841266453266, 0.04068908840417862, 0.004277705680578947, 0.18816858530044556, 0.014842893928289413, 0.02449631318449974, 0.00776895135641098, 0.03254484757781029, 0.0024527611676603556, 0.0028294397052377462, 0.004146992228925228, 0.0015657370677217841, 0.0005374553147703409, 0.00056838319869712, 0.0013245206791907549, 0.090756356716156, 0.5662615299224854], [0.021951964125037193, 0.008833574131131172, 0.0010912142461165786, 0.07635498046875, 0.03201563283801079, 0.01884187012910843, 0.012629048898816109, 0.03440897911787033, 0.0012141885235905647, 0.002536223968490958, 0.0015373632777482271, 0.0005363650852814317, 0.0002510295307729393, 0.00025242145056836307, 0.00035597194801084697, 0.07208907604217529, 0.7150999903678894], [0.026728713884949684, 0.013632495887577534, 0.000352581701008603, 0.004948843736201525, 0.0007260505226440728, 0.030457744374871254, 0.016975710168480873, 0.005797065328806639, 0.0008660478633828461, 0.0041870237328112125, 0.0005808625719510019, 0.0003489389200694859, 0.0006249314174056053, 0.001352996681816876, 0.0009692244348116219, 0.04289929196238518, 0.8485514521598816], [0.01775876060128212, 0.0013548166025429964, 2.443992161715869e-05, 0.0012662396766245365, 0.0003187762922607362, 0.002105525229126215, 0.15170355141162872, 0.0005195881240069866, 0.0012425438035279512, 0.0007456588791683316, 0.00014364371600095183, 0.00021914209355600178, 0.00034897870500572026, 0.0013205610448494554, 0.0009981425246223807, 0.046605225652456284, 0.7733244299888611], [0.006372065283358097, 0.0027526074554771185, 0.00019625162531156093, 0.003941969480365515, 0.0018802362028509378, 0.0043764919973909855, 0.012461760081350803, 0.027696695178747177, 0.04852652549743652, 0.0015595281729474664, 0.0007121071103028953, 0.00027233094442635775, 0.0031200323719531298, 0.007002597209066153, 0.0021085168700665236, 0.02133805863559246, 0.8556821346282959], [0.03621207922697067, 0.005007922183722258, 0.00011573821393540129, 0.0020879569929093122, 0.00039247795939445496, 0.0009410266648046672, 0.0025766592007130384, 0.006576973013579845, 0.06884200125932693, 0.0018563432386144996, 0.00038826128002256155, 0.00026308203814551234, 0.001277366653084755, 0.002348940586671233, 0.002156368689611554, 0.09768258780241013, 0.7712740302085876], [0.01048310287296772, 0.00282229483127594, 0.00041244056774303317, 0.0014724250650033355, 0.00030992203392088413, 0.006872052792459726, 0.0004675215750467032, 0.017656659707427025, 0.0006460805307142437, 0.06831221282482147, 0.0018840752309188247, 0.0021536839194595814, 0.0026704990305006504, 0.0028477932792156935, 0.012063112109899521, 0.006991168484091759, 0.8619350790977478], [0.07104054093360901, 0.0027856777887791395, 0.025645747780799866, 0.0007308551575988531, 0.00023382990912068635, 0.00033423033892177045, 0.00017238206055480987, 0.0009836071403697133, 0.00011859436926897615, 0.005616725888103247, 0.08789525926113129, 0.032056231051683426, 0.03485617786645889, 0.034773021936416626, 0.016665363684296608, 0.029464194551110268, 0.6566277146339417], [0.036697424948215485, 0.0005949932965449989, 0.0005916686495766044, 0.00045584599138237536, 9.235892503056675e-05, 0.00014102326531428844, 3.122256384813227e-05, 0.0005369398859329522, 9.03727050172165e-05, 0.0004715401737485081, 0.001339860842563212, 0.15832841396331787, 0.007607225328683853, 0.011083293706178665, 0.01925993151962757, 0.02065216191112995, 0.7420257925987244], [0.05745594948530197, 0.000750238832551986, 0.00026851100847125053, 0.00023611490905750543, 0.00016379386943299323, 0.0004465423116926104, 0.0003134776488877833, 0.0022286176681518555, 0.0011021057143807411, 0.0017895306227728724, 0.001399766537360847, 0.011472312733530998, 0.1691807210445404, 0.2907455563545227, 0.05671108514070511, 0.0278822910040617, 0.3778533935546875], [0.0485006682574749, 0.0009592800051905215, 0.0002972405345644802, 0.00022094370797276497, 0.00010976576595567167, 0.0003359811089467257, 0.00023364365915767848, 0.0017205218318849802, 0.0011438571382313967, 0.0012191551504656672, 0.0010149732697755098, 0.014554581604897976, 0.15377981960773468, 0.3026014268398285, 0.07854750752449036, 0.028571417555212975, 0.3661891222000122], [0.009510970674455166, 0.0021202373318374157, 0.00021210774139035493, 0.00042227344238199294, 0.0004793246916960925, 0.001308868289925158, 0.0034451119136065245, 0.013329090550541878, 0.004652267321944237, 0.0034515096340328455, 0.0012489969376474619, 0.0013466834789142013, 0.01188607793301344, 0.04414118826389313, 0.07534623891115189, 0.007396213710308075, 0.819702684879303], [0.049786701798439026, 0.016425445675849915, 0.007716245483607054, 0.015725821256637573, 0.0018004784360527992, 0.008244374766945839, 0.015240434557199478, 0.017573976889252663, 0.002058529993519187, 0.0052136643789708614, 0.010317180305719376, 0.0026143123395740986, 0.002072887495160103, 0.005044214427471161, 0.003275044495239854, 0.07938694953918457, 0.7575036883354187], [0.008052660152316093, 0.001641637645661831, 0.0005253698909655213, 0.0013024486834183335, 0.0003496478602755815, 0.0010335881961509585, 0.0006476143025793135, 0.000687606050632894, 0.0004416880547069013, 0.0020642862655222416, 0.0009231442818418145, 0.0007652255590073764, 0.0008097232785075903, 0.0010170312598347664, 0.0021951645612716675, 0.00474684638902545, 0.9727963805198669]], [[0.024067481979727745, 0.03561197966337204, 0.008676915429532528, 0.008167261257767677, 0.00476675434038043, 0.006088973954319954, 0.005615551955997944, 0.0012913034297525883, 0.0032576655503362417, 0.0030977849382907152, 0.002951903035864234, 0.0011815287871286273, 0.0019248861353844404, 0.003938178066164255, 0.00505078723654151, 0.03662769868969917, 0.847683310508728], [0.008785175159573555, 0.11270586401224136, 0.004120354540646076, 0.00439401576295495, 0.0019753770902752876, 6.946374924154952e-05, 0.0014031428145244718, 7.034291047602892e-05, 0.0004944932297803462, 3.890432708431035e-05, 0.0001248885237146169, 9.862832666840404e-05, 6.489504357887199e-06, 1.2986357432964724e-05, 0.00029582573915831745, 0.007007763255387545, 0.8583962917327881], [0.011203542351722717, 0.269674688577652, 0.012052922509610653, 0.016930481418967247, 0.013246152549982071, 0.00020925782155245543, 0.001398220076225698, 0.0005563817685469985, 0.00045268170651979744, 0.0005014432244934142, 0.0001263193116756156, 2.593764838820789e-05, 4.853060454479419e-05, 0.00010415188444312662, 0.000526167917996645, 0.014031864702701569, 0.6589112877845764], [0.023263156414031982, 0.32023483514785767, 0.06974878162145615, 0.046731866896152496, 0.013681276701390743, 0.0003818380064330995, 0.0008999439887702465, 0.0010221555130556226, 0.00015586063091177493, 8.22214933577925e-05, 0.00011240704043302685, 8.737378811929375e-05, 7.787763934175018e-06, 1.6630936443107203e-05, 0.0003318636736366898, 0.01745947264134884, 0.5057825446128845], [0.007064571604132652, 0.010157937183976173, 0.006507080979645252, 0.8747283220291138, 0.006069924216717482, 0.005209918133914471, 0.007117006927728653, 0.008976145647466183, 0.000132238696096465, 0.00010623296839185059, 8.858413639245555e-05, 3.769639806705527e-05, 4.6442553866654634e-05, 2.785683136607986e-05, 0.00010174330964218825, 0.0017924472922459245, 0.07183574140071869], [0.018122557550668716, 0.05842170491814613, 0.03698692470788956, 0.4989430606365204, 0.1369805485010147, 0.00986179243773222, 0.005172155797481537, 0.003913907799869776, 0.0005147047922946513, 0.0023929812014102936, 0.0007421693881042302, 7.435533916577697e-05, 3.569496766431257e-05, 4.127678039367311e-05, 0.0007880618213675916, 0.007326433900743723, 0.21968156099319458], [0.017318615689873695, 0.01088175643235445, 0.019395679235458374, 0.07331077754497528, 0.02137276902794838, 0.06608278304338455, 0.020749205723404884, 0.0027111293748021126, 0.007918043062090874, 0.013284377753734589, 0.00036858191015198827, 3.304250640212558e-05, 2.5306340830866247e-05, 1.3179412235331256e-05, 0.00013773095270153135, 0.01198167260736227, 0.7344152927398682], [0.08383630216121674, 0.0016426597721874714, 0.009020552970468998, 0.018088197335600853, 0.0035224054008722305, 0.29675495624542236, 0.18890272080898285, 0.028603054583072662, 0.0007660193368792534, 0.0020167501643300056, 0.0008831180166453123, 0.00023460114607587457, 3.6582376196747646e-05, 3.097578519373201e-05, 0.0002593361714389175, 0.032089121639728546, 0.33331263065338135], [0.015988782048225403, 0.00012630152923520654, 0.001209674752317369, 0.01620764285326004, 0.0009393423679284751, 0.0025816881097853184, 0.003529947716742754, 0.9462544322013855, 3.3013333450071514e-05, 0.0003742929548025131, 4.5125252654543146e-05, 7.893932342994958e-05, 1.9664341380121186e-05, 5.715548468288034e-06, 0.0004979305667802691, 0.0012711914023384452, 0.010836337693035603], [0.024816177785396576, 0.0012278046924620867, 0.0007003176142461598, 0.0030744161922484636, 0.0007304172031581402, 0.007099330425262451, 0.003918912261724472, 0.021352922543883324, 0.006698106415569782, 0.003641329240053892, 0.0005260410252958536, 0.0006859246059320867, 0.0003484590561129153, 0.00024209187540691346, 0.0007278224802576005, 0.006264757830649614, 0.9179452657699585], [0.03679521381855011, 0.0018484678585082293, 0.001141043147072196, 0.004617634695023298, 0.004106698092073202, 0.008078535087406635, 0.007718685548752546, 0.0012594442814588547, 0.012957357801496983, 0.02660105563700199, 0.002740942407399416, 0.0005845068953931332, 0.0008779492927715182, 0.0008425581036135554, 0.0012050237273797393, 0.006676863878965378, 0.8819481134414673], [0.02029181458055973, 0.00010490897693671286, 0.00045597119606100023, 0.00019928286201320589, 0.00014264097262639552, 3.602815559133887e-05, 0.00010122247476829216, 4.9172194849234074e-05, 0.00018604173965286463, 0.049919214099645615, 0.19339175522327423, 0.002816419815644622, 0.001141643151640892, 0.0008870321325957775, 0.0028322888538241386, 0.0027640259359031916, 0.7246805429458618], [0.05548655986785889, 0.0014690598472952843, 0.0002199885930167511, 0.0008611898520030081, 5.72462631680537e-05, 0.0003984856011811644, 0.0019369293004274368, 0.0010553945321589708, 0.0029188229236751795, 0.09669698029756546, 0.007681556511670351, 0.022133151069283485, 0.00602797232568264, 0.0029597689863294363, 0.002674869494512677, 0.01866496354341507, 0.7787570953369141], [0.049481023102998734, 0.0012470785295590758, 0.00019596362835727632, 0.0025664479471743107, 5.79562401981093e-05, 0.0005087655154056847, 0.0020777189638465643, 0.001552756642922759, 0.00181230076123029, 0.08282142877578735, 0.00962011981755495, 0.0368506982922554, 0.021981719881296158, 0.007083170115947723, 0.003387412754818797, 0.02477702684700489, 0.7539783716201782], [0.07350295037031174, 0.010882590897381306, 0.005716607440263033, 0.007775898091495037, 0.00432176236063242, 0.0005978154367767274, 0.0007831767434254289, 0.002774959895759821, 0.0008278675377368927, 0.04624493420124054, 0.007911826483905315, 0.02920175902545452, 0.017983876168727875, 0.03378475829958916, 0.04829229786992073, 0.028502941131591797, 0.6808940768241882], [0.056677721440792084, 0.005245707929134369, 0.0032058923970907927, 0.0014099881518632174, 0.002019039820879698, 0.0011592970695346594, 0.003201681887730956, 0.0009642616496421397, 0.0011122857686132193, 0.0037784804590046406, 0.004569004289805889, 0.0007504411041736603, 0.001806640182621777, 0.0038714795373380184, 0.011710875667631626, 0.081196129322052, 0.8173209428787231], [0.015438254922628403, 0.011475374922156334, 0.01969204656779766, 0.006866198964416981, 0.009962303563952446, 0.0036385776475071907, 0.0028672805055975914, 0.001100827124901116, 0.008253338746726513, 0.005933691281825304, 0.010637558996677399, 0.002540069865062833, 0.005332556087523699, 0.009609995409846306, 0.005207560025155544, 0.024944545701146126, 0.8564997911453247]], [[0.016470788046717644, 0.28934741020202637, 0.012309059500694275, 0.032369181513786316, 0.011237096972763538, 0.014753776602447033, 0.008395478129386902, 0.014904964715242386, 0.004913416691124439, 0.021934321150183678, 0.003012211760506034, 0.0010912858415395021, 0.0005473171477206051, 0.0011506895534694195, 0.019585251808166504, 0.036678116768598557, 0.5112996101379395], [0.006480686832219362, 0.0741521492600441, 0.02390025369822979, 0.6297125816345215, 0.10082850605249405, 0.029679620638489723, 0.014452075585722923, 0.002584591507911682, 0.007906036451458931, 0.0052725160494446754, 0.0005266429507173598, 0.00034649120061658323, 9.37662334763445e-05, 0.00011107366299256682, 0.0017432761378586292, 0.007629338651895523, 0.09458024799823761], [0.014803779311478138, 0.10510604828596115, 0.04327753186225891, 0.2426925003528595, 0.07779235392808914, 0.0791025161743164, 0.0670306533575058, 0.023107437416911125, 0.011489960364997387, 0.012907488271594048, 0.004159517120569944, 0.00045357877388596535, 0.0004921337822452188, 0.0008685674401931465, 0.001223016413860023, 0.014572173357009888, 0.3009207248687744], [0.04444687440991402, 0.00305939675308764, 0.010310526937246323, 0.010181396268308163, 0.013936436735093594, 0.3614965081214905, 0.17582710087299347, 0.05557971075177193, 0.03272133693099022, 0.04481758922338486, 0.002361063379794359, 0.00022319448180496693, 0.000396645104046911, 0.0005707591772079468, 0.005554502829909325, 0.01759491302073002, 0.22092211246490479], [0.04150281846523285, 0.010620266199111938, 0.010797388851642609, 0.019386854022741318, 0.011005358770489693, 0.33491259813308716, 0.33615800738334656, 0.014535107649862766, 0.030589429661631584, 0.04059286043047905, 0.002116945805028081, 4.8358433559769765e-05, 0.00014481371908914298, 0.0002902830601669848, 0.002599315019324422, 0.019176634028553963, 0.12552295625209808], [0.07325704395771027, 0.0013123584212735295, 0.003645184449851513, 0.0005473289056681097, 0.001043717609718442, 0.004997852724045515, 0.06794946640729904, 0.7032713294029236, 0.013881606981158257, 0.010313853621482849, 0.0020174230448901653, 0.000301490566926077, 0.00020845187827944756, 0.00011852179159177467, 0.0029540699906647205, 0.04367000237107277, 0.07051033526659012], [0.08821768313646317, 0.004700771998614073, 0.00853229034692049, 0.013892216607928276, 0.013848280534148216, 0.030955612659454346, 0.11723539233207703, 0.25964176654815674, 0.05172102153301239, 0.07560435682535172, 0.0037329429760575294, 0.002098125172778964, 0.0009501984459348023, 0.0008207624196074903, 0.010316893458366394, 0.032060232013463974, 0.2856714725494385], [0.013377673923969269, 0.0030074859969317913, 0.001557414885610342, 0.0015529432566836476, 0.008088164031505585, 0.3280850052833557, 0.018654100596904755, 0.010059867054224014, 0.07898614555597305, 0.4133470356464386, 0.001780473510734737, 0.0002616936981212348, 0.0015642688376829028, 0.0014124775771051645, 0.007009670604020357, 0.003620804287493229, 0.10763470828533173], [0.014394348487257957, 0.0011713112471625209, 0.001056585693731904, 0.0005183813627809286, 0.0018959884764626622, 0.018620895221829414, 0.023981956765055656, 0.00036199900205247104, 0.017304018139839172, 0.5449223518371582, 0.007730658631771803, 0.000270297285169363, 0.03141051530838013, 0.02304355800151825, 0.0030158411245793104, 0.008313456550240517, 0.30198779702186584], [0.05131518840789795, 0.0002135911927325651, 0.00033579368027858436, 0.00020750057592522353, 0.00010102908709086478, 0.0017677785363048315, 0.004724698141217232, 0.0007761328015476465, 0.0007338287541642785, 0.005942068994045258, 0.002575997496023774, 0.0010983115062117577, 0.0024693210143595934, 0.00180917140096426, 0.03804060444235802, 0.02502751722931862, 0.8628613948822021], [0.05605756863951683, 0.004289919510483742, 0.0022040270268917084, 0.003179802093654871, 0.0005001290119253099, 0.0054018315859138966, 0.0009200917556881905, 0.0021097999997437, 0.0021686775144189596, 0.02780819684267044, 0.01113178115338087, 0.2507185935974121, 0.00867532193660736, 0.011060128919780254, 0.07040538638830185, 0.05728670582175255, 0.48608213663101196], [0.009092360734939575, 0.0005566697800531983, 0.0001037281472235918, 0.00011547424946911633, 3.261670281062834e-05, 0.0003481395251583308, 5.992062506265938e-05, 8.041637920541689e-05, 2.2958674890105613e-05, 0.003018841380253434, 0.0007340130396187305, 0.006035650614649057, 0.006396256387233734, 0.014372620731592178, 0.07455120980739594, 0.004140337463468313, 0.880338728427887], [0.024678794667124748, 0.0004823776544071734, 0.0002069379697786644, 0.0004080796497873962, 0.00010690096678445116, 0.000928822613786906, 5.5769443861208856e-05, 0.00030163285555318, 0.0003258048091083765, 0.006328501272946596, 0.002316597616299987, 0.06646808236837387, 0.01360476016998291, 0.018629135563969612, 0.409790962934494, 0.021326666697859764, 0.4340401589870453], [0.023666994646191597, 0.00023414687893819064, 0.00024375697830691934, 0.0003704549162648618, 9.263343235943466e-05, 0.0008184369071386755, 9.86199956969358e-05, 0.00011682356125675142, 0.00019054714357480407, 0.0033937408588826656, 0.0017905909335240722, 0.03470601886510849, 0.009213464334607124, 0.010493788868188858, 0.4007819890975952, 0.017895014956593513, 0.49589303135871887], [0.005583146587014198, 0.00040682643884792924, 0.00021035697136539966, 0.0004464970843400806, 0.00018436148820910603, 0.00019780686125159264, 0.00031093458528630435, 0.0008275725995190442, 7.294618990272284e-05, 0.0015357446391135454, 0.0006049470393918455, 0.004572230391204357, 0.0028204701375216246, 0.007474154233932495, 0.08779314160346985, 0.0030850665643811226, 0.8838738203048706], [0.03922688961029053, 0.24791596829891205, 0.009705889970064163, 0.05390043184161186, 0.008594576269388199, 0.02341797947883606, 0.017279695719480515, 0.020554816350340843, 0.004564253613352776, 0.010664375498890877, 0.0022968342527747154, 0.0014319452457129955, 0.0012380037223920226, 0.0025825798511505127, 0.032150544226169586, 0.07524154335260391, 0.44923368096351624], [0.0037323867436498404, 0.0008540780982002616, 0.0002478575916029513, 0.00039739516796544194, 0.00015735147462692112, 0.00039569285581819713, 0.0007912970031611621, 0.00028909393586218357, 0.00019473517022561282, 0.0012643177760764956, 0.0002392559836152941, 0.0002955910749733448, 0.00023010119912214577, 0.0003149309486616403, 0.0020104870200157166, 0.0023337577003985643, 0.9862517714500427]], [[0.03883545100688934, 0.027781406417489052, 0.022360432893037796, 0.10461895912885666, 0.018359296023845673, 0.03793235868215561, 0.016277773305773735, 0.03256237506866455, 0.019813090562820435, 0.0728364884853363, 0.023782288655638695, 0.025297803804278374, 0.005956828128546476, 0.015188811346888542, 0.03531826660037041, 0.018237587064504623, 0.4848408102989197], [0.0063619185239076614, 0.03333969786763191, 0.014679701067507267, 0.06295923888683319, 0.03994140028953552, 0.09741915762424469, 0.04649851843714714, 0.10326356440782547, 0.06956804543733597, 0.19666558504104614, 0.053934141993522644, 0.07804615795612335, 0.006225711666047573, 0.012896373867988586, 0.07216883450746536, 0.04286015406250954, 0.06317181140184402], [0.01163443922996521, 0.032241519540548325, 0.024450166150927544, 0.05553201958537102, 0.021387992426753044, 0.07865176349878311, 0.03991985693573952, 0.05046823248267174, 0.037595272064208984, 0.2020791918039322, 0.16504091024398804, 0.06042921170592308, 0.011102985590696335, 0.019250724464654922, 0.0780915841460228, 0.056549664586782455, 0.05557442829012871], [0.013117981143295765, 0.01478557474911213, 0.008256216533482075, 0.01616515964269638, 0.005629850085824728, 0.031132696196436882, 0.05237872153520584, 0.08473116159439087, 0.042033858597278595, 0.20926719903945923, 0.08540885150432587, 0.0655445009469986, 0.021268585696816444, 0.029292164370417595, 0.22057810425758362, 0.037897683680057526, 0.06251176446676254], [0.007153555750846863, 0.007259890902787447, 0.0038844908121973276, 0.008756733499467373, 0.002872855868190527, 0.019007930532097816, 0.04217654839158058, 0.06693743169307709, 0.04271850734949112, 0.2504535913467407, 0.07490330189466476, 0.05874345079064369, 0.019246211275458336, 0.03086978942155838, 0.2673821747303009, 0.03523445129394531, 0.062399111688137054], [0.010900197550654411, 0.008160545490682125, 0.003074036445468664, 0.0069555058144032955, 0.003856335999444127, 0.013360901735723019, 0.009254319593310356, 0.14596611261367798, 0.06206168234348297, 0.13671986758708954, 0.07222273200750351, 0.05633636936545372, 0.030406862497329712, 0.046518292278051376, 0.19325320422649384, 0.022444214671850204, 0.17850889265537262], [0.014600584283471107, 0.0064902761951088905, 0.0018801302649080753, 0.01447492279112339, 0.002456799615174532, 0.01801454648375511, 0.01284842286258936, 0.029089804738759995, 0.04494209587574005, 0.1358407884836197, 0.07422793656587601, 0.08883003145456314, 0.023832401260733604, 0.04215403273701668, 0.06239554286003113, 0.054810117930173874, 0.3731115460395813], [0.029715606942772865, 0.008534843102097511, 0.003725753165781498, 0.00533660501241684, 0.0012726826826110482, 0.009333314374089241, 0.02227739244699478, 0.02955578826367855, 0.042761459946632385, 0.13157781958580017, 0.04576140642166138, 0.023878060281276703, 0.026559550315141678, 0.03403941169381142, 0.08457732200622559, 0.07758023589849472, 0.423512727022171], [0.02726714126765728, 0.00825037993490696, 0.0022893077693879604, 0.006795735098421574, 0.0010632312623783946, 0.004880582448095083, 0.012307566590607166, 0.012303714640438557, 0.01056733913719654, 0.12797722220420837, 0.0441528782248497, 0.08498905599117279, 0.038333069533109665, 0.06722920387983322, 0.22449080646038055, 0.07993447780609131, 0.24716825783252716], [0.01804475300014019, 0.017333103343844414, 0.0024143322370946407, 0.01020197756588459, 0.00440467381849885, 0.009746922180056572, 0.004427798558026552, 0.02491156943142414, 0.008210479281842709, 0.06474681943655014, 0.02307041361927986, 0.0839984193444252, 0.020091261714696884, 0.03301109001040459, 0.11492301523685455, 0.02056046389043331, 0.5399028658866882], [0.022076960653066635, 0.026833461597561836, 0.006644336972385645, 0.014635232277214527, 0.003987881820648909, 0.008543213829398155, 0.010681227780878544, 0.015407655388116837, 0.006740749813616276, 0.05656607449054718, 0.0379745177924633, 0.09919118881225586, 0.012208598665893078, 0.023952582851052284, 0.09617980569601059, 0.09818414598703384, 0.46019241213798523], [0.011009393259882927, 0.006490473635494709, 0.0013827861985191703, 0.00586833106353879, 0.001102360663935542, 0.0020982208661735058, 0.002143086399883032, 0.00835997611284256, 0.0018079481087625027, 0.032175373286008835, 0.013979348354041576, 0.04269183799624443, 0.008122445084154606, 0.015162749215960503, 0.060787804424762726, 0.028499305248260498, 0.7583184838294983], [0.015791531652212143, 0.006164731457829475, 0.0016459228936582804, 0.005678542423993349, 0.001115045859478414, 0.001790871494449675, 0.003788412781432271, 0.003307587932795286, 0.0017668029759079218, 0.023685535416007042, 0.015497013926506042, 0.10349790751934052, 0.011038118042051792, 0.021267198026180267, 0.06463208794593811, 0.039426255971193314, 0.6799064874649048], [0.017866959795355797, 0.006661546882241964, 0.002013207646086812, 0.0064693414606153965, 0.0012561476323753595, 0.002339465543627739, 0.003934792242944241, 0.004979168996214867, 0.0020365656819194555, 0.031086336821317673, 0.013047004118561745, 0.1068178191781044, 0.010742295533418655, 0.018449433147907257, 0.09709669649600983, 0.04413271322846413, 0.6310704946517944], [0.008996978402137756, 0.005194924306124449, 0.0013587448047474027, 0.00483305100351572, 0.0014278556918725371, 0.0031287213787436485, 0.0028402721509337425, 0.00748366629704833, 0.0037897806614637375, 0.022900186479091644, 0.005322813987731934, 0.017203446477651596, 0.006515969522297382, 0.014203310944139957, 0.02755437232553959, 0.008661845698952675, 0.8585841059684753], [0.029203956946730614, 0.055195923894643784, 0.015671521425247192, 0.061408985406160355, 0.01792609505355358, 0.039045099169015884, 0.019119640812277794, 0.03744610399007797, 0.027723614126443863, 0.047275759279727936, 0.022047903388738632, 0.0431230254471302, 0.009610826149582863, 0.024917462840676308, 0.03953854367136955, 0.05994020029902458, 0.4508053660392761], [0.008137522265315056, 0.0006376779638230801, 0.00016789452638477087, 0.0004941577208228409, 0.00012959216837771237, 0.00048728843103162944, 0.0005579143762588501, 0.0014749132096767426, 0.0007612481131218374, 0.004437213763594627, 0.001418664469383657, 0.0018322666874155402, 0.0010539306094869971, 0.0016894052969291806, 0.005987223237752914, 0.004139684606343508, 0.9665935039520264]], [[0.039022982120513916, 0.09050805121660233, 0.03144441172480583, 0.014156716875731945, 0.004405316896736622, 0.008775999769568443, 0.0005281036137603223, 0.007541451137512922, 0.008149751462042332, 0.01863258145749569, 0.001643161871470511, 0.0038615099620074034, 0.00039801435195840895, 0.00023519860405940562, 0.005908399820327759, 0.08242882043123245, 0.6823596358299255], [0.00846058875322342, 0.0444200336933136, 0.4939686954021454, 0.04422243312001228, 0.009550798684358597, 0.005923261400312185, 0.0025121194776147604, 0.0024058676790446043, 0.0029343587812036276, 0.002551139099523425, 0.0007799703162163496, 0.0007390239043161273, 0.0002965204184874892, 0.000333145639160648, 0.006314103491604328, 0.0039046802558004856, 0.3706832826137543], [0.001676053972914815, 0.009300068952143192, 0.008466493338346481, 0.4642044007778168, 0.01846941001713276, 0.09243042767047882, 0.0006848702905699611, 0.000514480983838439, 0.00038151812623254955, 0.005475728772580624, 6.699706136714667e-05, 0.000154079360072501, 9.701047019916587e-06, 1.683887603576295e-05, 0.00033684674417600036, 0.0007264750893227756, 0.3970855474472046], [0.009486664086580276, 0.006531561724841595, 0.017082579433918, 0.02968515083193779, 0.24784281849861145, 0.25420087575912476, 0.0103668924421072, 0.032552607357501984, 0.04594757780432701, 0.009979886002838612, 0.0003514892014209181, 0.0002237058215541765, 2.7176225557923317e-05, 5.64054207643494e-05, 0.0015250821597874165, 0.0036817470099776983, 0.3304577171802521], [0.00035457045305520296, 0.004372118506580591, 0.002453903667628765, 0.09128512442111969, 0.011644264683127403, 0.8039302825927734, 0.0028058583848178387, 0.0033568134531378746, 0.0003277497598901391, 0.007383049465715885, 6.113225390436128e-05, 0.00011608909699134529, 8.928421380005602e-07, 2.885518142647925e-06, 0.0015717920614406466, 0.0003523902269080281, 0.06998106092214584], [0.008772080764174461, 0.0003606255049817264, 0.0013935051392763853, 0.003380538895726204, 0.015530286356806755, 0.015379616990685463, 0.127900630235672, 0.551128625869751, 0.11601845920085907, 0.010576649568974972, 0.0025560182984918356, 0.001214248826727271, 0.00010874717554543167, 0.0001453214936191216, 0.009212891571223736, 0.001873559202067554, 0.1344481110572815], [0.0004110308364033699, 0.00038573669735342264, 0.0006492347456514835, 0.0021598273888230324, 0.0017025937559083104, 0.04571187496185303, 0.01080308761447668, 0.8877814412117004, 0.02802928350865841, 0.006124412175267935, 0.00030008016619831324, 0.0005278546013869345, 2.7913469239138067e-05, 3.251969610573724e-05, 0.0016269373008981347, 0.0003943338233511895, 0.01333177275955677], [0.0009083556360565126, 7.891531276982278e-05, 0.0016518643824383616, 7.151706086006016e-05, 0.0003895658883266151, 0.0005158314015716314, 0.00026849727146327496, 0.008383792825043201, 0.9760134220123291, 0.003435820806771517, 0.00018553901463747025, 4.083415478817187e-05, 1.2651653378270566e-05, 1.1659418305498548e-05, 0.00036618672311306, 7.326954801101238e-05, 0.007592342793941498], [0.0012550498358905315, 0.0016536604380235076, 0.0016215125797316432, 0.001102046575397253, 0.00011561062274267897, 0.019525187090039253, 0.0005386578268371522, 0.0015475568361580372, 0.006542537361383438, 0.6997316479682922, 0.002208596095442772, 0.0018605515360832214, 4.441952478373423e-05, 3.26080116792582e-05, 0.004173867870122194, 0.0006383638828992844, 0.2574080526828766], [0.006322016473859549, 0.0004131128662265837, 0.0003038198919966817, 0.00018775707576423883, 4.996392453904264e-05, 0.0006436211406253278, 0.0008264881325885653, 0.0057791187427937984, 0.001985972747206688, 0.005752430763095617, 0.015111610293388367, 0.05455809459090233, 0.002021204447373748, 0.002501256065443158, 0.042942289263010025, 0.0005690823891200125, 0.8600320219993591], [0.00021662846847902983, 0.00039293462759815156, 3.005356302310247e-05, 3.081118848058395e-05, 2.305069983776775e-06, 0.00013563492393586785, 3.408913471503183e-05, 9.440684516448528e-05, 0.00010204358113696799, 0.0010388294467702508, 0.004152536857873201, 0.9075651168823242, 0.0010429585818201303, 0.0017776151653379202, 0.020911235362291336, 9.664498793426901e-05, 0.062376245856285095], [0.0033911378122866154, 0.0006830201018601656, 0.0009636367321945727, 0.0008388501591980457, 0.0004470873100217432, 0.0008459644159302115, 4.6599918277934194e-05, 0.0002202815085183829, 0.00635450379922986, 0.007707116659730673, 0.002893417375162244, 0.010809932835400105, 0.18264104425907135, 0.37119370698928833, 0.3054748475551605, 0.004701010417193174, 0.10078790783882141], [0.00012824196892324835, 9.968596714315936e-05, 0.00030622302438132465, 0.0001403158821631223, 2.5186623133777175e-06, 3.822354119620286e-05, 2.64684440480778e-05, 1.5260828149621375e-05, 2.3853028324083425e-05, 0.0007090254221111536, 0.0008070345502346754, 0.021331777796149254, 0.0010212190682068467, 0.004284718073904514, 0.9691344499588013, 0.0006822579307481647, 0.001248698914423585], [5.3717216360382736e-05, 7.140229718061164e-05, 0.00015660618373658508, 0.00011279930185992271, 2.3043160126690054e-06, 7.099489448592067e-05, 1.898776099551469e-05, 2.3564598450320773e-05, 1.0676990314095747e-05, 0.0007166521390900016, 0.0004584754351526499, 0.007637698203325272, 0.00014798004121985286, 0.00033297843765467405, 0.9873327612876892, 0.0004165146965533495, 0.002435956383123994], [0.14426012337207794, 0.0009150656405836344, 0.0008835524204187095, 0.0008496996015310287, 0.0006270267767831683, 0.00046669255243614316, 5.408382276073098e-05, 0.00010935885802609846, 0.0007881241617724299, 0.00029875957989133894, 5.397468703449704e-05, 0.00013375715934671462, 0.000481362541904673, 0.00036050743074156344, 0.0028454558923840523, 0.135772243142128, 0.7111002206802368], [0.01704365201294422, 0.07668028771877289, 0.026589863002300262, 0.009424609132111073, 0.0020965037401765585, 0.004929926712065935, 0.000699985190294683, 0.0035117417573928833, 0.0033825919963419437, 0.014255562797188759, 0.0014984158333390951, 0.013584211468696594, 0.0006969849928282201, 0.0008206429774872959, 0.034908514469861984, 0.08572902530431747, 0.704147458076477], [0.22227902710437775, 0.0023621281143277884, 0.006768378894776106, 0.0036711529828608036, 0.003946430049836636, 0.0030953502282500267, 0.0019395550480112433, 0.0037635990884155035, 0.007604321464896202, 0.0057543618604540825, 0.005985395982861519, 0.008202862925827503, 0.008393160998821259, 0.008389059454202652, 0.01949598826467991, 0.07182177156209946, 0.6165274977684021]], [[0.02380971424281597, 0.01035622600466013, 0.005495659541338682, 0.006853699684143066, 0.005275770556181669, 0.006712428759783506, 0.003464309498667717, 0.004518606700003147, 0.0021455418318510056, 0.02590704709291458, 0.008582928217947483, 0.011830425821244717, 0.002482497598975897, 0.0032143942080438137, 0.033869002014398575, 0.06888645887374878, 0.7765952348709106], [0.4026724696159363, 0.048935264348983765, 0.02655475214123726, 0.017273621633648872, 0.002312740311026573, 0.0007876770105212927, 0.0011692113475874066, 0.0007277245749719441, 0.0003302357508800924, 0.00045061594573780894, 0.0008116852259263396, 0.0009629279957152903, 8.144242747221142e-05, 0.0001746313355397433, 0.0021943245083093643, 0.11771978437900543, 0.37684088945388794], [0.17836642265319824, 0.44437822699546814, 0.04201103746891022, 0.04557722434401512, 0.01093180663883686, 0.0075441268272697926, 0.007088980637490749, 0.003599810181185603, 0.0024914394598454237, 0.002079733880236745, 0.0012561019975692034, 0.002350297523662448, 0.00036893950891681015, 0.0008551114005967975, 0.005451761651784182, 0.03580057993531227, 0.20984837412834167], [0.1255873143672943, 0.22541414201259613, 0.19718976318836212, 0.03321708366274834, 0.09621488302946091, 0.02658895216882229, 0.005324491299688816, 0.0017578683327883482, 0.0020375640597194433, 0.0021721881348639727, 0.00028296394157223403, 0.0037140892818570137, 0.0001219877740368247, 0.00030046445317566395, 0.004929755814373493, 0.028066694736480713, 0.24707981944084167], [0.023271821439266205, 0.14152878522872925, 0.3879644274711609, 0.2098972648382187, 0.07592003792524338, 0.06513231992721558, 0.007076341658830643, 0.0024640089832246304, 0.00032623845618218184, 0.0004061715735588223, 5.550151036004536e-05, 0.0009075900889001787, 1.938637433340773e-05, 3.221224687877111e-05, 0.0014365523820742965, 0.0059015341103076935, 0.07765984535217285], [0.030308503657579422, 0.07200773805379868, 0.16675151884555817, 0.3646290600299835, 0.1986560821533203, 0.044062741100788116, 0.03034430742263794, 0.010237227194011211, 0.004072198178619146, 0.0016060823109000921, 0.00044885717215947807, 0.0029412962030619383, 4.6543587814085186e-05, 9.872080408968031e-05, 0.0016422081971541047, 0.008170702494680882, 0.06397629529237747], [0.029124855995178223, 0.055422343313694, 0.07398447394371033, 0.16122877597808838, 0.1578240990638733, 0.19116459786891937, 0.010267055593430996, 0.04131051152944565, 0.00741621945053339, 0.016594750806689262, 0.0005065584555268288, 0.0002855536586139351, 8.372818410862237e-05, 0.00010677404497982934, 0.001153913326561451, 0.01019889023154974, 0.24332691729068756], [0.00679265521466732, 0.0035749420057982206, 0.009220081381499767, 0.038915738463401794, 0.06290601193904877, 0.4888816177845001, 0.22349761426448822, 0.01565488800406456, 0.005953880026936531, 0.022709611803293228, 0.003075088607147336, 0.002216566354036331, 0.0006045944173820317, 0.0003481235180515796, 0.0005040682735852897, 0.0017863208195194602, 0.11335816979408264], [0.02198404259979725, 0.0036726815160363913, 0.011355101130902767, 0.041463740170001984, 0.015224475413560867, 0.27697619795799255, 0.4280179440975189, 0.15494415163993835, 0.0038739859592169523, 0.0064885965548455715, 0.0024833609350025654, 0.0009085146011784673, 0.0001163448832812719, 8.884118142304942e-05, 0.0004518504720181227, 0.0021554012782871723, 0.02979477494955063], [0.05154566466808319, 0.017039505764842033, 0.03870552033185959, 0.025335660204291344, 0.010083958506584167, 0.032326143234968185, 0.03818246349692345, 0.2954295575618744, 0.07994119077920914, 0.007207845337688923, 0.009069307707250118, 0.009455880150198936, 0.00036400320823304355, 0.0005003598053008318, 0.004256625659763813, 0.007162705063819885, 0.37339359521865845], [0.07402043044567108, 0.02391628362238407, 0.013387381099164486, 0.01457536593079567, 0.007346701342612505, 0.07614608854055405, 0.006711955647915602, 0.11591356247663498, 0.09604129195213318, 0.1008497104048729, 0.025893939658999443, 0.009660023264586926, 0.0008926859009079635, 0.0008267365046776831, 0.005626305006444454, 0.007193343713879585, 0.42099815607070923], [0.05323365703225136, 0.009054401889443398, 0.0021417567040771246, 0.001878722570836544, 0.0015646431129425764, 0.003856109222397208, 0.0035125375725328922, 0.004521287512034178, 0.008031917735934258, 0.033403102308511734, 0.15699635446071625, 0.02339543215930462, 0.011667522601783276, 0.015046551823616028, 0.005650550592690706, 0.0149899423122406, 0.6510555148124695], [0.02718021720647812, 0.0021177998278290033, 0.0016470177797600627, 0.001213879557326436, 0.00029192224610596895, 0.0019375371048226953, 0.001169794355519116, 0.0018820929108187556, 0.0022328963968902826, 0.07345513999462128, 0.10692272335290909, 0.5804518461227417, 0.017978327348828316, 0.010714568197727203, 0.039136771112680435, 0.009738406166434288, 0.1219291016459465], [0.05505668371915817, 0.003051629289984703, 0.002066380111500621, 0.0018423751462250948, 0.0003622168442234397, 0.003194499993696809, 0.0008936271769925952, 0.0019166205311194062, 0.0014266346115618944, 0.05791415646672249, 0.09034443646669388, 0.5462787747383118, 0.023852301761507988, 0.01305278018116951, 0.030704205855727196, 0.019307715818285942, 0.14873504638671875], [0.03985365107655525, 0.0031697992235422134, 0.0018936374690383673, 0.01001930795609951, 0.001818849123083055, 0.014697377569973469, 0.016397841274738312, 0.007094469852745533, 0.00565844913944602, 0.09557388722896576, 0.06884012371301651, 0.228638157248497, 0.20277139544487, 0.17071090638637543, 0.0272359699010849, 0.015577859245240688, 0.09004821628332138], [0.028412429615855217, 0.006153356749564409, 0.0026338710449635983, 0.006791676394641399, 0.0027731170412153006, 0.004310999996960163, 0.0029336626175791025, 0.001657856977544725, 0.0015158896567299962, 0.05169868469238281, 0.00813291035592556, 0.05498296022415161, 0.020962093025445938, 0.02026277408003807, 0.14047153294086456, 0.11470858752727509, 0.5315975546836853], [0.016530372202396393, 0.0034407489001750946, 0.003003294812515378, 0.0033561978489160538, 0.003967819735407829, 0.004829411394894123, 0.0036597358994185925, 0.005125901661813259, 0.003181151580065489, 0.013612605631351471, 0.006566177122294903, 0.004026471171528101, 0.0047339750453829765, 0.0064054131507873535, 0.004351308569312096, 0.017986789345741272, 0.8952226042747498]], [[0.02761245332658291, 0.019685430452227592, 0.018588559702038765, 0.022454291582107544, 0.005464993882924318, 0.005499798804521561, 0.006343455519527197, 0.004322887398302555, 0.0010704591404646635, 0.001640694565139711, 0.000894200406037271, 0.0004966880078427494, 0.00013534942991100252, 0.0002694400609470904, 0.0017166101606562734, 0.035100143402814865, 0.8487045168876648], [0.04661884903907776, 0.04967769980430603, 0.24614466726779938, 0.245499849319458, 0.009811848402023315, 0.038076002150774, 0.009735919535160065, 0.0021800759714096785, 0.0005802571540698409, 0.0016637778608128428, 0.005647334735840559, 0.001086383475922048, 2.830466110026464e-05, 3.228094647056423e-05, 0.0012212940491735935, 0.08901439607143402, 0.2529810667037964], [0.03936252370476723, 0.05590537562966347, 0.016939401626586914, 0.11089305579662323, 0.01146789826452732, 0.22148507833480835, 0.05071556940674782, 0.025444505736231804, 0.0014165272004902363, 0.006195642054080963, 0.009480467066168785, 0.0005150690558366477, 4.523820098256692e-05, 6.275517080212012e-05, 0.0008922626730054617, 0.051874078810214996, 0.3973046541213989], [0.04289646074175835, 0.31138065457344055, 0.024339426308870316, 0.014218471944332123, 0.024360667914152145, 0.06074460595846176, 0.03004889190196991, 0.022699806839227676, 0.0022066363599151373, 0.0004378857265692204, 0.005756292026489973, 0.000233640952501446, 0.0001409865217283368, 0.00017797076725400984, 0.0006194002344273031, 0.07061876356601715, 0.3891195058822632], [0.05708054080605507, 0.08214631676673889, 0.02155459299683571, 0.0070951590314507484, 0.003896255511790514, 0.05839157849550247, 0.0330168642103672, 0.06681212782859802, 0.010875741951167583, 0.005367904901504517, 0.010930544696748257, 0.00021753681357949972, 0.00015630859707016498, 0.00018859216652344912, 0.0027754148468375206, 0.09147690236568451, 0.5480176210403442], [0.2221316546201706, 0.04107039049267769, 0.003593974746763706, 0.003180255414918065, 0.0018176737939938903, 0.0060273450799286366, 0.020553279668092728, 0.0035764346830546856, 0.0025315519887953997, 0.007520868442952633, 0.00797161553055048, 0.00019401901226956397, 0.0003413059748709202, 0.0006722671678289771, 0.009302917867898941, 0.12513414025306702, 0.5443804264068604], [0.05496660992503166, 0.1506122499704361, 0.0012428066693246365, 0.0016262612771242857, 0.001803880906663835, 0.0017875520279631019, 0.006345843896269798, 0.02572588436305523, 0.026133757084608078, 0.009580686688423157, 0.012506324797868729, 0.0031491986010223627, 0.002394186332821846, 0.0035119052045047283, 0.012408260256052017, 0.032378900796175, 0.6538257598876953], [0.06541264802217484, 0.012747184373438358, 0.006471140310168266, 0.006970816291868687, 0.011911066249012947, 0.006798082962632179, 0.006726483348757029, 0.03281451389193535, 0.09223555773496628, 0.03758809715509415, 0.004703692626208067, 0.00217616674490273, 0.0008063812274485826, 0.0017238798318430781, 0.005115286912769079, 0.03236590698361397, 0.6734330654144287], [0.02829792909324169, 0.000926465610973537, 0.003255568677559495, 0.0009702633833512664, 0.0004940350772812963, 0.004115987569093704, 0.004532448947429657, 0.013191631995141506, 0.003262263024225831, 0.044129032641649246, 0.0036148757208138704, 0.0007656418601982296, 0.0005487192538566887, 0.0007302064914256334, 0.007345232646912336, 0.06040630489587784, 0.8234134912490845], [0.04795258492231369, 0.0012836999958381057, 0.007556273136287928, 0.00030650486587546766, 0.00015327501751016825, 0.002156044589355588, 0.013006830587983131, 0.00455428333953023, 0.0008843496907502413, 0.002340509556233883, 0.014643528498709202, 0.00599539652466774, 0.013671144843101501, 0.014698371291160583, 0.038072943687438965, 0.03704860433936119, 0.7956757545471191], [0.057506997138261795, 0.0023908617440611124, 0.0005434027989394963, 0.0014909320743754506, 0.0003752854245249182, 0.005478540901094675, 0.003945028875023127, 0.0023071204777806997, 0.0007601915276609361, 0.025422900915145874, 0.011634069494903088, 0.021873829886317253, 0.0982280969619751, 0.1490982621908188, 0.02410358190536499, 0.024276141077280045, 0.5705646872520447], [0.024793511256575584, 0.0005454757483676076, 0.00025754939997568727, 0.000394362403312698, 8.512351632816717e-05, 0.0005148519994691014, 0.0008743756916373968, 0.0009471039520576596, 3.978860331699252e-05, 0.003415136132389307, 0.0030017136596143246, 0.005356910172849894, 0.019468605518341064, 0.04051058739423752, 0.025704747065901756, 0.007672619074583054, 0.8664174675941467], [0.04650573804974556, 0.0043230620212852955, 0.001274971873499453, 0.0011039135279133916, 0.00036339901271276176, 0.0016912402352318168, 0.0014391274889931083, 0.0014010193990543485, 0.0007764357724227011, 0.006409448571503162, 0.004658623598515987, 0.008879111148416996, 0.055577002465724945, 0.055969901382923126, 0.04707101732492447, 0.025647150352597237, 0.7369087934494019], [0.038666605949401855, 0.0057732039131224155, 0.0011136363027617335, 0.0014488352462649345, 0.000393515860196203, 0.0008504357538186014, 0.0008144430466927588, 0.0018418784020468593, 0.0004715750110335648, 0.006949552334845066, 0.003863319056108594, 0.0063300058245658875, 0.02565263956785202, 0.028499986976385117, 0.026055864989757538, 0.01992616429924965, 0.8313485383987427], [0.01733800768852234, 0.006903090979903936, 0.002523178933188319, 0.004450348671525717, 0.00038858596235513687, 0.0016823486657813191, 0.003246638225391507, 0.0011202840832993388, 0.0008611160446889699, 0.006002870853990316, 0.0038968853186815977, 0.002948456211015582, 0.0007887521642260253, 0.0010959410574287176, 0.02579072304069996, 0.020857064053416252, 0.9001057744026184], [0.08628879487514496, 0.026132183149456978, 0.031837109476327896, 0.0351630300283432, 0.014374456368386745, 0.008968127891421318, 0.014025687240064144, 0.009385963901877403, 0.0036190517712384462, 0.0042864675633609295, 0.0013892750721424818, 0.000372252834495157, 0.00014564039884135127, 0.00021626226953230798, 0.002227107295766473, 0.11209146678447723, 0.6494771838188171], [0.01345259789377451, 0.012355928309261799, 0.005234953947365284, 0.003795132739469409, 0.001777579658664763, 0.0036537309642881155, 0.0030398203525692225, 0.005377291701734066, 0.002183085074648261, 0.0031375682447105646, 0.0036424051504582167, 0.003636626061052084, 0.003165364032611251, 0.0039230939000844955, 0.004248748533427715, 0.008978346362709999, 0.9183977246284485]]], "negativeColor": "#1f78b4", "positiveColor": "#e31a1c", "tokens": ["[CLS]", "then", "i", "tried", "to", "find", "some", "way", "of", "embracing", "my", "mother", "'", "s", "ghost", ".", "[SEP]"]}
    )
    </script></div></div>
</div>
<p>Take a look at head 3. There is a relatively strong relationship here between
“my” and “i”.</p>
<p>Finally, here is the last layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_heads</span><span class="p">(</span>
    <span class="n">attentions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">negative_color</span> <span class="o">=</span> <span class="s2">&quot;#1f78b4&quot;</span><span class="p">,</span>
    <span class="n">positive_color</span> <span class="o">=</span> <span class="s2">&quot;#e31a1c&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div id="circuits-vis-e147fc3a-5c24" style="margin: 15px 0;"/>
    <script crossorigin type="module">
    import { render, AttentionHeads } from "https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js";
    render(
      "circuits-vis-e147fc3a-5c24",
      AttentionHeads,
      {"attention": [[[0.10855787992477417, 0.03021479770541191, 0.05475052818655968, 0.04455363750457764, 0.014521567150950432, 0.015963982790708542, 0.03289621323347092, 0.033705759793519974, 0.02670327201485634, 0.08961420506238937, 0.007791650947183371, 0.005578245501965284, 0.0069292765110731125, 0.009098319336771965, 0.05866732820868492, 0.23256561160087585, 0.2278878092765808], [0.03420460224151611, 0.01788824424147606, 0.03985222429037094, 0.015430380590260029, 0.006759054958820343, 0.004837868735194206, 0.002775764325633645, 0.004857957828789949, 0.008922857232391834, 0.031755659729242325, 0.01632266491651535, 0.0036563421599566936, 0.0035914278123527765, 0.004934739321470261, 0.015791207551956177, 0.41519278287887573, 0.37322625517845154], [0.02726776711642742, 0.007186734117567539, 0.016876336187124252, 0.011485742405056953, 0.004902378655970097, 0.005941607989370823, 0.0031681382097303867, 0.007252244744449854, 0.007173936348408461, 0.025340471416711807, 0.006587425712496042, 0.0029008660931140184, 0.0037521386984735727, 0.003995703998953104, 0.02075684815645218, 0.45397287607192993, 0.391438752412796], [0.01937289908528328, 0.007614504545927048, 0.0060502937994897366, 0.008809533901512623, 0.0030178783927112818, 0.0029329191893339157, 0.0015006700996309519, 0.0033856830559670925, 0.003801151877269149, 0.017526380717754364, 0.0022017923183739185, 0.0010045161470770836, 0.001520915306173265, 0.00192083022557199, 0.011397636495530605, 0.496958464384079, 0.41098394989967346], [0.03283344954252243, 0.011207056231796741, 0.014554006978869438, 0.016586385667324066, 0.004346231464296579, 0.002433476038277149, 0.0016020855400711298, 0.009585325606167316, 0.004799789283424616, 0.017147816717624664, 0.0049131796695292, 0.0019187768921256065, 0.001719354186207056, 0.002516713924705982, 0.011160776950418949, 0.46687933802604675, 0.3957962691783905], [0.035808153450489044, 0.006188442464917898, 0.00812903605401516, 0.011887782253324986, 0.0029251640662550926, 0.0018933155806735158, 0.0014900958631187677, 0.009273032657802105, 0.003352584782987833, 0.010610566474497318, 0.0021233076695352793, 0.0008626370108686388, 0.0010750603396445513, 0.00155226094648242, 0.007591343484818935, 0.49074485898017883, 0.4044922888278961], [0.019878076389431953, 0.005248410161584616, 0.01024597231298685, 0.0071411943063139915, 0.0018174852011725307, 0.001319131231866777, 0.0008646455244161189, 0.00549384905025363, 0.0027772390749305487, 0.00963154062628746, 0.003059059614315629, 0.0010391174582764506, 0.0010842520277947187, 0.0014150849310681224, 0.007355376612395048, 0.510016143321991, 0.41161346435546875], [0.01899668388068676, 0.002833080478012562, 0.004047606140375137, 0.0030182760674506426, 0.0015565665671601892, 0.0013349108630791306, 0.0014643159229308367, 0.003337799571454525, 0.004029653966426849, 0.01901869848370552, 0.0015672420850023627, 0.000839308719150722, 0.001449907780624926, 0.0015448530903086066, 0.012363210320472717, 0.5097561478614807, 0.412841796875], [0.035378243774175644, 0.00829923152923584, 0.010817090980708599, 0.0074264188297092915, 0.0021304660476744175, 0.0009538829326629639, 0.000986503204330802, 0.0035623989533632994, 0.0029752396512776613, 0.008984613232314587, 0.0045751179568469524, 0.001269425847567618, 0.001250696717761457, 0.0017816972685977817, 0.008544446900486946, 0.48765134811401367, 0.41341301798820496], [0.014854623936116695, 0.0028361889999359846, 0.004843458533287048, 0.002760335337370634, 0.0011284755310043693, 0.0008109359769150615, 0.00048800656804814935, 0.0007928390987217426, 0.000891170755494386, 0.0068275053054094315, 0.002158057875931263, 0.0018975631101056933, 0.0019132394809275866, 0.002621188061311841, 0.01090309303253889, 0.5222498774528503, 0.4220234751701355], [0.029888711869716644, 0.017268268391489983, 0.017745984718203545, 0.02049926295876503, 0.006157419644296169, 0.005084881093353033, 0.003188234055414796, 0.004052082076668739, 0.002970531117171049, 0.008252064697444439, 0.006369614973664284, 0.0034570114221423864, 0.0021585263311862946, 0.0021325477864593267, 0.01174067985266447, 0.4569814205169678, 0.4020526707172394], [0.009534205310046673, 0.004230990074574947, 0.0036156335845589638, 0.003175305435433984, 0.0010723116574808955, 0.0005921150441281497, 0.0003673039027489722, 0.00027996624703519046, 0.00032520631793886423, 0.0008063446148298681, 0.00217641843482852, 0.0030304770916700363, 0.0019525167299434543, 0.0014072653139010072, 0.0032291843090206385, 0.5417010188102722, 0.4225037395954132], [0.01183563657104969, 0.0048489049077034, 0.008180132135748863, 0.002351535717025399, 0.0008176079718396068, 0.0005841119564138353, 0.00034183490788564086, 0.0003718202351592481, 0.0003770803159568459, 0.0006649888819083571, 0.005028471350669861, 0.003336304798722267, 0.0015620572958141565, 0.0010681296698749065, 0.0035612951032817364, 0.5249758362770081, 0.43009430170059204], [0.012023214250802994, 0.0035566489677876234, 0.00830263365060091, 0.0021850266493856907, 0.0008026854484342039, 0.0004300929722376168, 0.00024529374786652625, 0.0002443804405629635, 0.00025828942307271063, 0.0007139783701859415, 0.0044075362384319305, 0.0042929574847221375, 0.0016857580048963428, 0.0012382708955556154, 0.003351018065586686, 0.5295377969741821, 0.42672455310821533], [0.010621648281812668, 0.004979688208550215, 0.005919287446886301, 0.002499180380254984, 0.0012118977028876543, 0.0007743930909782648, 0.00047712435480207205, 0.00045240545296110213, 0.000798436813056469, 0.0029648372437804937, 0.004011616110801697, 0.00841735303401947, 0.004105354659259319, 0.004265759605914354, 0.013743880204856396, 0.5166716575622559, 0.4180854260921478], [0.009248637594282627, 0.004456457681953907, 0.012595480307936668, 0.003059728303924203, 0.0017599738202989101, 0.0015224464004859328, 0.000963585393037647, 0.0008055937942117453, 0.0018897723639383912, 0.005251426249742508, 0.007944922894239426, 0.0019527450203895569, 0.0013961177319288254, 0.00137426913715899, 0.005638061556965113, 0.5199901461601257, 0.42015066742897034], [0.008528738282620907, 0.0038354056887328625, 0.01116661261767149, 0.0026727512013167143, 0.0014833586756139994, 0.0012894183164462447, 0.0008027036674320698, 0.000665254658088088, 0.0015888925408944488, 0.004756939131766558, 0.006810804363340139, 0.0016427505761384964, 0.0011593862436711788, 0.0011496993247419596, 0.0049998704344034195, 0.524887204170227, 0.4225602149963379]], [[0.23651695251464844, 0.036323562264442444, 0.03135655075311661, 0.040306687355041504, 0.025122737511992455, 0.012090850621461868, 0.0069850729778409, 0.027094002813100815, 0.042539387941360474, 0.11449520289897919, 0.04784151911735535, 0.020968273282051086, 0.0312669575214386, 0.03850441798567772, 0.05028317868709564, 0.1181417778134346, 0.12016286700963974], [0.057873208075761795, 0.0327354297041893, 0.01655852049589157, 0.015223809517920017, 0.006416128948330879, 0.004164468962699175, 0.0020219208672642708, 0.0031228619627654552, 0.005577170290052891, 0.0034615276381373405, 0.0022080044727772474, 0.001823005615733564, 0.0009592260466888547, 0.0008460091194137931, 0.003117889165878296, 0.43615594506263733, 0.40773484110832214], [0.033264949917793274, 0.021354131400585175, 0.0037638742942363024, 0.008878589607775211, 0.006529560778290033, 0.008725585415959358, 0.004764067009091377, 0.003599012503400445, 0.0069960965774953365, 0.00687934085726738, 0.0019351514056324959, 0.0067809526808559895, 0.003537630196660757, 0.003168360562995076, 0.00919788796454668, 0.46221184730529785, 0.40841296315193176], [0.061256252229213715, 0.025233564898371696, 0.007485759444534779, 0.0068391296081244946, 0.0043696025386452675, 0.0020270184613764286, 0.0018453116063028574, 0.0017270651878789067, 0.003421593690291047, 0.0027485755272209644, 0.0014896533684805036, 0.0021418484393507242, 0.0008069822797551751, 0.0005112927756272256, 0.002495340071618557, 0.4604315161705017, 0.41516950726509094], [0.051890116184949875, 0.02454238198697567, 0.012103610672056675, 0.011394371278584003, 0.005966268014162779, 0.003014505375176668, 0.0017610893119126558, 0.003527240827679634, 0.008116542361676693, 0.004816559609025717, 0.0015516499988734722, 0.0017388497944921255, 0.0006486552301794291, 0.0004036807513330132, 0.002362203784286976, 0.45289137959480286, 0.4132709205150604], [0.06380607187747955, 0.0218901839107275, 0.01816604845225811, 0.01638370379805565, 0.006651540752500296, 0.0034298121463507414, 0.001916665118187666, 0.003430636366829276, 0.005179165862500668, 0.0061438484117388725, 0.0027693286538124084, 0.001302147633396089, 0.0006860039429739118, 0.00045701549970544875, 0.0025228518061339855, 0.44360482692718506, 0.4016600549221039], [0.01918773725628853, 0.007339912932366133, 0.011501002125442028, 0.009984161704778671, 0.0035059505607932806, 0.003747014794498682, 0.0016759316204115748, 0.0030922323931008577, 0.002971482928842306, 0.006424116902053356, 0.0022024833597242832, 0.0013783338945358992, 0.0007906226674094796, 0.0005348720587790012, 0.0018526323838159442, 0.49569442868232727, 0.4281170666217804], [0.027054136618971825, 0.011143503710627556, 0.013203318230807781, 0.017811059951782227, 0.017978016287088394, 0.03133615478873253, 0.03098730370402336, 0.014125081710517406, 0.01571555994451046, 0.015930062159895897, 0.0037689998280256987, 0.003648788435384631, 0.0019237505039200187, 0.0013797285500913858, 0.003105955896899104, 0.4185943007469177, 0.37229427695274353], [0.019370758906006813, 0.0108527522534132, 0.008146817795932293, 0.01673448085784912, 0.010833311825990677, 0.012356925755739212, 0.012621065601706505, 0.0192446019500494, 0.017715387046337128, 0.012845365330576897, 0.002051254967227578, 0.001531219226308167, 0.0016374614788219333, 0.0015267666894942522, 0.0037468140944838524, 0.4501454532146454, 0.39863964915275574], [0.04134753346443176, 0.034908317029476166, 0.020619746297597885, 0.058480389416217804, 0.03588353842496872, 0.029405122622847557, 0.026038622483611107, 0.036629315465688705, 0.040943779051303864, 0.028476083651185036, 0.0061048599891364574, 0.003147561103105545, 0.002940567908808589, 0.0024762775283306837, 0.011141378432512283, 0.31801098585128784, 0.3034459948539734], [0.015258487313985825, 0.015639236196875572, 0.003364427015185356, 0.005620388314127922, 0.004487556405365467, 0.006077175494283438, 0.0034856856800615788, 0.0034738448448479176, 0.004561434034258127, 0.006111594382673502, 0.0012283907271921635, 0.00828097015619278, 0.004736284259706736, 0.0037626074627041817, 0.008136383257806301, 0.48204317688941956, 0.42373231053352356], [0.007792210206389427, 0.00828501582145691, 0.011429950594902039, 0.00772528350353241, 0.0020530454348772764, 0.001683086040429771, 0.00084102270193398, 0.0017378986813127995, 0.0013589810114353895, 0.005052543245255947, 0.00349845620803535, 0.0033251168206334114, 0.006995014380663633, 0.006343609653413296, 0.012106835842132568, 0.4894380569458008, 0.4303339123725891], [0.005348453763872385, 0.003679691581055522, 0.006545945070683956, 0.0047048283740878105, 0.002316883532330394, 0.002673099981620908, 0.001346126664429903, 0.002531021134927869, 0.00279719615355134, 0.009856854565441608, 0.0044403355568647385, 0.01171429455280304, 0.01856774091720581, 0.020744074136018753, 0.017077332362532616, 0.4761600196361542, 0.4094960689544678], [0.005663000512868166, 0.0038767403457313776, 0.006872482597827911, 0.0049823615700006485, 0.0016951793804764748, 0.0021927838679403067, 0.0012805333826690912, 0.0032904576510190964, 0.0024328497238457203, 0.01225376222282648, 0.004438852891325951, 0.010078887455165386, 0.02002415619790554, 0.019841594621539116, 0.01667417772114277, 0.4768788516521454, 0.40752342343330383], [0.020422594621777534, 0.017070192843675613, 0.015047776512801647, 0.024597616866230965, 0.013163845054805279, 0.015988269820809364, 0.007209653966128826, 0.01668522320687771, 0.01712781749665737, 0.07227980345487595, 0.01175922341644764, 0.014652763493359089, 0.024617839604616165, 0.03273521736264229, 0.040216632187366486, 0.34358158707618713, 0.31284385919570923], [0.007210525218397379, 0.0077256085351109505, 0.004487893544137478, 0.005812467075884342, 0.00280723231844604, 0.0036234448198229074, 0.0030817128717899323, 0.0031269502360373735, 0.0023950478062033653, 0.003331045852974057, 0.0019026810768991709, 0.0017056441865861416, 0.001894081593491137, 0.0012956609716638923, 0.00247303512878716, 0.5156096816062927, 0.43151721358299255], [0.0067040398716926575, 0.006853511556982994, 0.0040117050521075726, 0.005051634274423122, 0.002398434793576598, 0.0029999983962625265, 0.002495829714462161, 0.002659411868080497, 0.0020964667201042175, 0.0029536536894738674, 0.0016401739558205009, 0.0014348092954605818, 0.0016095474129542708, 0.001100091147236526, 0.002139565534889698, 0.5201776027679443, 0.43367353081703186]], [[0.04816687852144241, 0.32839784026145935, 0.04958090931177139, 0.18943996727466583, 0.057342711836099625, 0.03986898437142372, 0.02950040064752102, 0.029343409463763237, 0.05757169425487518, 0.07022017240524292, 0.013483719900250435, 0.00879399012774229, 0.011347883380949497, 0.014532714150846004, 0.014205588027834892, 0.01839279942214489, 0.019810259342193604], [0.007165192160755396, 0.07844630628824234, 0.05445577949285507, 0.09618937224149704, 0.040948815643787384, 0.032657451927661896, 0.04703838378190994, 0.0337422750890255, 0.03767382353544235, 0.04505883529782295, 0.022818878293037415, 0.004983858671039343, 0.002961598103865981, 0.003490008180961013, 0.01066737249493599, 0.24030683934688568, 0.24139517545700073], [0.016508422791957855, 0.03766336664557457, 0.04512700065970421, 0.03575616329908371, 0.016752080991864204, 0.011495433747768402, 0.014962776564061642, 0.02137857861816883, 0.022784186527132988, 0.04086676612496376, 0.01894170604646206, 0.005073474254459143, 0.00480344844982028, 0.005037550814449787, 0.009003641083836555, 0.35812610387802124, 0.33571934700012207], [0.016615325585007668, 0.03329116851091385, 0.06523072719573975, 0.030727170407772064, 0.017191438004374504, 0.016107484698295593, 0.017704015597701073, 0.024034617468714714, 0.054058462381362915, 0.04583020508289337, 0.01879100874066353, 0.0024624732322990894, 0.0020695622079074383, 0.0018723024986684322, 0.00572094926610589, 0.32782605290412903, 0.3204670250415802], [0.03975778818130493, 0.02194461040198803, 0.08114657551050186, 0.03290080279111862, 0.04277762770652771, 0.051206074655056, 0.05818574130535126, 0.05308346077799797, 0.1244746670126915, 0.09191559255123138, 0.04238559305667877, 0.003136291401460767, 0.00399455102160573, 0.0040666148997843266, 0.013413893058896065, 0.16871625185012817, 0.16689389944076538], [0.02839917130768299, 0.0070750354789197445, 0.045141663402318954, 0.013872222974896431, 0.018278393894433975, 0.056668203324079514, 0.08468310534954071, 0.06284502893686295, 0.08636368066072464, 0.08527005463838577, 0.029985398054122925, 0.005665996111929417, 0.006683864630758762, 0.006669220048934221, 0.017489751800894737, 0.22906650602817535, 0.21584264934062958], [0.019010063260793686, 0.008870474994182587, 0.025269189849495888, 0.009803373366594315, 0.012923532165586948, 0.02454296685755253, 0.049987975507974625, 0.03637019172310829, 0.036947909742593765, 0.047131072729825974, 0.01614474505186081, 0.0032872215379029512, 0.004351575393229723, 0.0043955915607512, 0.010318150743842125, 0.3637681007385254, 0.3268779218196869], [0.004853544756770134, 0.007939092814922333, 0.028444524854421616, 0.01174110732972622, 0.013532409444451332, 0.012582673691213131, 0.024494759738445282, 0.040872834622859955, 0.030989039689302444, 0.04492749273777008, 0.010039218701422215, 0.0038641695864498615, 0.003651428734883666, 0.0028622590471059084, 0.009288319386541843, 0.39491963386535645, 0.354997456073761], [0.02285326085984707, 0.017604287713766098, 0.06564755737781525, 0.029464544728398323, 0.03990808501839638, 0.048438847064971924, 0.08057600259780884, 0.052890341728925705, 0.14927156269550323, 0.13748693466186523, 0.03009171597659588, 0.0073295291513204575, 0.007215192075818777, 0.0063820285722613335, 0.012462552636861801, 0.14928025007247925, 0.14309720695018768], [0.012639766559004784, 0.013394205830991268, 0.014374744147062302, 0.006471680011600256, 0.0027037407271564007, 0.0034382669255137444, 0.004358690232038498, 0.008197544142603874, 0.012998372316360474, 0.06055276840925217, 0.005582606419920921, 0.00523827038705349, 0.0052788411267101765, 0.006031466647982597, 0.014527845196425915, 0.42581722140312195, 0.3983939588069916], [0.023899301886558533, 0.03191668167710304, 0.07205849885940552, 0.04412257671356201, 0.019126558676362038, 0.012222487479448318, 0.011217083781957626, 0.012224395759403706, 0.017147298902273178, 0.06941642612218857, 0.045800838619470596, 0.03493769094347954, 0.021660776808857918, 0.023360129445791245, 0.0324350968003273, 0.27015912532806396, 0.25829508900642395], [0.013553808443248272, 0.005168687552213669, 0.019688542932271957, 0.00219124648720026, 0.0014565172605216503, 0.0014510713517665863, 0.001442842883989215, 0.001973919803276658, 0.0019949967972934246, 0.007827173918485641, 0.012159071862697601, 0.02536129578948021, 0.021354636177420616, 0.022948380559682846, 0.025521477684378624, 0.43366169929504395, 0.4022446870803833], [0.030128296464681625, 0.017851319164037704, 0.025092123076319695, 0.0030413849744945765, 0.001705411821603775, 0.0011746917152777314, 0.0023733950220048428, 0.003816600190475583, 0.002629224443808198, 0.011645962484180927, 0.010863371193408966, 0.026365965604782104, 0.03534764423966408, 0.034206490963697433, 0.024780498817563057, 0.4055785536766052, 0.36339905858039856], [0.026858460158109665, 0.015653785318136215, 0.02559630014002323, 0.0028847476933151484, 0.0016699578845873475, 0.0013104969402775168, 0.002596481004729867, 0.003871015040203929, 0.002754703164100647, 0.011372840963304043, 0.01563863642513752, 0.02985033392906189, 0.029332289472222328, 0.030284950509667397, 0.025739802047610283, 0.4081905782222748, 0.36639463901519775], [0.017650822177529335, 0.011073816567659378, 0.019526252523064613, 0.0012709288857877254, 0.0008612539386376739, 0.0008713824208825827, 0.0014828837011009455, 0.0020898813381791115, 0.0021355459466576576, 0.010951362550258636, 0.015441006980836391, 0.028664838522672653, 0.04383780062198639, 0.035837505012750626, 0.02598096989095211, 0.4143233299255371, 0.36800047755241394], [0.01352445688098669, 0.008800188079476357, 0.0076267835684120655, 0.005059736780822277, 0.002964259823784232, 0.0032052986789494753, 0.003700552275404334, 0.0032453015446662903, 0.0024338741786777973, 0.006020450033247471, 0.006652620621025562, 0.0044187940657138824, 0.004581666085869074, 0.004931359551846981, 0.007606604136526585, 0.485077828168869, 0.43015021085739136], [0.01171096507459879, 0.008348973467946053, 0.00725976936519146, 0.004866094794124365, 0.0027706294786185026, 0.002955824136734009, 0.0034354671370238066, 0.0029729092493653297, 0.00231229979544878, 0.005823869723826647, 0.006040989886969328, 0.003908857237547636, 0.004052177537232637, 0.004404585342854261, 0.007206719368696213, 0.4886724650859833, 0.43325743079185486]], [[0.038331396877765656, 0.2067907452583313, 0.16121366620063782, 0.22216783463954926, 0.038343992084264755, 0.03076106682419777, 0.017433255910873413, 0.016506699845194817, 0.010886847041547298, 0.046310905367136, 0.03415039926767349, 0.009679107926785946, 0.002550749806687236, 0.0036080488935112953, 0.020175667479634285, 0.06856285780668259, 0.07252673804759979], [0.009004752151668072, 0.036851756274700165, 0.006578925997018814, 0.0034391232766211033, 0.00126233184710145, 0.0005674047861248255, 0.0005024133715778589, 0.0007060187053866684, 0.0006754504865966737, 0.0012581223854795098, 0.0015806431183591485, 4.890166383120231e-05, 0.00010069466952700168, 9.964223863789812e-05, 0.0003910076047759503, 0.5063140392303467, 0.43061888217926025], [0.028352150693535805, 0.03822099789977074, 0.03726005181670189, 0.015526434406638145, 0.007068153005093336, 0.0031727163586765528, 0.001691725687123835, 0.003171010408550501, 0.0032524014823138714, 0.005006421823054552, 0.008994314819574356, 0.0010924445232376456, 0.0013515959726646543, 0.0015138990711420774, 0.0032450896687805653, 0.44531524181365967, 0.39576539397239685], [0.011605813167989254, 0.0116024911403656, 0.007845647633075714, 0.004880497232079506, 0.0016929571283981204, 0.0009426469914615154, 0.0008369607385247946, 0.0018764086998999119, 0.0010960183572024107, 0.0009070069063454866, 0.0017130570486187935, 0.00011084719153586775, 0.00013249956828076392, 0.00016984004469122738, 0.0003136903396807611, 0.5240627527236938, 0.4302108585834503], [0.030447643250226974, 0.03237618878483772, 0.024320291355252266, 0.016556516289711, 0.007138807792216539, 0.006820575799793005, 0.006305490154772997, 0.008702232502400875, 0.004581546876579523, 0.004174180328845978, 0.006144471932202578, 0.0011270464165136218, 0.0006784560973756015, 0.0008131428039632738, 0.0015670263674110174, 0.4502565860748291, 0.3979897201061249], [0.02011101320385933, 0.016713013872504234, 0.017369460314512253, 0.009753415361046791, 0.0036190529353916645, 0.0045790341682732105, 0.0038702827878296375, 0.005319013725966215, 0.0022568325512111187, 0.0025843430776149035, 0.003303402103483677, 0.0019916356541216373, 0.0012071944074705243, 0.001338225556537509, 0.0018469643546268344, 0.48627492785453796, 0.4178621768951416], [0.03265739977359772, 0.025223111733794212, 0.03808343783020973, 0.021673744544386864, 0.014190388843417168, 0.023955315351486206, 0.025841692462563515, 0.023231472820043564, 0.008661940693855286, 0.010279661975800991, 0.007178511470556259, 0.0023423912934958935, 0.0010504148667678237, 0.0012898612767457962, 0.002203538781031966, 0.403048038482666, 0.35908910632133484], [0.014219953678548336, 0.016711845993995667, 0.013576874509453773, 0.013003102503716946, 0.009410868398845196, 0.015169276855885983, 0.02322656288743019, 0.027715502306818962, 0.008614705875515938, 0.006476514507085085, 0.002649083733558655, 0.0005224316264502704, 0.0002185894554713741, 0.0002335894969291985, 0.00033710041316226125, 0.45924052596092224, 0.38867342472076416], [0.01471509039402008, 0.016077235341072083, 0.010387120768427849, 0.01104035321623087, 0.006651000119745731, 0.006258177570998669, 0.006592734716832638, 0.006726423278450966, 0.006260523572564125, 0.004170349799096584, 0.0036941245198249817, 0.0006493807886727154, 0.0002869874588213861, 0.00035647189361043274, 0.000371550559066236, 0.48679324984550476, 0.41896921396255493], [0.010515789501369, 0.00594940735027194, 0.006083272397518158, 0.0030058962292969227, 0.001966300653293729, 0.0010547562269493937, 0.0013772366801276803, 0.002141276840120554, 0.003827471286058426, 0.005526375025510788, 0.004498278722167015, 0.0053159501403570175, 0.0021229840349406004, 0.002602535067126155, 0.0037283378187566996, 0.514354407787323, 0.4259296953678131], [0.04793502762913704, 0.037457309663295746, 0.04511605203151703, 0.035309743136167526, 0.02572733163833618, 0.01828395016491413, 0.006822315976023674, 0.006388296838849783, 0.008415968157351017, 0.016230612993240356, 0.10697215795516968, 0.0057548414915800095, 0.0023757386952638626, 0.002702661557123065, 0.007804596330970526, 0.32441791892051697, 0.30228549242019653], [0.0006832147482782602, 0.0009992836276069283, 0.0003154069709125906, 6.0008584114257246e-05, 7.263899897225201e-05, 0.00010781836317619309, 0.0001628831960260868, 0.0002536243700888008, 0.0005089507903903723, 0.0024774600751698017, 0.00099265668541193, 0.06799308210611343, 0.009687528014183044, 0.012807833962142467, 0.007623312529176474, 0.5130242705345154, 0.3822300136089325], [0.0019184175180271268, 0.0028358648996800184, 0.0007967234123498201, 0.00019625794084277004, 0.00019593104661908, 0.0002479085815139115, 0.0002701363991945982, 0.00030801593675278127, 0.0007042951183393598, 0.002486813347786665, 0.0014546833699569106, 0.06285068392753601, 0.026691129431128502, 0.028510689735412598, 0.022866379469633102, 0.4769909977912903, 0.37067505717277527], [0.0017330298433080316, 0.0027455107774585485, 0.0010221078991889954, 0.00018877960974350572, 0.00015598852769471705, 0.0001731442316668108, 0.00019902644271496683, 0.00032488146098330617, 0.0007654939545318484, 0.002506101969629526, 0.0019595129415392876, 0.048559222370386124, 0.020352890715003014, 0.023821312934160233, 0.024262160062789917, 0.4900488257408142, 0.3811819851398468], [0.007706911768764257, 0.004834697116166353, 0.003880598582327366, 0.0007854732102714479, 0.00048081111162900925, 0.00032646904583089054, 0.00040337929385714233, 0.0005315066082403064, 0.0015793744241818786, 0.0033240888733416796, 0.0030987034551799297, 0.011416805908083916, 0.008942075073719025, 0.011629397980868816, 0.026898693293333054, 0.5045921802520752, 0.4095689356327057], [0.011256322264671326, 0.007202582899481058, 0.00527913449332118, 0.0028199022635817528, 0.001214552321471274, 0.0006911620730534196, 0.0006678735371679068, 0.0012971371179446578, 0.0018429126357659698, 0.002395146992057562, 0.0023232921957969666, 0.0011945798760280013, 0.0009580437326803803, 0.0009591078269295394, 0.0020458779763430357, 0.5262014865875244, 0.431650847196579], [0.010348064824938774, 0.007143987342715263, 0.004814797546714544, 0.002628747606649995, 0.001114196958951652, 0.0006498904549516737, 0.0006273765466175973, 0.001209302805364132, 0.0016282962169498205, 0.0022015366703271866, 0.002117625204846263, 0.001061379094608128, 0.0008505976875312626, 0.0008566156029701233, 0.0018694456666707993, 0.5279167890548706, 0.4329614043235779]], [[0.011518976651132107, 0.12807655334472656, 0.05733388662338257, 0.11636066436767578, 0.022723106667399406, 0.01271261740475893, 0.040547605603933334, 0.05934249237179756, 0.015961214900016785, 0.13286760449409485, 0.012295315973460674, 0.02032020129263401, 0.01939358375966549, 0.03452979773283005, 0.13045881688594818, 0.09191722422838211, 0.09364043921232224], [0.036655910313129425, 0.09394476562738419, 0.05490449070930481, 0.01877463236451149, 0.004103940911591053, 0.003936448134481907, 0.002608922077342868, 0.004131073132157326, 0.002557123312726617, 0.006627898663282394, 0.014195263385772705, 0.0012814331566914916, 0.000647775421384722, 0.0007743316818960011, 0.006020910572260618, 0.3874068856239319, 0.36142823100090027], [0.031009627506136894, 0.06716901063919067, 0.01849174126982689, 0.017621323466300964, 0.005288449581712484, 0.0067306626588106155, 0.003947124816477299, 0.005754304584115744, 0.0035324818454682827, 0.006750194821506739, 0.00808002706617117, 0.005052483640611172, 0.0023452697787433863, 0.0025915061123669147, 0.008679301477968693, 0.43184521794319153, 0.37511134147644043], [0.019998325034976006, 0.028470618650317192, 0.005848872475326061, 0.011637158691883087, 0.0021413902286440134, 0.002097802236676216, 0.0012467310298234224, 0.002695320639759302, 0.0016899894690141082, 0.002438565716147423, 0.002611706964671612, 0.0013335936237126589, 0.00018038401321973652, 0.0002532846701797098, 0.001006211619824171, 0.5017723441123962, 0.41457775235176086], [0.017437640577554703, 0.01972872205078602, 0.006099965423345566, 0.011425705626606941, 0.002907100133597851, 0.0033253172878175974, 0.002069264417514205, 0.003954455256462097, 0.0021524578332901, 0.002366450848057866, 0.0033871701452881098, 0.0020526463631540537, 0.00022372897365130484, 0.00026520880055613816, 0.0007002443890087306, 0.5012701749801636, 0.4206337034702301], [0.0132591025903821, 0.01987140253186226, 0.005631864070892334, 0.02172239124774933, 0.005535114090889692, 0.007918109185993671, 0.005640001501888037, 0.00694452878087759, 0.0036595617420971394, 0.00235554832033813, 0.004294493701308966, 0.002470548264682293, 0.0002781460352707654, 0.0003181547799613327, 0.0006555591244250536, 0.48900675773620605, 0.41043880581855774], [0.006408998277038336, 0.012076946906745434, 0.0056552039459347725, 0.01722882129251957, 0.004118460230529308, 0.005552021786570549, 0.004437211900949478, 0.00551840104162693, 0.0026965460274368525, 0.0015324624255299568, 0.0036243940703570843, 0.001074460451491177, 8.325962699018419e-05, 0.00010990869486704469, 0.0002829149889294058, 0.5081712603569031, 0.42142871022224426], [0.006279374472796917, 0.006000687833875418, 0.005171325523406267, 0.012808908708393574, 0.0020797059405595064, 0.0032277260906994343, 0.0023397819604724646, 0.0037270488683134317, 0.001115421182475984, 0.0014722381019964814, 0.0015525261405855417, 0.00042006265721283853, 4.711597284767777e-05, 5.418822183855809e-05, 0.0002494824875611812, 0.5327826142311096, 0.4206717610359192], [0.01619897596538067, 0.021178049966692924, 0.014803535304963589, 0.021169256418943405, 0.006991810165345669, 0.006744235288351774, 0.005590479820966721, 0.008439714089035988, 0.006182133685797453, 0.006036500912159681, 0.005079269874840975, 0.0014444123953580856, 0.00026301181060262024, 0.00029998787795193493, 0.001526735140942037, 0.4623440206050873, 0.41570791602134705], [0.01804841123521328, 0.011778359301388264, 0.006103560794144869, 0.01254104170948267, 0.003391724079847336, 0.0048216283321380615, 0.006156314164400101, 0.01023890171200037, 0.004135101567953825, 0.01352879498153925, 0.0034979076590389013, 0.003053959459066391, 0.0009180569904856384, 0.001337465364485979, 0.009592341259121895, 0.48030734062194824, 0.4105490744113922], [0.03219227492809296, 0.03557926043868065, 0.02509789913892746, 0.01965194381773472, 0.009654450230300426, 0.009962160140275955, 0.009594999253749847, 0.012459994293749332, 0.010607951320707798, 0.019072305411100388, 0.013827317394316196, 0.009613198228180408, 0.004143782891333103, 0.005103270057588816, 0.02587718889117241, 0.39796707034111023, 0.3595948815345764], [0.03692704066634178, 0.011907847598195076, 0.024285033345222473, 0.012477776035666466, 0.008015832863748074, 0.011456141248345375, 0.009309317916631699, 0.003360544564202428, 0.008258841931819916, 0.04822614789009094, 0.014541622251272202, 0.017208650708198547, 0.025036390870809555, 0.022281698882579803, 0.09735546261072159, 0.338788241147995, 0.3105635344982147], [0.05781523138284683, 0.012126139365136623, 0.020796773955225945, 0.007810360286384821, 0.0037628384307026863, 0.003899186849594116, 0.0024905235040932894, 0.001638622023165226, 0.0029370891861617565, 0.023422004655003548, 0.016789646819233894, 0.01788085512816906, 0.04010864719748497, 0.02948390506207943, 0.03505675122141838, 0.3814813494682312, 0.3425000011920929], [0.03824818506836891, 0.006704144179821014, 0.012808739207684994, 0.005906971171498299, 0.003363646799698472, 0.003943595569580793, 0.002562600653618574, 0.001711262040771544, 0.0029535009525716305, 0.020075352862477303, 0.011534119956195354, 0.011382581666111946, 0.01813185214996338, 0.014462686143815517, 0.031446199864149094, 0.43538832664489746, 0.37937626242637634], [0.04621153324842453, 0.015086927451193333, 0.020854929462075233, 0.015587630681693554, 0.006516471039503813, 0.010006571188569069, 0.010237392969429493, 0.00970597192645073, 0.008709209971129894, 0.08375377207994461, 0.007635965012013912, 0.007199467625468969, 0.008643029257655144, 0.010457674041390419, 0.053220730274915695, 0.36276179552078247, 0.32341089844703674], [0.01146288774907589, 0.001972330268472433, 0.002596156904473901, 0.0013481664936989546, 0.0005396536435000598, 0.0007577197393402457, 0.0007589144515804946, 0.00043011773959733546, 0.0003831395588349551, 0.002014339668676257, 0.0013961894437670708, 0.0012394582154229283, 0.0008051136392168701, 0.0009040485601872206, 0.004065179731696844, 0.5407286286354065, 0.42859792709350586], [0.010470016859471798, 0.001936277374625206, 0.0024986513890326023, 0.0012748833978548646, 0.00048122796579264104, 0.0006631642463617027, 0.0006551237893290818, 0.0003743043926078826, 0.00033326822449453175, 0.0018765307031571865, 0.001265490660443902, 0.001125818002037704, 0.0006996691809035838, 0.0007980113150551915, 0.003794419579207897, 0.54217928647995, 0.42957380414009094]], [[0.024367323145270348, 0.02906242199242115, 0.05037279427051544, 0.02462160401046276, 0.007191405165940523, 0.004117128439247608, 0.005780793260782957, 0.014300664886832237, 0.01995187997817993, 0.1525058001279831, 0.08473946899175644, 0.01882389932870865, 0.021494092419743538, 0.02241124026477337, 0.3946465849876404, 0.060917746275663376, 0.0646950975060463], [0.047370824962854385, 0.18536560237407684, 0.03351340442895889, 0.08610601723194122, 0.013140412978827953, 0.009731285274028778, 0.003304034937173128, 0.0020718618761748075, 0.0029711355455219746, 0.010601067915558815, 0.008990240283310413, 0.000593514007050544, 0.0005303713260218501, 0.0006212338339537382, 0.003322784323245287, 0.30684301257133484, 0.2849233150482178], [0.04761669412255287, 0.03077784553170204, 0.05049499496817589, 0.03505382314324379, 0.008376292884349823, 0.008171903900802135, 0.004047393798828125, 0.003247190034016967, 0.0036324693355709314, 0.031901340931653976, 0.02354544959962368, 0.006692094728350639, 0.003568564308807254, 0.0037452748510986567, 0.01934250071644783, 0.3739921450614929, 0.34579411149024963], [0.06174460053443909, 0.093439981341362, 0.03374972939491272, 0.21705785393714905, 0.0435531921684742, 0.06845714896917343, 0.028810083866119385, 0.01652688905596733, 0.009028440341353416, 0.026302814483642578, 0.008715009316802025, 0.0011678623268380761, 0.001080764806829393, 0.001263017999008298, 0.006041146349161863, 0.2005658596754074, 0.18249551951885223], [0.0728611946105957, 0.023130429908633232, 0.022282248362898827, 0.03947216644883156, 0.018502354621887207, 0.023857826367020607, 0.019142664968967438, 0.015362911857664585, 0.009316233918070793, 0.01833222061395645, 0.012520398944616318, 0.004070512019097805, 0.001289757201448083, 0.0014455481432378292, 0.00799547228962183, 0.37502321600914, 0.33539479970932007], [0.053253769874572754, 0.01733638159930706, 0.016718050464987755, 0.028764741495251656, 0.014152249321341515, 0.021381303668022156, 0.032631613314151764, 0.018246101215481758, 0.013596984557807446, 0.01639789156615734, 0.011298376135528088, 0.00592278316617012, 0.0016220386605709791, 0.0019261965062469244, 0.009746822528541088, 0.3958576023578644, 0.34114712476730347], [0.02134576253592968, 0.00305115501396358, 0.007188478950411081, 0.004845479037612677, 0.002931182272732258, 0.004002838395535946, 0.02550608664751053, 0.013834957033395767, 0.008630198426544666, 0.0076939198188483715, 0.010216016322374344, 0.0037471698597073555, 0.0005027719889767468, 0.0004842453054152429, 0.002573820762336254, 0.479425311088562, 0.4040206968784332], [0.022194670513272285, 0.005555316340178251, 0.007305007427930832, 0.010411635972559452, 0.0072542172856628895, 0.011152959428727627, 0.058345336467027664, 0.056276045739650726, 0.045597143471241, 0.022313669323921204, 0.012171992100775242, 0.004713634494692087, 0.0008759578340686858, 0.0008651942480355501, 0.00275811948813498, 0.3967938721179962, 0.3354153037071228], [0.015808138996362686, 0.0029009028803557158, 0.0029974677599966526, 0.0026313397102057934, 0.0010209064930677414, 0.001773156109265983, 0.0022360519506037235, 0.0027044634334743023, 0.004558593034744263, 0.01561589166522026, 0.0050432924181222916, 0.006317383609712124, 0.000613749900367111, 0.0008364157984033227, 0.003307538339868188, 0.5123265385627747, 0.4193080961704254], [0.021657153964042664, 0.0029627857729792595, 0.006348973140120506, 0.00520319165661931, 0.0021716116461902857, 0.0047342474572360516, 0.004326913505792618, 0.011020179837942123, 0.023002035915851593, 0.23160098493099213, 0.009120513685047626, 0.016733048483729362, 0.009802130982279778, 0.014703826978802681, 0.06067332997918129, 0.31215134263038635, 0.2637876272201538], [0.025065090507268906, 0.004629320465028286, 0.010172508656978607, 0.005313645116984844, 0.0013781284214928746, 0.0012179751647636294, 0.0006553767598234117, 0.0008633237448520958, 0.0007986639975570142, 0.029426027089357376, 0.026827193796634674, 0.01968950591981411, 0.004857273306697607, 0.004946814849972725, 0.021786706522107124, 0.4540470242500305, 0.3883253335952759], [0.005245755892246962, 0.000581228407099843, 0.0020314636640250683, 0.0004608547315001488, 0.00012163814972154796, 0.00018099910812452435, 9.03647523955442e-05, 0.00014337344327941537, 0.00015995187277439982, 0.004811203572899103, 0.0074067795649170876, 0.18269169330596924, 0.004449727479368448, 0.003393903374671936, 0.006159232929348946, 0.43187177181243896, 0.3502000570297241], [0.0015990145038813353, 0.0001730184885673225, 0.000514042389113456, 9.146573574980721e-05, 3.470879892120138e-05, 3.8668542401865125e-05, 2.0052633772138506e-05, 8.735149458516389e-05, 2.7386282454244792e-05, 0.0012385172303766012, 0.0011445446871221066, 0.015381988137960434, 0.0034537333995103836, 0.004392250441014767, 0.007010811939835548, 0.547117292881012, 0.4176751673221588], [0.0012990925461053848, 0.00014570342318620533, 0.0004750272200908512, 8.582772716181353e-05, 2.852569923561532e-05, 3.765385190490633e-05, 1.7939584722626023e-05, 8.131215145112947e-05, 3.372177525307052e-05, 0.002071635564789176, 0.0008999546989798546, 0.013622650876641273, 0.00398082472383976, 0.006275547202676535, 0.010217689909040928, 0.5503903031349182, 0.4103364944458008], [0.007895882241427898, 0.000778423622250557, 0.003473726101219654, 0.0008139197598211467, 0.0004977501230314374, 0.000590624928008765, 0.0005126693286001682, 0.001526453997939825, 0.0009958839509636164, 0.026862023398280144, 0.0065581826493144035, 0.021498903632164, 0.015048409812152386, 0.023228922858834267, 0.03448436036705971, 0.47287705540657043, 0.3823568522930145], [0.012940396554768085, 0.0031101559288799763, 0.007215147837996483, 0.004109174013137817, 0.0036332341842353344, 0.003015357768163085, 0.0019523275550454855, 0.0012673023156821728, 0.001737413345836103, 0.009802022017538548, 0.0074138822965323925, 0.003524167463183403, 0.002612186362966895, 0.002851645229384303, 0.007016749121248722, 0.498696506023407, 0.4291023015975952], [0.013009405694901943, 0.003105882555246353, 0.007207048125565052, 0.0039137485437095165, 0.0033088349737226963, 0.0027093682438135147, 0.0018057734705507755, 0.0012182127684354782, 0.0016267446335405111, 0.00976905133575201, 0.007649262435734272, 0.003571178764104843, 0.00249925022944808, 0.0027191631961613894, 0.0070781465619802475, 0.498272567987442, 0.4305364489555359]], [[0.11489919573068619, 0.013221855275332928, 0.03735189139842987, 0.08797597885131836, 0.07684022933244705, 0.053711093962192535, 0.04061892256140709, 0.07189719378948212, 0.05045950785279274, 0.07198472321033478, 0.017407719045877457, 0.013531627133488655, 0.024736830964684486, 0.03183431178331375, 0.050431400537490845, 0.12617185711860657, 0.11692563444375992], [0.055199552327394485, 0.00944628193974495, 0.02106042206287384, 0.022721201181411743, 0.015977006405591965, 0.018966620787978172, 0.013376008719205856, 0.025776226073503494, 0.02365182898938656, 0.04813447594642639, 0.012773309834301472, 0.004292336292564869, 0.00380725204013288, 0.004545174073427916, 0.028947822749614716, 0.3700292706489563, 0.32129523158073425], [0.07158935815095901, 0.014839093200862408, 0.049562521278858185, 0.020038889721035957, 0.013940513134002686, 0.02103378437459469, 0.01746089570224285, 0.017113743349909782, 0.019913746044039726, 0.018859587609767914, 0.02026614546775818, 0.011505956761538982, 0.005817866418510675, 0.006613787729293108, 0.01797289028763771, 0.358878493309021, 0.3145928680896759], [0.06873436272144318, 0.011785660870373249, 0.027774067595601082, 0.024605071172118187, 0.019192488864064217, 0.016684502363204956, 0.016880303621292114, 0.01359256636351347, 0.02228783629834652, 0.033780574798583984, 0.01752837561070919, 0.004756651818752289, 0.001975921681150794, 0.0017915364587679505, 0.008760823868215084, 0.38024866580963135, 0.32962051033973694], [0.09014319628477097, 0.014069752767682076, 0.03779232129454613, 0.02926056832075119, 0.03334253281354904, 0.02170267514884472, 0.020592210814356804, 0.0216361116617918, 0.037955060601234436, 0.05369049310684204, 0.027533043175935745, 0.010999278165400028, 0.003725603921338916, 0.003226905595511198, 0.018713850528001785, 0.3031608462333679, 0.27245545387268066], [0.09682676196098328, 0.01569930836558342, 0.043329622596502304, 0.025556502863764763, 0.02706506848335266, 0.018725911155343056, 0.020777612924575806, 0.018718862906098366, 0.04110639914870262, 0.04270487651228905, 0.020876241847872734, 0.010483003221452236, 0.0040589431300759315, 0.0036192061379551888, 0.014925661496818066, 0.31499722599983215, 0.2805287837982178], [0.0676925778388977, 0.011506312526762486, 0.04737943783402443, 0.027854282408952713, 0.023243241012096405, 0.014764664694666862, 0.015544304624199867, 0.008804013952612877, 0.021672066301107407, 0.032459817826747894, 0.02054433710873127, 0.0071537792682647705, 0.0026029834989458323, 0.0023835052270442247, 0.009573616087436676, 0.3684810996055603, 0.3183399736881256], [0.08485181629657745, 0.01916385628283024, 0.039998169988393784, 0.029103707522153854, 0.020889801904559135, 0.019439371302723885, 0.016263220459222794, 0.005835741758346558, 0.011307485401630402, 0.01616106927394867, 0.018393639475107193, 0.005006179213523865, 0.0023922750260680914, 0.0020162388682365417, 0.007868279702961445, 0.374454528093338, 0.3268546760082245], [0.07344987243413925, 0.02102099359035492, 0.036573369055986404, 0.03466281667351723, 0.03012588992714882, 0.022477613762021065, 0.013852977193892002, 0.011191358789801598, 0.015058415941894054, 0.023511691018939018, 0.04233454540371895, 0.012157928198575974, 0.00495976721867919, 0.0046803904697299, 0.02225729078054428, 0.3357250988483429, 0.2959599494934082], [0.06521966308355331, 0.021879151463508606, 0.022827882319688797, 0.038690391927957535, 0.020669780671596527, 0.01579250395298004, 0.013476140797138214, 0.011269625276327133, 0.017591074109077454, 0.0339326336979866, 0.01584690809249878, 0.007264350540935993, 0.004385762847959995, 0.004914630204439163, 0.0255967415869236, 0.3600676953792572, 0.320575088262558], [0.08043267577886581, 0.023156898096203804, 0.04541035741567612, 0.02270244061946869, 0.018460342660546303, 0.019105350598692894, 0.015785548835992813, 0.01023855060338974, 0.011782058514654636, 0.013067501597106457, 0.02018658258020878, 0.005559457466006279, 0.0030324761755764484, 0.003583425423130393, 0.014263971708714962, 0.36621472239494324, 0.3270176351070404], [0.08257293701171875, 0.033177535980939865, 0.03387274593114853, 0.05854618176817894, 0.023244129493832588, 0.014425689354538918, 0.011877056211233139, 0.007115676533430815, 0.014965401031076908, 0.019704408943653107, 0.025384245440363884, 0.013349846005439758, 0.005301160737872124, 0.006249867845326662, 0.025109287351369858, 0.32912614941596985, 0.29597771167755127], [0.06403925269842148, 0.024918336421251297, 0.028058726340532303, 0.04473400488495827, 0.02163197100162506, 0.022079195827245712, 0.016582811251282692, 0.006415278650820255, 0.007796129211783409, 0.01290886290371418, 0.020855367183685303, 0.0125652514398098, 0.006811066530644894, 0.006225244607776403, 0.02322172001004219, 0.36027422547340393, 0.32088249921798706], [0.04414345324039459, 0.015258886851370335, 0.01865142211318016, 0.02389688417315483, 0.012162046507000923, 0.011862399987876415, 0.00872220378369093, 0.004127887077629566, 0.00510533107444644, 0.009609153494238853, 0.013731034472584724, 0.009259862825274467, 0.0054940031841397285, 0.005127092357724905, 0.019287945702672005, 0.425911545753479, 0.3676489293575287], [0.08103541284799576, 0.023349979892373085, 0.027309086173772812, 0.06285936385393143, 0.02967885136604309, 0.024891115725040436, 0.013892032206058502, 0.012654010206460953, 0.022169314324855804, 0.04166838526725769, 0.018618037924170494, 0.01412105467170477, 0.007783697452396154, 0.007488888222724199, 0.027736376971006393, 0.30728277564048767, 0.2774616479873657], [0.02048255316913128, 0.009985817596316338, 0.029202545061707497, 0.0063805850222706795, 0.0028625032864511013, 0.0026074638590216637, 0.002277733525261283, 0.0015410997439175844, 0.0022744310554116964, 0.005869763437658548, 0.011116286739706993, 0.0019650538451969624, 0.0013664636062458158, 0.0014695568243041635, 0.002963667269796133, 0.4941141903400421, 0.4035203158855438], [0.019980374723672867, 0.008944038301706314, 0.02702077105641365, 0.005841282196342945, 0.002674915362149477, 0.002402544254437089, 0.0020719049498438835, 0.001455554272979498, 0.002183695789426565, 0.00580208795145154, 0.009995615109801292, 0.001768807996995747, 0.001211265567690134, 0.0013089735293760896, 0.0027844365686178207, 0.4985518753528595, 0.4060017466545105]], [[0.04087253287434578, 0.04899127781391144, 0.019951459020376205, 0.04027614742517471, 0.013942131772637367, 0.01969481259584427, 0.00613133329898119, 0.002486844314262271, 0.004684981890022755, 0.011342890560626984, 0.028416447341442108, 0.013896206393837929, 0.010027785785496235, 0.00843551941215992, 0.016890058293938637, 0.38309359550476074, 0.3308660387992859], [0.015580756589770317, 0.003236076794564724, 0.00507330521941185, 0.005813367664813995, 0.0012054545804858208, 0.0019279413390904665, 0.0010676590027287602, 0.0011759183835238218, 0.001672261510975659, 0.015157152898609638, 0.003593508852645755, 0.0022122450172901154, 0.0011844886466860771, 0.001552657806314528, 0.005564878229051828, 0.5305068492889404, 0.4034753739833832], [0.020709462463855743, 0.007858972996473312, 0.01802317425608635, 0.00787723995745182, 0.00295468932017684, 0.004283736925572157, 0.0020625696051865816, 0.0018151220865547657, 0.0028391836676746607, 0.018555322661995888, 0.016914740204811096, 0.007514816243201494, 0.003242944134399295, 0.003601198084652424, 0.00950173381716013, 0.4813727140426636, 0.3908722996711731], [0.007169789634644985, 0.0028937035240232944, 0.0030627152882516384, 0.008163735270500183, 0.0026764296926558018, 0.006465306039899588, 0.002681790152564645, 0.005232071038335562, 0.0065077608451247215, 0.049105145037174225, 0.0026770092081278563, 0.0016956040635704994, 0.0017322965431958437, 0.0023749556858092546, 0.0095523027703166, 0.5182979702949524, 0.36971139907836914], [0.010252865962684155, 0.0031880130991339684, 0.00457390071824193, 0.007698275148868561, 0.0030899341218173504, 0.0047124046832323074, 0.0018449606141075492, 0.004778267815709114, 0.008479833602905273, 0.03945734351873398, 0.00550945894792676, 0.002273542806506157, 0.002542488742619753, 0.003217061050236225, 0.007408479694277048, 0.5178042650222778, 0.3731688857078552], [0.004827783443033695, 0.0018058588029816747, 0.0016634081257507205, 0.0032835439778864384, 0.0013086526887491345, 0.0023464541882276535, 0.0010282841976732016, 0.0025630691088736057, 0.005152398720383644, 0.023072047159075737, 0.0016265553422272205, 0.0010934107704088092, 0.0014245554339140654, 0.0017635425319895148, 0.004242854192852974, 0.5547558665275574, 0.3880416452884674], [0.003714249236509204, 0.0009478545980527997, 0.001015047891996801, 0.001476850127801299, 0.0004992211470380425, 0.0008276058943010867, 0.0004381192266009748, 0.0009180125198327005, 0.0023238612338900566, 0.009571659378707409, 0.0010848701931536198, 0.00048237445298582315, 0.00042798431240953505, 0.0005056786467321217, 0.0012953232508152723, 0.5792033672332764, 0.39526790380477905], [0.0038220263086259365, 0.0004924778477288783, 0.0013533348683267832, 0.001567220431752503, 0.0007274568197317421, 0.0010329489596188068, 0.0005205145571380854, 0.0007663234136998653, 0.0029416184406727552, 0.01302171777933836, 0.0008930957992561162, 0.00040773177170194685, 0.0005786314141005278, 0.0006008351338095963, 0.0020019235089421272, 0.5758399367332458, 0.39343222975730896], [0.007633873727172613, 0.0013413482811301947, 0.0026706058997660875, 0.001979388063773513, 0.0009320778772234917, 0.000974636001046747, 0.0005274128634482622, 0.001594884553924203, 0.004268311895430088, 0.019922565668821335, 0.0029234548565000296, 0.0008835275075398386, 0.002312365220859647, 0.003050355240702629, 0.003374301129952073, 0.5569514632225037, 0.38865944743156433], [0.007853168994188309, 0.001396996551193297, 0.00474838400259614, 0.0017480086535215378, 0.000664887425955385, 0.0008974221418611705, 0.0004794897395186126, 0.0008404675754718482, 0.0018201071070507169, 0.020619533956050873, 0.0048541235737502575, 0.0023054741322994232, 0.002813509665429592, 0.0031443864572793245, 0.006834807340055704, 0.5419919490814209, 0.3969872295856476], [0.025019526481628418, 0.011280760169029236, 0.014292287640273571, 0.010512543842196465, 0.004063582047820091, 0.006301195360720158, 0.004378480836749077, 0.001913669635541737, 0.002353200688958168, 0.018831469118595123, 0.02035234309732914, 0.008396022953093052, 0.002236283617094159, 0.0035035612527281046, 0.008493104949593544, 0.47211647033691406, 0.38595548272132874], [0.01879558339715004, 0.004660888575017452, 0.022090358659625053, 0.003546938067302108, 0.0016888026148080826, 0.0021323682740330696, 0.0007283138111233711, 0.00028571925940923393, 0.0007145697018131614, 0.006621259264647961, 0.025540603324770927, 0.0391940176486969, 0.009389017708599567, 0.00859945360571146, 0.014914039522409439, 0.46123266220092773, 0.3798654079437256], [0.007494691293686628, 0.0007393066771328449, 0.0031806507613509893, 0.0006357184029184282, 0.00032962148543447256, 0.0007275025709532201, 0.00018232944421470165, 0.0002344384993193671, 0.0003359720285516232, 0.007733604870736599, 0.0023374687880277634, 0.005939286667853594, 0.007318358402699232, 0.005992460530251265, 0.006608896888792515, 0.5480959415435791, 0.4021137058734894], [0.01290153618901968, 0.0008845376432873309, 0.004818354733288288, 0.0008116745739243925, 0.00044565662392415106, 0.0010069208219647408, 0.00027253510779701173, 0.0002798154891934246, 0.00047214250662364066, 0.00758373411372304, 0.0033147537615150213, 0.008945917710661888, 0.007716719526797533, 0.006647984962910414, 0.007429749704897404, 0.5331363081932068, 0.4033316671848297], [0.02009672112762928, 0.004259160254150629, 0.010465599596500397, 0.003577594878152013, 0.0014967551687732339, 0.002434690948575735, 0.0009498750441707671, 0.001568979350849986, 0.0023782378993928432, 0.02598830871284008, 0.007520449813455343, 0.011526611633598804, 0.01141401194036007, 0.011546371504664421, 0.02726113423705101, 0.4720030426979065, 0.38551247119903564], [0.006028568372130394, 0.001151459407992661, 0.003946523182094097, 0.001196644501760602, 0.0006840471178293228, 0.0007913595763966441, 0.00041667194454930723, 0.00023309446987695992, 0.0005538652767427266, 0.004537313710898161, 0.0034777859691530466, 0.0012262890813872218, 0.0007926560938358307, 0.0008323753136210144, 0.002810794161632657, 0.5492326021194458, 0.42208799719810486], [0.006215030327439308, 0.0011688158847391605, 0.00404434185475111, 0.001247212989255786, 0.0006863840972073376, 0.0007984119583852589, 0.0004162331752013415, 0.00023218047863338143, 0.0005391656304709613, 0.004439761396497488, 0.003582186298444867, 0.0012122795451432467, 0.0007492733420804143, 0.0007873353897593915, 0.0027609001845121384, 0.5489266514778137, 0.42219385504722595]], [[0.03693682327866554, 0.012644791044294834, 0.017499540001153946, 0.09894421696662903, 0.12607213854789734, 0.11525100469589233, 0.13356879353523254, 0.13821595907211304, 0.09766674786806107, 0.07762087881565094, 0.007351359818130732, 0.007991368882358074, 0.009318222291767597, 0.02057909034192562, 0.030471934005618095, 0.03681240230798721, 0.03305468335747719], [0.01830410398542881, 0.24325339496135712, 0.03867960721254349, 0.11928615719079971, 0.046648383140563965, 0.0449809655547142, 0.02802908979356289, 0.006607354618608952, 0.022624753415584564, 0.010985656641423702, 0.011522931046783924, 0.004340989049524069, 0.004898357205092907, 0.006239115726202726, 0.011119347997009754, 0.1964113563299179, 0.18606843054294586], [0.03543721139431, 0.04362751916050911, 0.1165996566414833, 0.10052122920751572, 0.06697621196508408, 0.055202241986989975, 0.052192363888025284, 0.024139486253261566, 0.03383197262883186, 0.04119529202580452, 0.04706423357129097, 0.017239265143871307, 0.00774714071303606, 0.010135495103895664, 0.035642173141241074, 0.15837346017360687, 0.15407495200634003], [0.003085244679823518, 0.02828320488333702, 0.007934827357530594, 0.5199498534202576, 0.07352603226900101, 0.15451478958129883, 0.07179340720176697, 0.02233598195016384, 0.03236522525548935, 0.018142443150281906, 0.0010848648380488157, 0.0007123777759261429, 0.0007089017308317125, 0.0009773263009265065, 0.003929974511265755, 0.031208885833621025, 0.029446685686707497], [0.0036877968814224005, 0.011617804877460003, 0.005327577702701092, 0.11912164837121964, 0.2028506100177765, 0.16248819231987, 0.19953632354736328, 0.04774104058742523, 0.18484599888324738, 0.02043307200074196, 0.001810439513064921, 0.000865211128257215, 0.0009449325734749436, 0.0015845686430111527, 0.0011135704116895795, 0.01844845712184906, 0.017582697793841362], [0.0015354547649621964, 0.0026936507783830166, 0.00175965647213161, 0.06921903789043427, 0.08117268979549408, 0.3160131573677063, 0.3321705758571625, 0.0753382220864296, 0.08097881078720093, 0.01703922636806965, 0.0005999912973493338, 0.0001822250196710229, 0.00043070121319033206, 0.0006521181203424931, 0.0004951075534336269, 0.010404802858829498, 0.009314577095210552], [0.0005940725095570087, 0.0005348366103135049, 0.0005771577125415206, 0.010275999084115028, 0.02253621816635132, 0.08337399363517761, 0.7289555668830872, 0.08609295636415482, 0.054056912660598755, 0.004540957044810057, 0.0003917148569598794, 6.39391946606338e-05, 0.00010591138561721891, 0.00015972617256920785, 8.525523298885673e-05, 0.0039739408530294895, 0.003680838504806161], [0.0010767712956294417, 0.0011999508133158088, 0.0010956906480714679, 0.012423815205693245, 0.017424022778868675, 0.04112739488482475, 0.2141256332397461, 0.5926994681358337, 0.07269582897424698, 0.012777374126017094, 0.0004979234654456377, 0.0003336209920234978, 0.000280627777101472, 0.000422547833295539, 0.0004824567004106939, 0.016195878386497498, 0.015141036361455917], [0.0008520635892637074, 0.0015373307978734374, 0.000945491308812052, 0.017815250903367996, 0.05498145893216133, 0.03775238245725632, 0.06763557344675064, 0.03359925374388695, 0.7493574619293213, 0.015766600146889687, 0.0007340415031649172, 0.00018959619046654552, 0.0001897361798910424, 0.0004112884635105729, 0.0001074048996088095, 0.009437058120965958, 0.008688155561685562], [0.004407721105962992, 0.0016104970127344131, 0.004205465782433748, 0.025757066905498505, 0.0232655331492424, 0.04821294918656349, 0.03476525843143463, 0.0908062756061554, 0.1310083568096161, 0.48177868127822876, 0.0029797563329339027, 0.001148990704677999, 0.001480467151850462, 0.003142598085105419, 0.003016530303284526, 0.0746270939707756, 0.06778673827648163], [0.0267587099224329, 0.029987508431077003, 0.047511037439107895, 0.0597589835524559, 0.082296222448349, 0.04758470878005028, 0.08547370880842209, 0.015458712354302406, 0.06754202395677567, 0.06972166150808334, 0.1693430095911026, 0.04403764009475708, 0.011537431739270687, 0.017569340765476227, 0.029544109478592873, 0.09958452731370926, 0.0962906926870346], [0.0006145374500192702, 0.0008788368431851268, 0.0011898494558408856, 0.0012702061794698238, 0.0006354560027830303, 0.000294840574497357, 0.0003296710201539099, 0.00025436608120799065, 0.0010032076388597488, 0.0013823941117152572, 0.001641000504605472, 0.9273926615715027, 0.012748888693749905, 0.01840522326529026, 0.007485288195312023, 0.012524858117103577, 0.011948750354349613], [0.0022370729129761457, 0.003983395639806986, 0.002861081389710307, 0.005829326808452606, 0.0037631308659911156, 0.0030121817253530025, 0.005391701124608517, 0.004615358542650938, 0.009987164288759232, 0.005227161571383476, 0.006058286875486374, 0.07722432166337967, 0.39722713828086853, 0.3623296916484833, 0.023130180314183235, 0.046687860041856766, 0.040434982627630234], [0.0031142132356762886, 0.003656619694083929, 0.003804524429142475, 0.007697802037000656, 0.0038353437557816505, 0.00293013546615839, 0.006306282244622707, 0.0051728179678320885, 0.011732684448361397, 0.00710596377030015, 0.0067934938706457615, 0.10074976831674576, 0.25479570031166077, 0.46628236770629883, 0.025047965347766876, 0.04834171012043953, 0.042632631957530975], [0.008058401755988598, 0.005391272250562906, 0.011018729768693447, 0.03262590244412422, 0.009312787093222141, 0.008567099459469318, 0.007457933388650417, 0.016664475202560425, 0.012169120833277702, 0.02496303990483284, 0.008156979456543922, 0.042912401258945465, 0.04039910063147545, 0.07175621390342712, 0.33061423897743225, 0.19571658968925476, 0.17421568930149078], [0.0032299640588462353, 0.0022357439156621695, 0.003958859946578741, 0.0041521149687469006, 0.0015986524522304535, 0.0024297023192048073, 0.001692679594270885, 0.001349943457171321, 0.0011152795050293207, 0.009396810084581375, 0.0033249896951019764, 0.003032808192074299, 0.0014640275621786714, 0.00134662922937423, 0.012659281492233276, 0.51871258020401, 0.4282999634742737], [0.0034225431736558676, 0.0023098390083760023, 0.004157997667789459, 0.004333216696977615, 0.0016705614980310202, 0.002521316986531019, 0.001769170630723238, 0.0013627678854390979, 0.0011519818799570203, 0.009296173229813576, 0.0034774350933730602, 0.0032643547747284174, 0.0015343872364610434, 0.0014336406020447612, 0.013425080105662346, 0.5156823992729187, 0.4291871190071106]], [[0.04262335225939751, 0.032685816287994385, 0.03414519876241684, 0.12817010283470154, 0.1672535538673401, 0.07302507013082504, 0.12464296072721481, 0.07614452391862869, 0.14075887203216553, 0.06756167858839035, 0.022979844361543655, 0.009578150697052479, 0.018310651183128357, 0.02164594642817974, 0.03051474690437317, 0.00485747866332531, 0.005102057009935379], [0.08480636775493622, 0.12136493623256683, 0.03970417380332947, 0.04921233281493187, 0.0483412966132164, 0.033262740820646286, 0.03069188818335533, 0.019481904804706573, 0.05389212444424629, 0.02666279673576355, 0.022710202261805534, 0.030517980456352234, 0.01255724299699068, 0.017322847619652748, 0.009491168893873692, 0.20760682225227356, 0.19237318634986877], [0.1097731664776802, 0.06464359164237976, 0.0344313345849514, 0.044315483421087265, 0.04860797896981239, 0.04069161415100098, 0.034885089844465256, 0.012281624600291252, 0.036651503294706345, 0.023815512657165527, 0.031063975766301155, 0.03474041819572449, 0.01829894445836544, 0.024071447551250458, 0.017903272062540054, 0.2217191457748413, 0.20210592448711395], [0.07102110236883163, 0.04567316547036171, 0.022483771666884422, 0.03880078345537186, 0.04924264922738075, 0.04612477496266365, 0.048389267176389694, 0.02746337279677391, 0.05457262322306633, 0.018524646759033203, 0.011109934188425541, 0.00978940725326538, 0.006499662529677153, 0.009152563288807869, 0.0029544385615736246, 0.2829570174217224, 0.2552407681941986], [0.05235425382852554, 0.024391282349824905, 0.02232474461197853, 0.0401715449988842, 0.07731572538614273, 0.07655055820941925, 0.12984558939933777, 0.06838399171829224, 0.0908784419298172, 0.03607063367962837, 0.01582624949514866, 0.014801126904785633, 0.013808723539113998, 0.019763611257076263, 0.006846357602626085, 0.1620991975069046, 0.14856795966625214], [0.028487656265497208, 0.008590533398091793, 0.016293581575155258, 0.030366908758878708, 0.07469384372234344, 0.12225382030010223, 0.2544800639152527, 0.10727515071630478, 0.07016085833311081, 0.020586276426911354, 0.00991162471473217, 0.010530276224017143, 0.008345386944711208, 0.01016874797642231, 0.0036231963895261288, 0.11817183345556259, 0.10606025159358978], [0.022733045741915703, 0.006356240250170231, 0.01697375252842903, 0.018864473327994347, 0.04918279871344566, 0.06314937025308609, 0.2012820839881897, 0.12528066337108612, 0.04459622502326965, 0.021617935970425606, 0.015462369658052921, 0.007754504214972258, 0.012921859510242939, 0.012262821197509766, 0.007039804011583328, 0.19972223043441772, 0.174799844622612], [0.03510234132409096, 0.011252894066274166, 0.022094905376434326, 0.010790396481752396, 0.04754559323191643, 0.04830048233270645, 0.10588711500167847, 0.14560236036777496, 0.04383402690291405, 0.015322086401283741, 0.015284842811524868, 0.015360248275101185, 0.018643897026777267, 0.013985227793455124, 0.005755268968641758, 0.23453180491924286, 0.2107064574956894], [0.025950850918889046, 0.016893193125724792, 0.017351387068629265, 0.022576123476028442, 0.05820976570248604, 0.06918915361166, 0.11385003477334976, 0.06675488501787186, 0.09818533807992935, 0.050962794572114944, 0.013993441127240658, 0.014707518741488457, 0.02682371623814106, 0.02779468707740307, 0.008850237354636192, 0.193037211894989, 0.1748696118593216], [0.03244540095329285, 0.012178190983831882, 0.01953929103910923, 0.01875932700932026, 0.016060898080468178, 0.03214330971240997, 0.02691029943525791, 0.02535499818623066, 0.018584758043289185, 0.045912016183137894, 0.011993412859737873, 0.01676700823009014, 0.020777301862835884, 0.016675133258104324, 0.019706018269062042, 0.35052555799484253, 0.31566712260246277], [0.0852314829826355, 0.05064568296074867, 0.028805280104279518, 0.027673989534378052, 0.03085233084857464, 0.028620904311537743, 0.032812073826789856, 0.012360180728137493, 0.038734320551157, 0.02312842197716236, 0.04968830943107605, 0.036481596529483795, 0.028539402410387993, 0.025492943823337555, 0.025056440383195877, 0.2501848042011261, 0.22569185495376587], [0.04321378096938133, 0.0130160478875041, 0.010785811580717564, 0.014206702820956707, 0.018754279240965843, 0.015315146185457706, 0.017957396805286407, 0.009399025700986385, 0.028042076155543327, 0.014434204436838627, 0.026531733572483063, 0.24405330419540405, 0.04826156795024872, 0.0423639640212059, 0.019152842462062836, 0.22702676057815552, 0.2074853926897049], [0.03756827861070633, 0.007962170988321304, 0.01581777259707451, 0.0108704948797822, 0.025837738066911697, 0.025618763640522957, 0.04316704720258713, 0.009841574355959892, 0.030212989076972008, 0.01364313904196024, 0.02956392988562584, 0.03357338160276413, 0.21844422817230225, 0.12297548353672028, 0.07293616235256195, 0.15668553113937378, 0.14528131484985352], [0.03853420540690422, 0.006847749929875135, 0.013041539117693901, 0.010523024946451187, 0.02740355208516121, 0.030041638761758804, 0.04994790256023407, 0.010979250073432922, 0.030826279893517494, 0.010641715489327908, 0.0233103446662426, 0.0429501086473465, 0.20003636181354523, 0.13725492358207703, 0.08591755479574203, 0.14552977681159973, 0.13621404767036438], [0.06434673815965652, 0.032081443816423416, 0.03576227277517319, 0.04687761515378952, 0.040726881474256516, 0.035957030951976776, 0.026490628719329834, 0.01011913362890482, 0.02154523693025112, 0.018289120867848396, 0.022181594744324684, 0.03086596541106701, 0.06426622718572617, 0.054951269179582596, 0.20918089151382446, 0.1474766582250595, 0.13888128101825714], [0.01765185035765171, 0.0057211644016206264, 0.0073248399421572685, 0.0033765092957764864, 0.0014312030980363488, 0.00238594738766551, 0.0019777927082031965, 0.0026393032167106867, 0.0015222792280837893, 0.005351503379642963, 0.006492800544947386, 0.00923335924744606, 0.004330779891461134, 0.004672625567764044, 0.011444738134741783, 0.4947623312473297, 0.41968095302581787], [0.018037481233477592, 0.005717602092772722, 0.007298585958778858, 0.003420337801799178, 0.0014224692713469267, 0.0023890924639999866, 0.0019900877960026264, 0.002582426881417632, 0.0015084927435964346, 0.005210932809859514, 0.006298894062638283, 0.008672717027366161, 0.004185513127595186, 0.004476230591535568, 0.011018342338502407, 0.4951023459434509, 0.42066845297813416]], [[0.013348407112061977, 0.5892276167869568, 0.036884866654872894, 0.10089405626058578, 0.010156293399631977, 0.006758794654160738, 0.0075773634016513824, 0.02159717306494713, 0.011591989547014236, 0.03532536327838898, 0.018845422193408012, 0.005935763008892536, 0.001477452926337719, 0.0016748129855841398, 0.008846188895404339, 0.06119343638420105, 0.06866499036550522], [0.0251320730894804, 0.04696216434240341, 0.016447776928544044, 0.026016000658273697, 0.008850333280861378, 0.009969623759388924, 0.00806871335953474, 0.006767106242477894, 0.005491621792316437, 0.0149696609005332, 0.005358272697776556, 0.0075142355635762215, 0.002900002058595419, 0.0038323260378092527, 0.020805882290005684, 0.42423099279403687, 0.3666832447052002], [0.011457022279500961, 0.008557185530662537, 0.004226328805088997, 0.00640045665204525, 0.002647761721163988, 0.005072271451354027, 0.0047717224806547165, 0.004385933745652437, 0.002959538483992219, 0.005954721011221409, 0.002951555885374546, 0.002831317950040102, 0.0016509615816175938, 0.0021808226592838764, 0.012101269327104092, 0.5087095499038696, 0.4131416380405426], [0.09919283539056778, 0.06691892445087433, 0.04316854104399681, 0.12949129939079285, 0.03180419281125069, 0.03870313987135887, 0.01828104443848133, 0.01233562733978033, 0.006630233954638243, 0.008417055942118168, 0.014048532582819462, 0.002341886516660452, 0.0010457794414833188, 0.0014586562756448984, 0.011219741776585579, 0.26643913984298706, 0.2485034167766571], [0.04814838245511055, 0.0323343500494957, 0.012633989565074444, 0.02159915491938591, 0.0071130841970443726, 0.010733402334153652, 0.007453076541423798, 0.00794775690883398, 0.003958265297114849, 0.00842915941029787, 0.0053171454928815365, 0.0024814640637487173, 0.0007665591547265649, 0.0011171381920576096, 0.00988695677369833, 0.4370495676994324, 0.383030503988266], [0.01057881023734808, 0.009123232215642929, 0.003990905359387398, 0.005209293216466904, 0.0019797247368842363, 0.004736763890832663, 0.004786851350218058, 0.0069626192562282085, 0.0022558767814189196, 0.0052269394509494305, 0.0019266813760623336, 0.0017667514039203525, 0.000662263308186084, 0.000768564292229712, 0.007108135614544153, 0.5092329382896423, 0.423683762550354], [0.003910561557859182, 0.005001799203455448, 0.0016137962229549885, 0.002462678588926792, 0.0013440987095236778, 0.00259424839168787, 0.003607109421864152, 0.007221780251711607, 0.0011558320838958025, 0.003289812011644244, 0.0009402522118762136, 0.0009453763486817479, 0.0003325098077766597, 0.0004162312834523618, 0.0025576907210052013, 0.5404976010322571, 0.4221087694168091], [0.004235594999045134, 0.007646473124623299, 0.0035148607566952705, 0.003782714484259486, 0.0014521474950015545, 0.002086311811581254, 0.002259115455672145, 0.006702665705233812, 0.0010527591221034527, 0.003125277115032077, 0.0024734155740588903, 0.0017579441191628575, 0.0008614115649834275, 0.000995535752736032, 0.0025391222443431616, 0.536186695098877, 0.41932785511016846], [0.005697541404515505, 0.0077314418740570545, 0.0033503477461636066, 0.0033183933701366186, 0.001614583539776504, 0.001804979401640594, 0.0015143261989578605, 0.0037326531019061804, 0.002086072228848934, 0.004513436462730169, 0.0016335969557985663, 0.0015976702561601996, 0.0007370132370851934, 0.001079568755812943, 0.004490355495363474, 0.5328519940376282, 0.42224591970443726], [0.006019308231770992, 0.006896634120494127, 0.002552431309595704, 0.0036105033941566944, 0.0014702263288199902, 0.0024191762786358595, 0.0018034835811704397, 0.003873622976243496, 0.001902445568703115, 0.008087074384093285, 0.0024350802414119244, 0.004121437203139067, 0.002935228869318962, 0.004163532517850399, 0.012653697282075882, 0.5265163779258728, 0.4085398018360138], [0.0076094320975244045, 0.004465941339731216, 0.0025191286113113165, 0.0031309323385357857, 0.001419996959157288, 0.00353671214543283, 0.003952328581362963, 0.0032544515561312437, 0.0013391674729064107, 0.005891207605600357, 0.005332736298441887, 0.003981836140155792, 0.0029767947271466255, 0.003764248685911298, 0.014893310144543648, 0.5126643180847168, 0.4192674458026886], [0.0012152731651440263, 0.0013380678137764335, 0.0009593567228876054, 0.0005234593991190195, 0.00018797100347001106, 0.0003541136684361845, 0.0004538890498224646, 0.00036062029539607465, 0.00023037343635223806, 0.0009384336299262941, 0.0011642175959423184, 0.009943879209458828, 0.003726227907463908, 0.0044529028236866, 0.005720004439353943, 0.5547088980674744, 0.41372233629226685], [0.001903558848425746, 0.0013775697443634272, 0.0013056955067440867, 0.0005984275485388935, 0.00019440338655840605, 0.0005309315747581422, 0.0005200232844799757, 0.0008383786771446466, 0.00048228524974547327, 0.001907898928038776, 0.0018474188400432467, 0.008372257463634014, 0.005377613008022308, 0.006086638662964106, 0.008283422328531742, 0.5530015826225281, 0.40737184882164], [0.0027448576875030994, 0.002097580349072814, 0.0023480060044676065, 0.0010153398616239429, 0.0002670359972398728, 0.0006657856283709407, 0.0005332643049769104, 0.0008921247790567577, 0.000611854309681803, 0.0027121442835778, 0.0027920075226575136, 0.011465848423540592, 0.00691786827519536, 0.007896012626588345, 0.010515334084630013, 0.5376785397529602, 0.40884634852409363], [0.014857618138194084, 0.0069013494066894054, 0.008127101697027683, 0.004377981182187796, 0.0011500745313242078, 0.0011734592262655497, 0.0011516418308019638, 0.0011876636417582631, 0.0011571673676371574, 0.004894732031971216, 0.008555317297577858, 0.006908381823450327, 0.0047094812616705894, 0.006181864999234676, 0.012502802535891533, 0.49936768412590027, 0.41679561138153076], [0.004880980122834444, 0.0034470544196665287, 0.00492881378158927, 0.00565085094422102, 0.0012229735730215907, 0.0011139464331790805, 0.0010227002203464508, 0.0011291068512946367, 0.0008563756127841771, 0.0018770259339362383, 0.0026952261105179787, 0.0023295884020626545, 0.001472671516239643, 0.001680702087469399, 0.002829947741702199, 0.5367128252983093, 0.4261492192745209], [0.004398631397634745, 0.003277009818702936, 0.004523895680904388, 0.005068687722086906, 0.0010518868220970035, 0.0009483323665335774, 0.0008783171069808304, 0.0009778712410479784, 0.0007506624679081142, 0.0017083557322621346, 0.0023773005232214928, 0.0020422283560037613, 0.0012504957849159837, 0.0014416155172511935, 0.002592079807072878, 0.5390812158584595, 0.4276314377784729]], [[0.008357861079275608, 0.12298183888196945, 0.06315892934799194, 0.028002724051475525, 0.004663706757128239, 0.004141395445913076, 0.009766745381057262, 0.032219406217336655, 0.020355142652988434, 0.10810505598783493, 0.04125705361366272, 0.018662123009562492, 0.0066543929278850555, 0.00596862705424428, 0.013095375150442123, 0.2563374936580658, 0.2562720775604248], [0.010872198268771172, 0.029165837913751602, 0.034305401146411896, 0.02225821278989315, 0.007476365193724632, 0.008673537522554398, 0.004138988442718983, 0.0030753454193472862, 0.004093660041689873, 0.01411068718880415, 0.011322609148919582, 0.001710550975985825, 0.000839501153677702, 0.0008384318207390606, 0.005204705987125635, 0.45004719495773315, 0.39186665415763855], [0.02004808746278286, 0.006894794758409262, 0.059362102299928665, 0.0159781314432621, 0.009481994435191154, 0.010640962980687618, 0.005507307127118111, 0.0035776463337242603, 0.005050698295235634, 0.01933763176202774, 0.03209514915943146, 0.006798236630856991, 0.003866017796099186, 0.003163133980706334, 0.006563075818121433, 0.4233536720275879, 0.36828139424324036], [0.010412278585135937, 0.025673503056168556, 0.027048585936427116, 0.053556621074676514, 0.024839920923113823, 0.04366527870297432, 0.020684028044342995, 0.007371590938419104, 0.007668294943869114, 0.020011985674500465, 0.007826532237231731, 0.0007098607020452619, 0.00035274052061140537, 0.0003955046704504639, 0.0029238476417958736, 0.4117231070995331, 0.3351362645626068], [0.017132095992565155, 0.010939424857497215, 0.036507271230220795, 0.029369965195655823, 0.015935508534312248, 0.023831220343708992, 0.023115603253245354, 0.009641392156481743, 0.010117709636688232, 0.02710900828242302, 0.022138170897960663, 0.0032168207690119743, 0.0008308090036734939, 0.0010164317209273577, 0.003508985973894596, 0.4141804575920105, 0.3514091670513153], [0.009534064680337906, 0.0057982211001217365, 0.01574007049202919, 0.018847038969397545, 0.01002340205013752, 0.01870289258658886, 0.01963610202074051, 0.007772537413984537, 0.007044986821711063, 0.019786985591053963, 0.009830712340772152, 0.0015427448088303208, 0.00037145204260014, 0.0004592186596710235, 0.0021140261087566614, 0.473508358001709, 0.3792871832847595], [0.013310357928276062, 0.004312599077820778, 0.018094897270202637, 0.016207057982683182, 0.007456323131918907, 0.011617647483944893, 0.016096537932753563, 0.006716148927807808, 0.007140332832932472, 0.018586523830890656, 0.011828338727355003, 0.0008806041441857815, 0.00025122170336544514, 0.00031645988929085433, 0.0016285795718431473, 0.478354275226593, 0.38720202445983887], [0.005523501429706812, 0.0032984993886202574, 0.010266420431435108, 0.016491275280714035, 0.006340687163174152, 0.011232113465666771, 0.011623630300164223, 0.0036171001847833395, 0.0038419540505856276, 0.012544365599751472, 0.005202597472816706, 0.0005656529101543128, 0.00018934110994450748, 0.00021894165547564626, 0.0005879473173990846, 0.5169944167137146, 0.3914615213871002], [0.005229472648352385, 0.0014734138967469335, 0.011814446188509464, 0.007145684212446213, 0.0019200396491214633, 0.001956849591806531, 0.0017004194669425488, 0.0012420743005350232, 0.002405282109975815, 0.013273072429001331, 0.005807541776448488, 0.0009991908445954323, 0.0002465284487698227, 0.0003437399282120168, 0.0013225468574091792, 0.5274561047554016, 0.41566357016563416], [0.01507869828492403, 0.0034906340297311544, 0.01302800141274929, 0.013452512212097645, 0.00440745847299695, 0.006155538372695446, 0.005396257620304823, 0.0029113469645380974, 0.007149799726903439, 0.06088299676775932, 0.010706651024520397, 0.003778051119297743, 0.0021516825072467327, 0.003422386245802045, 0.0046890852972865105, 0.4611477255821228, 0.3821512758731842], [0.012718360871076584, 0.0014866221463307738, 0.009711728431284428, 0.0034259725362062454, 0.0019762360025197268, 0.0018497402779757977, 0.0015293874312192202, 0.0010287713957950473, 0.001615237328223884, 0.011580009013414383, 0.017078330740332603, 0.007688047364354134, 0.003106588264927268, 0.0028358628042042255, 0.0029585573356598616, 0.5055423974990845, 0.4138680696487427], [0.0021003265865147114, 0.00012378316023387015, 0.0009073004475794733, 0.00013687832688447088, 0.00020726931688841432, 0.00012242226512171328, 9.285317355534062e-05, 8.605378388892859e-05, 0.00041211736970581114, 0.00492023816332221, 0.004401670303195715, 0.03480207547545433, 0.0038414569571614265, 0.002937271725386381, 0.0024252431467175484, 0.5302505493164062, 0.41223257780075073], [0.00288435909897089, 6.213825690792874e-05, 0.00167086161673069, 6.523619958898053e-05, 6.309874152066186e-05, 7.79729598434642e-05, 3.522929910104722e-05, 5.5339696700684726e-05, 0.00021104956977069378, 0.002868383191525936, 0.0028330127242952585, 0.0030009711626917124, 0.003553219372406602, 0.0033291317522525787, 0.0016104724491015077, 0.5492580533027649, 0.4284215271472931], [0.0027046329341828823, 4.8478006647201255e-05, 0.0015461953589692712, 5.534202864510007e-05, 5.649925151374191e-05, 8.137599070323631e-05, 3.5715525882551447e-05, 5.426954885479063e-05, 0.00020496491924859583, 0.0033223466016352177, 0.002544945804402232, 0.0028868226800113916, 0.0035917391069233418, 0.003612767206504941, 0.0017808162374421954, 0.5480808615684509, 0.4293921887874603], [0.018577806651592255, 0.0022404773626476526, 0.01305194292217493, 0.0049085537903010845, 0.004442318342626095, 0.007260248064994812, 0.004311555065214634, 0.0028809180948883295, 0.00785180926322937, 0.03264562413096428, 0.013309775851666927, 0.007949093356728554, 0.007913347333669662, 0.0098506398499012, 0.015217700973153114, 0.4582192003726959, 0.3893689513206482], [0.003380106296390295, 0.0016397469444200397, 0.004664350766688585, 0.002993073547258973, 0.002132612746208906, 0.0024366863071918488, 0.001536490977741778, 0.0009728334262035787, 0.002900557592511177, 0.010685326531529427, 0.004088018089532852, 0.003244174877181649, 0.0025294963270425797, 0.0026832015719264746, 0.007701968774199486, 0.5295950174331665, 0.41681644320487976], [0.003256042255088687, 0.0016275919042527676, 0.004465317353606224, 0.0028694162610918283, 0.0019485893426463008, 0.002192456740885973, 0.001379719702526927, 0.0008542374707758427, 0.0026350473053753376, 0.010093886405229568, 0.00388536648824811, 0.0030696308240294456, 0.0022464911453425884, 0.0024040858261287212, 0.007101954892277718, 0.5315609574317932, 0.41840922832489014]]], "negativeColor": "#1f78b4", "positiveColor": "#e31a1c", "tokens": ["[CLS]", "then", "i", "tried", "to", "find", "some", "way", "of", "embracing", "my", "mother", "'", "s", "ghost", ".", "[SEP]"]}
    )
    </script></div></div>
</div>
</section>
<section id="token-to-token-relationships">
<h3><span class="section-number">7.5.2. </span>Token-to-token relationships<a class="headerlink" href="#token-to-token-relationships" title="Link to this heading">#</a></h3>
<p>We can also look at token-to-token relationships. Below, for every layer in the
network, we find the token with the highest attention score for a target token.
Note that we will ignore <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> and <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> tokens for this bit of code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">highest_attention_tokens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">attentions</span><span class="p">:</span>
    <span class="c1"># Drop `[CLS]` and `[SEP]`, then take the max over the heads</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">layer</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">max_attention</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># March through each token and find the maximum value in the attention</span>
    <span class="c1"># layer</span>
    <span class="n">layer_result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
        <span class="n">highest_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">max_attention</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">highest_token</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">highest_idx</span><span class="p">]</span>
        <span class="n">layer_result</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">token</span><span class="p">,</span> <span class="n">highest_token</span><span class="p">))</span>

    <span class="c1"># Add to the buffer</span>
    <span class="n">highest_attention_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer_result</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>For the three layers above, we now print out every token in the input sequence
along with the token that scores highest in the attention matrix.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Layer: </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="se">\n</span><span class="s2">----------&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">highest_attention_tokens</span><span class="p">[</span><span class="n">idx</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">source</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Layer: 0
----------
then -&gt; tried
i -&gt; tried
tried -&gt; to
to -&gt; tried
find -&gt; tried
some -&gt; find
way -&gt; of
of -&gt; embracing
embracing -&gt; of
my -&gt; mother
mother -&gt; &#39;
&#39; -&gt; s
s -&gt; &#39;
ghost -&gt; .
. -&gt; ghost


Layer: 5
----------
then -&gt; tried
i -&gt; tried
tried -&gt; then
to -&gt; tried
find -&gt; way
some -&gt; way
way -&gt; of
of -&gt; way
embracing -&gt; ghost
my -&gt; mother
mother -&gt; s
&#39; -&gt; ghost
s -&gt; ghost
ghost -&gt; mother
. -&gt; then


Layer: 11
----------
then -&gt; .
i -&gt; .
tried -&gt; .
to -&gt; .
find -&gt; .
some -&gt; some
way -&gt; way
of -&gt; of
embracing -&gt; .
my -&gt; .
mother -&gt; mother
&#39; -&gt; .
s -&gt; .
ghost -&gt; .
. -&gt; .
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="examining-context">
<h2><span class="section-number">7.6. </span>Examining Context<a class="headerlink" href="#examining-context" title="Link to this heading">#</a></h2>
<p>Let’s now look at an example of how dynamic embeddings different from static
ones. We’ll use the Emily Dickinson poems from the first language modeling
chapter.</p>
<p>First, tokenize:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenized</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">poems</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">,</span>
    <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
    <span class="n">truncation</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Send the inputs to the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">bert</span><span class="p">(</span><span class="o">**</span><span class="n">tokenized</span><span class="p">,</span> <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="comparing-cls-tokens">
<h3><span class="section-number">7.6.1. </span>Comparing <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> tokens<a class="headerlink" href="#comparing-cls-tokens" title="Link to this heading">#</a></h3>
<p>With this done, we extract the original embeddings from the model for each
<code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> token. The indexing logic of the second line is as follows: for all
documents, select the first token and all features for that token. Then convert
to NumPy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">original_embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">static_cls</span> <span class="o">=</span> <span class="n">original_embeddings</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Get the dynamic embeddings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_cls</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">pooler_output</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Compute cosine similarity scores between the static and dynamic embeddings for
<code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cos_sim</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">static_cls</span><span class="p">,</span> <span class="n">dynamic_cls</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This returns a square matrix of all-to-all comparisons. We just need the
diagonal, which contains one-to-one similarities between documents. Extract
this and convert to a DataFrame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">cos_sim</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">scores</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cosine_similarity&quot;</span><span class="p">],</span> <span class="n">index</span> <span class="o">=</span> <span class="n">poems</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As expected, these scores will be quite low. Context matters!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cosine_similarity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>59.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.048242</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.011847</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.020555</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.041218</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.051783</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.056750</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.068510</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>What about the poems as a whole? Let’s look at how our embeddings change for
every layer in the network.</p>
</section>
<section id="defining-a-pooler">
<h3><span class="section-number">7.6.2. </span>Defining a pooler<a class="headerlink" href="#defining-a-pooler" title="Link to this heading">#</a></h3>
<p>Before we do that, however, we’ll define a pooler, which will produce
document-level embeddings for each poem. The pooler below takes the mean of all
tokens in a document. Importantly, it also removes <code class="docutils literal notranslate"><span class="pre">[PAD]</span></code> token embeddings
from the model outputs. While the model didn’t use these tokens to compute
attention, it still produces embeddings for them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mean_pool</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform mean pooling across an embedding layer.</span>

<span class="sd">    This is based on the mean pooling implementation in SBERT.</span>
<span class="sd">        SBERT: https://github.com/UKPLab/sentence-transformers</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    layer : torch.Tensor</span>
<span class="sd">        Embeddings layer with the shape (batch_size, num_tokens, num_dim)</span>
<span class="sd">    attention_mask : torch.Tensor</span>
<span class="sd">        Attention mask for the tokens with the shape (batch_size, num_tokens)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pooled : torch.Tensor</span>
<span class="sd">        Pooled embeddings with the shape (batch_size, num_dim)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Expand the attention mask to have the same size as the embeddings layer</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    
    <span class="c1"># Sum the embeddings multiplied by the mask. `[PAD]` tokens are 0s in</span>
    <span class="c1"># mask, so multiplication will remove those tokens&#39; values in the</span>
    <span class="c1"># embeddings</span>
    <span class="n">sum_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">layer</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Sum the mask and clamp it to avoid floating point errors in division</span>
    <span class="n">sum_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sum_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">sum_mask</span><span class="p">,</span> <span class="nb">min</span> <span class="o">=</span> <span class="mf">1e-9</span><span class="p">)</span>

    <span class="c1"># Take the mean</span>
    <span class="n">pooled</span> <span class="o">=</span> <span class="n">sum_layer</span> <span class="o">/</span> <span class="n">sum_mask</span>

    <span class="k">return</span> <span class="n">pooled</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s pool our original word embeddings matrix and look at the resultant shape.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tokenized</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
<span class="n">original_embeddings</span> <span class="o">=</span> <span class="n">mean_pool</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attention_mask</span><span class="p">)</span>
<span class="n">original_embeddings</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([59, 768])
</pre></div>
</div>
</div>
</div>
</section>
<section id="comparing-document-embeddings">
<h3><span class="section-number">7.6.3. </span>Comparing document embeddings<a class="headerlink" href="#comparing-document-embeddings" title="Link to this heading">#</a></h3>
<p>In the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop below, we step through each layer, then derive the cosine
similarity between the mean static embeddings for a poem and the layer’s mean
poem embeddings. Note that we start at index <code class="docutils literal notranslate"><span class="pre">1</span></code> because the first layer in the
hidden states is the original embeddings matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb2layer</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="c1"># Pool the layer</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">mean_pool</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">static</span><span class="p">,</span> <span class="n">dynamic</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">original_embeddings</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
        <span class="c1"># Compute cosine similarity</span>
        <span class="n">similarities</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">([</span><span class="n">static</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">])</span>

        <span class="c1"># `similarities` is a (2, 2) square matrix. We get the lower left</span>
        <span class="c1"># value, then append the layer and the score</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">similarities</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">score</span><span class="p">))</span>

    <span class="c1"># Add the layer&#39;s scores to our running list</span>
    <span class="n">emb2layer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Reformat into a DataFrame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb2layer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">emb2layer</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;layer&quot;</span><span class="p">,</span> <span class="s2">&quot;cosine_similarity&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Now we plot the document-level cosine similarity scores for each layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">emb2layer</span><span class="p">,</span>
    <span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;layer&quot;</span><span class="p">,</span>
    <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;cosine_similarity&quot;</span><span class="p">,</span>
    <span class="n">hue</span> <span class="o">=</span> <span class="s2">&quot;layer&quot;</span><span class="p">,</span>
    <span class="n">palette</span> <span class="o">=</span> <span class="s2">&quot;Paired&quot;</span><span class="p">,</span>
    <span class="n">legend</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Layer-wise Cosine Similarity Scores for Static -&gt; Dynamic Docs&quot;</span><span class="p">,</span>
    <span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Layer&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;Cosine similarity scores&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4ea11bcec7544bac3e5485619a8aa0d57ac8fa2d851a66ad8b1d5bbccdaad028.png" src="../_images/4ea11bcec7544bac3e5485619a8aa0d57ac8fa2d851a66ad8b1d5bbccdaad028.png" />
</div>
</div>
<p>This plot shows how, at every subsequent layer in our model, poem embeddings
further diverge from the original embeddings furnished by the model. One way to
interpret this progression is via context: at every layer, the model further
specifies context for the inputs, until the link between the context-less
embeddings and the contextual ones becomes quite weak.</p>
<p>Slightly modifying the above procedure will show this layer-by-layer change.
Below, we make our comparisons from one layer to the next.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layer2layer</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">previous</span> <span class="o">=</span> <span class="n">original_embeddings</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="c1"># Pool the layer</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">mean_pool</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">static</span><span class="p">,</span> <span class="n">dynamic</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">previous</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
        <span class="c1"># Compute cosine similarity</span>
        <span class="n">similarities</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">([</span><span class="n">static</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">])</span>

        <span class="c1"># `similarities` is a (2, 2) square matrix. We get the lower left</span>
        <span class="c1"># value, set up a step tracker, and append both</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">similarities</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">step</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">step</span><span class="p">,</span> <span class="n">score</span><span class="p">))</span>

    <span class="c1"># Add the layer transition scores to our running list</span>
    <span class="n">layer2layer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

    <span class="c1"># Set the current layer to `previous`</span>
    <span class="n">previous</span> <span class="o">=</span> <span class="n">layer</span>
</pre></div>
</div>
</div>
</div>
<p>Reformat.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layer2layer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">layer2layer</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">,</span> <span class="s2">&quot;cosine_similarity&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">layer2layer</span><span class="p">,</span>
    <span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;step&quot;</span><span class="p">,</span>
    <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;cosine_similarity&quot;</span><span class="p">,</span>
    <span class="n">hue</span> <span class="o">=</span> <span class="s2">&quot;step&quot;</span><span class="p">,</span>
    <span class="n">palette</span> <span class="o">=</span> <span class="s2">&quot;Paired&quot;</span><span class="p">,</span>
    <span class="n">legend</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Layer-to-layer Cosine Similarity Scores for Docs&quot;</span><span class="p">,</span>
    <span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Layer step&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;Cosine similarity scores&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ef2cb798d5a4bc404c238b7a3c9c14b40b6a607da123e4da1b2bb983e3763c83.png" src="../_images/ef2cb798d5a4bc404c238b7a3c9c14b40b6a607da123e4da1b2bb983e3763c83.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="06_vector-spaces.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">6. </span>Vector Space Semantics</p>
      </div>
    </a>
    <a class="right-next"
       href="08_bert.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8. </span>Bidirectional Encoder Representations from Transformers (BERT)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">7.1. Preliminaries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-a-pretrained-model">7.1.1. Using a pretrained model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-a-model">7.1.2. Loading a model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subword-tokenization">7.2. Subword Tokenization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-ids">7.2.1. Input IDs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-type-ids">7.2.2. Token type IDs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mask">7.2.3. Attention mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#padding-and-truncation">7.2.4. Padding and truncation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#components-of-the-transformer">7.3. Components of the Transformer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-embeddings">7.3.1. Input embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-default-embeddings">7.3.2. Other default embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">7.3.3. Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-transform">7.3.4. Linear transform</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-and-dropout">7.3.5. Normalization and dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-layer">7.3.6. Activation layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-the-model">7.4. Running the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-outputs">7.4.1. Model outputs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-layer-which-token">7.4.2. Which layer? Which token?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-attention">7.5. Examining Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-attention-weights">7.5.1. Visualizing attention weights</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-to-token-relationships">7.5.2. Token-to-token relationships</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-context">7.6. Examining Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-cls-tokens">7.6.1. Comparing <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> tokens</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-a-pooler">7.6.2. Defining a pooler</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-document-embeddings">7.6.3. Comparing document embeddings</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tyler Shoemaker
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>