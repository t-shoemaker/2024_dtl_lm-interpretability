
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>11. Attention in Vector Space &#8212; Introduction to Interpretability for Language Models</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/11_attention-in-vector-space';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="10. Cross Entropy" href="10_cross-entropy.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Introduction to Interpretability for Language Models</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Python Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_getting-started.html">1. Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_python-basics.html">2. Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_data-analysis-basics.html">3. Data Analysis in Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Language Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_ngram-models.html">4. N-gram Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_vectorization.html">5. Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_vector-spaces.html">6. Vector Space Semantics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Large Language Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="07_intro-to-llms.html">7. Large Language Models: An Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_bert.html">8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_gpt.html">9. Generative Pre-Trained Transformers (GPT)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_cross-entropy.html">10. Cross Entropy</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">11. Attention in Vector Space</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/t-shoemaker/2024_dtl_lm-interpretability" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/11_attention-in-vector-space.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Attention in Vector Space</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">11.1. Preliminaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extracting-attention">11.2. Extracting Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-functions">11.3. Plotting Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-self-attention">11.4. Visualizing Self-Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-projection">11.5. Vector Projection</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="attention-in-vector-space">
<h1><span class="section-number">11. </span>Attention in Vector Space<a class="headerlink" href="#attention-in-vector-space" title="Link to this heading">#</a></h1>
<p>This chapter demonstrates attention in terms of vector space operations.</p>
<section id="preliminaries">
<h2><span class="section-number">11.1. </span>Preliminaries<a class="headerlink" href="#preliminaries" title="Link to this heading">#</a></h2>
<p>We need the following libraries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll use BERT for our demonstration.</p>
<div class="cell tag_remove-stderr docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">attn_implementation</span> <span class="o">=</span> <span class="s2">&quot;eager&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="extracting-attention">
<h2><span class="section-number">11.2. </span>Extracting Attention<a class="headerlink" href="#extracting-attention" title="Link to this heading">#</a></h2>
<p>With the model loaded, we process a sentence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;Oh, this book? I&#39;ve enjoyed it.&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output_attentions</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we extract the layer attentions and build a list of tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">attentions</span> <span class="o">=</span> <span class="p">[</span><span class="n">attn</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">attn</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokid</span><span class="p">)</span> <span class="k">for</span> <span class="n">tokid</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll drop the <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> and <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> tokens for this demonstration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">attentions</span> <span class="o">=</span> <span class="p">[</span><span class="n">attn</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">attn</span> <span class="ow">in</span> <span class="n">attentions</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="plotting-functions">
<h2><span class="section-number">11.3. </span>Plotting Functions<a class="headerlink" href="#plotting-functions" title="Link to this heading">#</a></h2>
<p>We need to define two functions for visualizing attention in vector space. The
first one transforms attention embeddings into two-dimensional vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">to_xy_coords</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">summary_stat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">norm</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reduce the dimensionality of attention scores for plotting.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    scores : np.ndarray</span>
<span class="sd">        Multihead scores for a layer</span>
<span class="sd">    summary_stat : Callable</span>
<span class="sd">        What statistic to use to summarize the multiple attention heads</span>
<span class="sd">    norm : bool</span>
<span class="sd">        Whether to normalize the scores</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Calculate our summary stat. We need to do this to handle scores across</span>
    <span class="c1"># the multiple attention heads</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">summary_stat</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Reduce the dimensions of the data to XY coordinates</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

    <span class="c1"># Are we normalizing?</span>
    <span class="k">if</span> <span class="n">norm</span><span class="p">:</span>
        <span class="n">norm_by</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>
        <span class="n">xy</span> <span class="o">/=</span> <span class="n">norm_by</span>
    
    <span class="k">return</span> <span class="n">xy</span>
</pre></div>
</div>
</div>
</div>
<p>The second is the plotting function itself.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_vectors</span><span class="p">(</span>
    <span class="o">*</span><span class="n">vectors</span><span class="p">,</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">title</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot 2-dimensional vectors.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vectors : nd.ndarray</span>
<span class="sd">        Vectors to plot</span>
<span class="sd">    labels : list</span>
<span class="sd">        Labels for the vectors</span>
<span class="sd">    colors : list</span>
<span class="sd">        Vector colors (string names like &quot;black&quot;, &quot;red&quot;, etc.)</span>
<span class="sd">    fig : matplotlib.figure.Figure, optional</span>
<span class="sd">        Existing figure object to use for the plot</span>
<span class="sd">    ax : matplotlib.axes.Axes, optional</span>
<span class="sd">        Existing axis object to use for the plot</span>
<span class="sd">    title : str, optional</span>
<span class="sd">        Subplot title</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fig, ax : tuple</span>
<span class="sd">        The figure and axis</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Wrap vectors into a single array</span>
    <span class="n">vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
    <span class="n">n_vector</span><span class="p">,</span> <span class="n">n_dim</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">n_dim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;We can only plot 2-dimensional vectors&quot;</span><span class="p">)</span>

    <span class="c1"># Create a new figure and axis if not provided</span>
    <span class="k">if</span> <span class="n">fig</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="n">figsize</span><span class="p">)</span>

    <span class="c1"># Populate colors</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">colors</span><span class="p">:</span>
        <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;black&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_vector</span>

    <span class="c1"># Create a (0, 0) origin point for each vector</span>
    <span class="n">origin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_vector</span><span class="p">))</span>

    <span class="c1"># Then plot each vector, storing the handles and labels for each</span>
    <span class="n">handles</span><span class="p">,</span> <span class="n">handle_labels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vectors</span><span class="p">):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">labels</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">arrow</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span>
            <span class="o">*</span><span class="n">origin</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span>
            <span class="n">vector</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">vector</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">units</span> <span class="o">=</span> <span class="s2">&quot;xy&quot;</span><span class="p">,</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">label</span>
        <span class="p">)</span>
        <span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arrow</span><span class="p">)</span>
        <span class="n">handle_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

    <span class="c1"># Set plot limits</span>
    <span class="n">limit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">vectors</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">])</span>

    <span class="c1"># Set axes to be in the center of the plot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span>

    <span class="c1"># Remove the outer box</span>
    <span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">spine</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Set the title</span>
    <span class="k">if</span> <span class="n">title</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

    <span class="c1"># Return the figure, axis, handles, and labels</span>
    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handles</span><span class="p">,</span> <span class="n">handle_labels</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualizing-self-attention">
<h2><span class="section-number">11.4. </span>Visualizing Self-Attention<a class="headerlink" href="#visualizing-self-attention" title="Link to this heading">#</a></h2>
<p>With all of the above defined, we can now plot our attention scores in a
two-dimensional vector space. Remember: in this space, proximity means
similarity. During our vector space semantics session we derived this
information using the dot product, which tells us how much of one vector is
projected along another vector.</p>
<p>If you look back to the <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention()</span></code> function in chapter 7,
you’ll see that calculating attention is a souped-up version of the dot
product. Those matrix multiplication calls are effectively dot product
operations, e.g. in the example below.</p>
<p>For a query matrix <span class="math notranslate nohighlight">\(Q\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q = \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
    5 &amp; 6 \\
    \end{bmatrix}
\end{split}\]</div>
<p>A key matrix <span class="math notranslate nohighlight">\(K\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
K = \begin{bmatrix}
    7 &amp; 8 \\
    9 &amp; 10 \\
    \end{bmatrix}
\end{split}\]</div>
<p>…and its transpose <span class="math notranslate nohighlight">\(K^T\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
K^T = \begin{bmatrix}
    7 &amp; 9 \\
    8 &amp; 10 \\
    \end{bmatrix}
\end{split}\]</div>
<p>Multiplying the two together creates a scores matrix <span class="math notranslate nohighlight">\(S\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
S = \begin{bmatrix}
    (1 \cdot 7 + 2 \cdot 8) &amp; (1 \cdot 9 + 2 \cdot 10) \\
    (3 \cdot 7 + 4 \cdot 8) &amp; (3 \cdot 9 + 4 \cdot 10) \\
    (5 \cdot 7 + 6 \cdot 8) &amp; (5 \cdot 9 + 6 \cdot 10) \\
    \end{bmatrix}
\end{split}\]</div>
<p>Or:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
S = \begin{bmatrix}
    23 &amp; 29 \\
    53 &amp; 67 \\
    83 &amp; 105 \\
    \end{bmatrix}
\end{split}\]</div>
<p>For pairs of query and key vectors, we get a dot product score. That means
attention is just capturing information about how much the query vectors are
projected along the key vectors. For a given token in the input, attention
determines the orientation of that token to all other tokens. Then, it applies
that information to the value matrix as a weighting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set up a plot and roll through the attention layers</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">layer</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">attentions</span><span class="p">)):</span>
    <span class="c1"># Convert the attention scores to XY coordinates and produce some colors</span>
    <span class="c1"># for highlighting</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">to_xy_coords</span><span class="p">(</span><span class="n">layer</span><span class="p">))</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;tab20&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">xy</span><span class="p">))</span>

    <span class="c1"># Create a subplot for this layer</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handles</span><span class="p">,</span> <span class="n">handle_labels</span> <span class="o">=</span> <span class="n">plot_vectors</span><span class="p">(</span>
        <span class="o">*</span><span class="n">xy</span><span class="p">,</span>
        <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span><span class="p">,</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">,</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">fig</span><span class="p">,</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span>
        <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Annotate every row with the token labels</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span>
        <span class="n">handles</span><span class="p">,</span>
        <span class="n">handle_labels</span><span class="p">,</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper left&quot;</span><span class="p">,</span>
        <span class="n">bbox_to_anchor</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">fontsize</span> <span class="o">=</span> <span class="s2">&quot;small&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Show the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c56fe51b4cf27b929d9695374cfa041d662c74d14702db51d63ec297931beca8.png" src="../_images/c56fe51b4cf27b929d9695374cfa041d662c74d14702db51d63ec297931beca8.png" />
</div>
</div>
</section>
<section id="vector-projection">
<h2><span class="section-number">11.5. </span>Vector Projection<a class="headerlink" href="#vector-projection" title="Link to this heading">#</a></h2>
<p>Using the dot product, we can take two vectors, <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code>, and create a third
“projection” vector, which shows how much of <code class="docutils literal notranslate"><span class="pre">A</span></code> sits along the direction of
<code class="docutils literal notranslate"><span class="pre">B</span></code>. Attention is capturing this kind of information as it runs, but it’s
helpful to see the projection ourselves.</p>
<p>Let’s define a function to create this vector below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">vector_projection</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Project vector A onto B.</span>

<span class="sd">    Formula:</span>
<span class="sd">        (A•B / ||B||^2) * B</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    A, B: np.ndarray</span>
<span class="sd">        The two vectors</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    projection : np.ndarray</span>
<span class="sd">        The projection of A onto B</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ab_dot</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">B</span>
    <span class="n">b_magnitude_squared</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">B</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">projection</span> <span class="o">=</span> <span class="p">(</span><span class="n">ab_dot</span> <span class="o">/</span> <span class="n">b_magnitude_squared</span><span class="p">)</span> <span class="o">*</span> <span class="n">B</span>

    <span class="k">return</span> <span class="n">projection</span>
</pre></div>
</div>
</div>
</div>
<p>With the function defined, we project the attention vector for “it” onto the
one for “book” across every layer in BERT. This will create a new projection
vector whose orientation in vector space represents the amount of “it” along
“book.” Keep the following in mind as you inspect the result:</p>
<ul class="simple">
<li><p>If the projection vector tends toward the vector for “book,” this means more
of “it” is captured along “book”</p></li>
<li><p>If the projection vector tends away from the vector for “book,” this means
less of “it” is captured along “book”</p></li>
</ul>
<p>Given the nature of attention, what we would expect is that, at certain layer,
or set of layers, BERT will be able to determine that “book” and “it” refer to
the same thing. A major goal in mechanistic interpretability is to find out the
location of this behavior.</p>
<p>But for model training, the goal is this: the model should be better able to
capture the relationship between two tokens. How? It furnishes vectors for each
token, captures their relationship via the dot product to weight the vectors on
the basis of that relationship (i.e., it calculates attention), and uses the
weighted vectors to make a prediction. Then, based on how well it has made this
prediction, the model makes adjustments to the initial vectors it uses to
represent the tokens as well as the amount of weighting it uses to change those
vectors when it calculates attention.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the index positions for &quot;book&quot; and &quot;it&quot;</span>
<span class="n">book_idx</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">it_idx</span> <span class="o">=</span> <span class="mi">9</span>

<span class="c1"># Set up a plot and roll through the attention layers</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">layer</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">attentions</span><span class="p">)):</span>
    <span class="c1"># Convert the attention scores to XY coordinates. We turn off </span>
    <span class="c1"># normalization here because we&#39;re only focusing on two vectors, which</span>
    <span class="c1"># we&#39;ll normalize separately</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">to_xy_coords</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">norm</span> <span class="o">=</span> <span class="kc">False</span><span class="p">))</span>

    <span class="c1"># Select our two vectors, normalize them, and calculate the projection</span>
    <span class="c1"># vector</span>
    <span class="n">book</span><span class="p">,</span> <span class="n">it</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[</span><span class="n">book_idx</span><span class="p">],</span> <span class="n">xy</span><span class="p">[</span><span class="n">it_idx</span><span class="p">]</span>
    <span class="n">book</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">book</span><span class="p">)</span>
    <span class="n">it</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">it</span><span class="p">)</span>
    <span class="n">projection</span> <span class="o">=</span> <span class="n">vector_projection</span><span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="n">book</span><span class="p">)</span>

    <span class="c1"># Create our colors and labels</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="s2">&quot;red&quot;</span><span class="p">]</span>
    <span class="n">plot_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">book_idx</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">it_idx</span><span class="p">]]</span>
    <span class="n">plot_labels</span> <span class="o">+=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="n">it_idx</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; along &#39;</span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="n">book_idx</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">]</span>

    <span class="c1"># Create a subplot for this layer</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handles</span><span class="p">,</span> <span class="n">handle_labels</span> <span class="o">=</span> <span class="n">plot_vectors</span><span class="p">(</span>
        <span class="n">book</span><span class="p">,</span>
        <span class="n">it</span><span class="p">,</span>
        <span class="n">projection</span><span class="p">,</span>
        <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span><span class="p">,</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">plot_labels</span><span class="p">,</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">fig</span><span class="p">,</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span>
        <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Annotate every row with the token labels</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span>
        <span class="n">handles</span><span class="p">,</span>
        <span class="n">handle_labels</span><span class="p">,</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper left&quot;</span><span class="p">,</span>
        <span class="n">bbox_to_anchor</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">fontsize</span> <span class="o">=</span> <span class="s2">&quot;small&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Show the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a5d529caa3be368093f6e2931e8f26cfbcaca394dfbe58f1f468b041e282318b.png" src="../_images/a5d529caa3be368093f6e2931e8f26cfbcaca394dfbe58f1f468b041e282318b.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="10_cross-entropy.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Cross Entropy</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">11.1. Preliminaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extracting-attention">11.2. Extracting Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-functions">11.3. Plotting Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-self-attention">11.4. Visualizing Self-Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-projection">11.5. Vector Projection</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tyler Shoemaker
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>