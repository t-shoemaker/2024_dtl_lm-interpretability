
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6. Vector Space Semantics &#8212; Introduction to Interpretability for Language Models</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/06_vector-spaces';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="5. Vectorization" href="05_vectorization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Introduction to Interpretability for Language Models</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Python Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_getting-started.html">1. Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_python-basics.html">2. Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_data-analysis-basics.html">3. Data Analysis in Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Language Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_ngram-models.html">4. N-gram Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_vectorization.html">5. Vectorization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. Vector Space Semantics</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/t-shoemaker/2024_dtl_lm-interpretability" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/06_vector-spaces.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Vector Space Semantics</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">6.1. Preliminaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-space">6.2. Vector Space</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-components">6.2.1. Vector components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-operations">6.2.2. Vector operations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-similarity">6.3. Cosine Similarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-similarity">6.3.1. Document similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-similarity">6.3.2. Token similarity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embeddings">6.4. Word Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-similarity-redux">6.4.1. Token similarity (redux)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#concept-modeling">6.4.2. Concept modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analogies">6.4.3. Analogies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-of-speech-disambiguation">6.5. Part-of-Speech Disambiguation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">6.5.1. Data preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-a-logistic-regression-model">6.5.2. Fitting a logistic regression model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-model-coefficients">6.5.3. Examining model coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tag-associations">6.5.4. Tag associations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shifting-vectors">6.5.5. Shifting vectors</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="vector-space-semantics">
<h1><span class="section-number">6. </span>Vector Space Semantics<a class="headerlink" href="#vector-space-semantics" title="Link to this heading">#</a></h1>
<p>This chapter overviews vector space semantics. It introduces core operations on
vectors and vector spaces and demonstrates how to use these operations to
define a measure of of semantic similarity. We then turn to static word
embeddings to discuss concept modeling as well as embedding analogies; a final
experiment demonstrates how to disambiguate part-of-speech tags in embeddings.</p>
<ul class="simple">
<li><p><strong>Data:</strong> A document-term matrix representation of Melanie Walsh’s
<a class="reference external" href="https://melaniewalsh.github.io/Intro-Cultural-Analytics/00-Datasets/00-Datasets.html#politics-history">corpus</a> of ~380 obituaries from the <em>New York Times</em> and tagged
<a class="reference external" href="https://wordnet.princeton.edu/">WordNet</a> embeddings</p></li>
<li><p><strong>Credits:</strong> Portions of this chapter are adapted from the UC Davis DataLab’s
<a class="reference external" href="https://ucdavisdatalab.github.io/workshop_nlp_reader">Natural Language Processing for Data Science</a></p></li>
</ul>
<section id="preliminaries">
<h2><span class="section-number">6.1. </span>Preliminaries<a class="headerlink" href="#preliminaries" title="Link to this heading">#</a></h2>
<p>We need the following libraries:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Normalizer</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<p>Our documents have already been vectorized into a document-term matrix (DTM).
The only consideration is that the tokens have been <strong>lemmatized</strong>, that is,
reduced to their uninflected forms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dtm</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s2">&quot;data/datasets/nyt_obituaries_dtm.parquet&quot;</span><span class="p">)</span>
<span class="n">dtm</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Index: 379 entries, Ada Lovelace to Karen Sparck Jones
Columns: 31165 entries, aachen to zwilich
dtypes: int64(31165)
memory usage: 90.1+ MB
</pre></div>
</div>
</div>
</div>
</section>
<section id="vector-space">
<h2><span class="section-number">6.2. </span>Vector Space<a class="headerlink" href="#vector-space" title="Link to this heading">#</a></h2>
<p>In the last chapter we plotted Henry James chapters in a two-dimensional
scatter plot. We can do the same with our DTM using the function below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scatter_2d</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">norm</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">highlight</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot a matrix in 2D vector space.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    matrix : np.ndarray</span>
<span class="sd">        Matrix to plot</span>
<span class="sd">    norm : bool</span>
<span class="sd">        Whether to normalize the matrix values</span>
<span class="sd">    highlight : None or list</span>
<span class="sd">        List of indices to highlight</span>
<span class="sd">    figsize : tuple</span>
<span class="sd">        Figure size</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Find a way to reduce the matrix to two dimensions with PCA, adding</span>
    <span class="c1"># optional normalization along the way</span>
    <span class="k">if</span> <span class="n">norm</span><span class="p">:</span>
        <span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">Normalizer</span><span class="p">(),</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pipeline</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">reduced</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
    <span class="n">vis_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reduced</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">])</span>

    <span class="c1"># Plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="n">figsize</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">vis_data</span><span class="p">)</span>

    <span class="c1"># Highlight markers, if applicable</span>
    <span class="k">if</span> <span class="n">highlight</span><span class="p">:</span>
        <span class="n">selected</span> <span class="o">=</span> <span class="n">vis_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">highlight</span><span class="p">]</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
            <span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">selected</span>
        <span class="p">)</span>

    <span class="n">g</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Dim. 1&quot;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;Dim. 2&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Below, we plot the DTM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scatter_2d</span><span class="p">(</span><span class="n">dtm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4baeceab7b36f0330cb8b16272f21c45d3ae839a08aa3111eb8c10b144dc5ffb.png" src="../_images/4baeceab7b36f0330cb8b16272f21c45d3ae839a08aa3111eb8c10b144dc5ffb.png" />
</div>
</div>
<p>Doing so projects our matrix into a two-dimensional <strong>vector space</strong>. In the
reigning metaphor of NLP, a space of this sort is a stand-in for meaning: the
closer two points are in the scatter plot, the more similar they are in
meaning.</p>
<p>Selecting the names of two related people in the obituaries will make this
clear.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Bela Bartok&quot;</span><span class="p">,</span> <span class="s2">&quot;Maurice Ravel&quot;</span><span class="p">]</span>
<span class="n">highlight</span> <span class="o">=</span> <span class="p">[</span><span class="n">dtm</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">]</span>
<span class="n">scatter_2d</span><span class="p">(</span><span class="n">dtm</span><span class="p">,</span> <span class="n">highlight</span> <span class="o">=</span> <span class="n">highlight</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a994a1b612dd2b83182ba649d100435ef39c570c5e9250d99676e240404781a4.png" src="../_images/a994a1b612dd2b83182ba649d100435ef39c570c5e9250d99676e240404781a4.png" />
</div>
</div>
<p>Let’s add in a third that we’d expect to be less similar.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Bela Bartok&quot;</span><span class="p">,</span> <span class="s2">&quot;Maurice Ravel&quot;</span><span class="p">,</span> <span class="s2">&quot;FDR&quot;</span><span class="p">]</span>
<span class="n">highlight</span> <span class="o">=</span> <span class="p">[</span><span class="n">dtm</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">]</span>
<span class="n">scatter_2d</span><span class="p">(</span><span class="n">dtm</span><span class="p">,</span> <span class="n">highlight</span> <span class="o">=</span> <span class="n">highlight</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/035826f337d1521bc874e03607f2a5a11bc50fa7aed3e5fc2abc78797e1bec07.png" src="../_images/035826f337d1521bc874e03607f2a5a11bc50fa7aed3e5fc2abc78797e1bec07.png" />
</div>
</div>
<p>While we can only visualize these similarities in two- or three-dimensional
spaces, which are called <strong>Euclidean spaces</strong> (i.e., they conform to physical
space), the same idea—and importantly, the math—holds for similarity in
high-dimensional spaces. But before we turn to what similarity means in vector
space, we’ll overview how vectors work generally.</p>
<p>Our two example vectors will be the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">dtm</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">&quot;Lucille Ball&quot;</span><span class="p">],</span> <span class="n">dtm</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">&quot;Carl Sagan&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>And we will limit ourselves to only two dimensions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">terms</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;television&quot;</span><span class="p">,</span> <span class="s2">&quot;star&quot;</span><span class="p">]</span>
<span class="n">A</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">terms</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">terms</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<section id="vector-components">
<h3><span class="section-number">6.2.1. </span>Vector components<a class="headerlink" href="#vector-components" title="Link to this heading">#</a></h3>
<p>A vector has a <strong>magnitude</strong> and a <strong>direction</strong>.</p>
<p><strong>Magnitude</strong></p>
<ul class="simple">
<li><p>Description: The length of a vector from its origin to its end point. This is
calculated as the square root of the sum of squares of its components</p></li>
<li><p>Notation: <span class="math notranslate nohighlight">\(||A|| = \sqrt{a_1^2 + a_2^2 + \dots + a_n^2}\)</span></p></li>
<li><p>Result: Single value (scalar)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">manual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span>
<span class="n">numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">manual</span> <span class="o">==</span> <span class="n">numpy</span><span class="p">,</span> <span class="s2">&quot;Magnitudes don&#39;t match!&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12.649110640673518
</pre></div>
</div>
</div>
</div>
<p><strong>Direction</strong></p>
<ul class="simple">
<li><p>Description: The orientation of a vector in space. In Cartesian coordinates,
it is the angles a vector forms across its axes. But direction can also be
represented as a second <strong>unit vector</strong>, a vector of magnitude 1 that points
in the same direction as the first. Most vector operations in NLP use the
latter</p></li>
<li><p>Notation: <span class="math notranslate nohighlight">\(\hat{A} = \frac{A}{||A||}\)</span></p></li>
<li><p>Result: Vector of length <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>television    0.948683
star          0.316228
Name: Lucille Ball, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Let’s plot our two vectors to show their magnitude and orientation.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Show vector plot code</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_vectors</span><span class="p">(</span>
    <span class="o">*</span><span class="n">vectors</span><span class="p">,</span>
    <span class="n">vector_labels</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">axis_labels</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot 2-dimensional vectors.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vectors : nd.ndarray</span>
<span class="sd">        Vectors to plot</span>
<span class="sd">    vector_labels : list</span>
<span class="sd">        Labels for the vectors</span>
<span class="sd">    axis_labels : list</span>
<span class="sd">        Labels for the axes in (x, y) order</span>
<span class="sd">    colors : list</span>
<span class="sd">        Vector colors (string names like &quot;black&quot;, &quot;red&quot;, etc.)</span>
<span class="sd">    figsize : tuple</span>
<span class="sd">        Figure size</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Wrap vectors into a single array</span>
    <span class="n">vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
    <span class="n">n_vector</span><span class="p">,</span> <span class="n">n_dim</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">n_dim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;We can only plot 2-dimensional vectors&quot;</span><span class="p">)</span>

    <span class="c1"># Populate colors</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">colors</span><span class="p">:</span>
        <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;black&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_vector</span>

    <span class="c1"># Create a (0, 0) origin point for each vector</span>
    <span class="n">origin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_vector</span><span class="p">))</span>

    <span class="c1"># Then plot each vector</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="n">figsize</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vectors</span><span class="p">):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span>
            <span class="o">*</span><span class="n">origin</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">],</span>
            <span class="n">vector</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">vector</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">units</span> <span class="o">=</span> <span class="s2">&quot;xy&quot;</span><span class="p">,</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">vector_labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">if</span> <span class="n">vector_labels</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>

    <span class="c1"># Set plot limits</span>
    <span class="n">limit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">vectors</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">])</span>

    <span class="c1"># Set axes to be in the center of the plot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span>

    <span class="c1"># Remove the outer box</span>
    <span class="k">for</span> <span class="n">spine</span> <span class="ow">in</span> <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">spine</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Add axis labels, if applicable</span>
    <span class="k">if</span> <span class="n">axis_labels</span><span class="p">:</span>
        <span class="n">xlab</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="n">axis_labels</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlab</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">ylab</span><span class="p">)</span>

    <span class="c1"># Add a label legend, if applicable</span>
    <span class="k">if</span> <span class="n">vector_labels</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Show the plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vector_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;L. Ball&quot;</span><span class="p">,</span> <span class="s2">&quot;C. Sagan&quot;</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="s2">&quot;blue&quot;</span><span class="p">]</span>
<span class="n">plot_vectors</span><span class="p">(</span>
    <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">vector_labels</span> <span class="o">=</span> <span class="n">vector_labels</span><span class="p">,</span> <span class="n">axis_labels</span> <span class="o">=</span> <span class="n">terms</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c9a0759db2a9340fc37b8f920537ad2b0b9350be9c8c67aa10a6063278c923e3.png" src="../_images/c9a0759db2a9340fc37b8f920537ad2b0b9350be9c8c67aa10a6063278c923e3.png" />
</div>
</div>
<p>We can normalize our vectors by their direction. This will make the magnitude
of each vector equal to 1. You’ll see this operation called <strong>L2
normalization</strong>. (In <code class="docutils literal notranslate"><span class="pre">scatter_2d</span></code> above, this is what the <code class="docutils literal notranslate"><span class="pre">Normalizer</span></code> object
does.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">l2_norm</span><span class="p">(</span><span class="n">vector</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform L2 normalization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vector : np.ndarray</span>
<span class="sd">        Vector to normalize</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    vector : np.ndarray</span>
<span class="sd">        Normed vector</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">norm_by</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vector</span> <span class="o">/</span> <span class="n">norm_by</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_vectors</span><span class="p">(</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">A</span><span class="p">),</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
    <span class="n">vector_labels</span> <span class="o">=</span> <span class="n">vector_labels</span><span class="p">,</span> 
    <span class="n">axis_labels</span> <span class="o">=</span> <span class="n">terms</span><span class="p">,</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f28ffd8960b9be8d327c4e9f797537299cdaa9686d93ebdbe378f9871ff5a396.png" src="../_images/f28ffd8960b9be8d327c4e9f797537299cdaa9686d93ebdbe378f9871ff5a396.png" />
</div>
</div>
</section>
<section id="vector-operations">
<h3><span class="section-number">6.2.2. </span>Vector operations<a class="headerlink" href="#vector-operations" title="Link to this heading">#</a></h3>
<p>We turn now to basic operations you can perform on/with vectors.</p>
<p><strong>Summation</strong></p>
<ul class="simple">
<li><p>Description: Element-wise sums</p></li>
<li><p>Notation: <span class="math notranslate nohighlight">\(A + B = (a_1 + b_1, a_2 + b_2, \dots, a_n + b_n)\)</span></p></li>
<li><p>Result: Vector of length <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">B</span>
<span class="n">plot_vectors</span><span class="p">(</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">A</span><span class="p">),</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">C</span><span class="p">),</span>
    <span class="n">vector_labels</span> <span class="o">=</span> <span class="n">vector_labels</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Result&quot;</span><span class="p">],</span> 
    <span class="n">axis_labels</span> <span class="o">=</span> <span class="n">terms</span><span class="p">,</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;green&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/61561ae614be4d584a4148e96ceb29cce539355a73d6614f45e8f8c5d1939e5f.png" src="../_images/61561ae614be4d584a4148e96ceb29cce539355a73d6614f45e8f8c5d1939e5f.png" />
</div>
</div>
<p><strong>Subtraction</strong></p>
<ul class="simple">
<li><p>Description: Element-wise differences</p></li>
<li><p>Notation: <span class="math notranslate nohighlight">\(A - B = (a_1 - b_1, a_2 - b_2, \dots, a_n - b_n)\)</span></p></li>
<li><p>Result: Vector of length <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">B</span>
<span class="n">plot_vectors</span><span class="p">(</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">A</span><span class="p">),</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">C</span><span class="p">),</span>
    <span class="n">vector_labels</span> <span class="o">=</span> <span class="n">vector_labels</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Result&quot;</span><span class="p">],</span> 
    <span class="n">axis_labels</span> <span class="o">=</span> <span class="n">terms</span><span class="p">,</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;green&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/215ca0c40a7165912b80d5a64ad9a9e6cbe2da745a2c85f970a514a27df4737a.png" src="../_images/215ca0c40a7165912b80d5a64ad9a9e6cbe2da745a2c85f970a514a27df4737a.png" />
</div>
</div>
<p><strong>Multiplication, element-wise</strong></p>
<ul class="simple">
<li><p>Description: Element-wise products</p></li>
<li><p>Notation: <span class="math notranslate nohighlight">\(A \circ B = (a_1 \cdot b_1, a_2 \cdot b_2, \dots, a_n \cdot b_n)\)</span></p></li>
<li><p>Result: Vector of length <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">A</span> <span class="o">*</span> <span class="n">B</span>
<span class="n">plot_vectors</span><span class="p">(</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">A</span><span class="p">),</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
    <span class="n">l2_norm</span><span class="p">(</span><span class="n">C</span><span class="p">),</span>
    <span class="n">vector_labels</span> <span class="o">=</span> <span class="n">vector_labels</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Result&quot;</span><span class="p">],</span> 
    <span class="n">axis_labels</span> <span class="o">=</span> <span class="n">terms</span><span class="p">,</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;green&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e51377282bfaa3be2c727f5239e3000345b23b2d8b065d9975463131b94d0792.png" src="../_images/e51377282bfaa3be2c727f5239e3000345b23b2d8b065d9975463131b94d0792.png" />
</div>
</div>
<p><strong>Multiplication, dot product</strong></p>
<ul class="simple">
<li><p>Description: The sum of the products</p></li>
<li><p>Notation: <span class="math notranslate nohighlight">\(A \cdot B = \Sigma_{i=1}^{n} a_i \cdot b_i\)</span></p></li>
<li><p>Result: Single value (scalar)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">@</span> <span class="n">B</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>80
</pre></div>
</div>
</div>
</div>
<p>The dot product is one of the most important operations in modern machine
learning. It measures the extent to which two vectors point in the same
direction. If the dot product is positive, the angle between two vectors is
under 90 degrees. This means they point somewhat in the same direction. If it
is negative, they point in opposite directions. And when the dot product is
zero, the vectors are perpendicular.</p>
</section>
</section>
<section id="cosine-similarity">
<h2><span class="section-number">6.3. </span>Cosine Similarity<a class="headerlink" href="#cosine-similarity" title="Link to this heading">#</a></h2>
<p>Recall that, initially, we wanted to understand how similar two vectors are. We
can do so with the dot product. The dot product allows us to derive a measure
of how similar two vectors are in their orientation in vector space by
considering the angle formed between them. This measure is called <strong>cosine
similarity</strong>, and it is a quintessential method for working in semantic space.</p>
<p>We express it as follows:</p>
<div class="math notranslate nohighlight">
\[
cos \theta = \frac{A \cdot B}{||A||||B||}
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(cos \theta\)</span>: cosine of the angle <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A \cdot B\)</span>: dot product of A and B</p></li>
<li><p><span class="math notranslate nohighlight">\(||A||||B||\)</span>: the product of A and B’s magnitudes</p></li>
</ul>
<p>The denominator is a normalization operation, similar in nature to L2
normalization. It helps control for magnitude variance between vectors, which
is important when documents are very different in length.</p>
<p>The function below derives a cosine similarity score for two vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_cosine_similarity</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate cosine similarity between two vectors.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    A : np.ndarray</span>
<span class="sd">        First vector</span>
<span class="sd">    B : np.ndarray</span>
<span class="sd">        Second vector</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    cos_sim : float</span>
<span class="sd">        Cosine similarity of A and B</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dot</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">B</span>
    <span class="n">norm_by</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">dot</span> <span class="o">/</span> <span class="n">norm_by</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">cos_sim</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Scores are between <span class="math notranslate nohighlight">\([-1, 1]\)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(1\)</span>: same orientation; perfect similarity</p></li>
<li><p><span class="math notranslate nohighlight">\(0\)</span>: orthogonal vectors; vectors have nothing in common</p></li>
<li><p><span class="math notranslate nohighlight">\(-1\)</span>: opposite orientation; vectors are the opposite of one another</p></li>
</ul>
<p>Here’s A and A:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calculate_cosine_similarity</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
</pre></div>
</div>
</div>
</div>
<p>A and B:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calculate_cosine_similarity</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.89442719
</pre></div>
</div>
</div>
</div>
<p>And A and its opposite:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calculate_cosine_similarity</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="o">-</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-1.0
</pre></div>
</div>
</div>
</div>
<section id="document-similarity">
<h3><span class="section-number">6.3.1. </span>Document similarity<a class="headerlink" href="#document-similarity" title="Link to this heading">#</a></h3>
<p>Typically, however, you’d just use the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> implementation. It
returns a square matrix comparing each vector with every other vector in the
input. Below, we run it across our DTM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc_sim</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">dtm</span><span class="p">)</span>
<span class="n">doc_sim</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">doc_sim</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">dtm</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">dtm</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">doc_sim</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Ada Lovelace</th>
      <th>Robert E Lee</th>
      <th>Andrew Johnson</th>
      <th>Bedford Forrest</th>
      <th>Lucretia Mott</th>
      <th>Charles Darwin</th>
      <th>Ulysses Grant</th>
      <th>Mary Ewing Outerbridge</th>
      <th>Emma Lazarus</th>
      <th>Louisa M Alcott</th>
      <th>...</th>
      <th>Bella Abzug</th>
      <th>Fred W Friendly</th>
      <th>Frank Sinatra</th>
      <th>Hassan II</th>
      <th>Iris Murdoch</th>
      <th>King Hussein</th>
      <th>Pierre Trudeau</th>
      <th>Elliot Richardson</th>
      <th>Charles M Schulz</th>
      <th>Karen Sparck Jones</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Ada Lovelace</th>
      <td>1.000000</td>
      <td>0.072500</td>
      <td>0.095733</td>
      <td>0.101333</td>
      <td>0.091677</td>
      <td>0.141985</td>
      <td>0.170934</td>
      <td>0.087086</td>
      <td>0.140926</td>
      <td>0.130201</td>
      <td>...</td>
      <td>0.157298</td>
      <td>0.116398</td>
      <td>0.099424</td>
      <td>0.108706</td>
      <td>0.194042</td>
      <td>0.102954</td>
      <td>0.131275</td>
      <td>0.120062</td>
      <td>0.135016</td>
      <td>0.289609</td>
    </tr>
    <tr>
      <th>Robert E Lee</th>
      <td>0.072500</td>
      <td>1.000000</td>
      <td>0.250310</td>
      <td>0.292756</td>
      <td>0.084679</td>
      <td>0.119690</td>
      <td>0.487814</td>
      <td>0.093315</td>
      <td>0.124351</td>
      <td>0.129774</td>
      <td>...</td>
      <td>0.121218</td>
      <td>0.101031</td>
      <td>0.104586</td>
      <td>0.161621</td>
      <td>0.125416</td>
      <td>0.108918</td>
      <td>0.135583</td>
      <td>0.167889</td>
      <td>0.088386</td>
      <td>0.076981</td>
    </tr>
    <tr>
      <th>Andrew Johnson</th>
      <td>0.095733</td>
      <td>0.250310</td>
      <td>1.000000</td>
      <td>0.214493</td>
      <td>0.146720</td>
      <td>0.181105</td>
      <td>0.457404</td>
      <td>0.148732</td>
      <td>0.145326</td>
      <td>0.154563</td>
      <td>...</td>
      <td>0.264893</td>
      <td>0.156056</td>
      <td>0.165122</td>
      <td>0.231702</td>
      <td>0.160823</td>
      <td>0.151998</td>
      <td>0.225098</td>
      <td>0.281469</td>
      <td>0.148176</td>
      <td>0.121830</td>
    </tr>
    <tr>
      <th>Bedford Forrest</th>
      <td>0.101333</td>
      <td>0.292756</td>
      <td>0.214493</td>
      <td>1.000000</td>
      <td>0.090199</td>
      <td>0.155993</td>
      <td>0.385671</td>
      <td>0.065007</td>
      <td>0.129868</td>
      <td>0.130259</td>
      <td>...</td>
      <td>0.139353</td>
      <td>0.124977</td>
      <td>0.114385</td>
      <td>0.159466</td>
      <td>0.129266</td>
      <td>0.115831</td>
      <td>0.132275</td>
      <td>0.108845</td>
      <td>0.108885</td>
      <td>0.098397</td>
    </tr>
    <tr>
      <th>Lucretia Mott</th>
      <td>0.091677</td>
      <td>0.084679</td>
      <td>0.146720</td>
      <td>0.090199</td>
      <td>1.000000</td>
      <td>0.161976</td>
      <td>0.129401</td>
      <td>0.100451</td>
      <td>0.130107</td>
      <td>0.205686</td>
      <td>...</td>
      <td>0.155760</td>
      <td>0.075647</td>
      <td>0.151797</td>
      <td>0.133952</td>
      <td>0.139900</td>
      <td>0.063735</td>
      <td>0.148130</td>
      <td>0.118926</td>
      <td>0.100889</td>
      <td>0.118720</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 379 columns</p>
</div></div></div>
</div>
<p>Let’s look at some examples. Below, we define a function to query for a
document’s <strong>nearest neighbors</strong>. That is, we look for which documents are
closest to that document in the vector space.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">k_nearest_neighbors</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">similarities</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Find the k-nearest neighbors for a query.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    query : str</span>
<span class="sd">        Index name for the query vector</span>
<span class="sd">    similarities : pd.DataFrame</span>
<span class="sd">        Cosine similarities to query</span>
<span class="sd">    k : int</span>
<span class="sd">        Number of neighbors to return</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    output : list[tuple]</span>
<span class="sd">        Neighbors and their scores</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">similarities</span><span class="p">[</span><span class="n">query</span><span class="p">]</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">neighbors</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">neighbors</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">[(</span><span class="n">neighbor</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span> <span class="k">for</span> <span class="n">neighbor</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">scores</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;Miles Davis&quot;</span><span class="p">,</span> <span class="s2">&quot;Eleanor Roosevelt&quot;</span><span class="p">,</span> <span class="s2">&quot;Willa Cather&quot;</span><span class="p">):</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">k_nearest_neighbors</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">doc_sim</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Name&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Name               Score
---------------  -------
Miles Davis        1
Sammy Davis Jr     0.458
Thelonious Monk    0.383
Dizzy Gillespie    0.382
Coleman Hawkins    0.344

Name                      Score
----------------------  -------
Eleanor Roosevelt         1
FDR                       0.62
Jacqueline Kennedy        0.434
Elizabeth Cady Stanton    0.427
Anne O Hare McCormick     0.423

Name                 Score
-----------------  -------
Willa Cather         1
Edith Wharton        0.437
Truman Capote        0.428
Marjorie Rawlings    0.409
Scott Fitzgerald     0.396
</pre></div>
</div>
</div>
</div>
<p>Does this conform to what we see in the scatter plot of documents?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">names</span> <span class="o">=</span> <span class="n">doc_sim</span><span class="p">[</span><span class="s2">&quot;Miles Davis&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">index</span>
<span class="n">highlight</span> <span class="o">=</span> <span class="p">[</span><span class="n">dtm</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">]</span>
<span class="n">scatter_2d</span><span class="p">(</span><span class="n">dtm</span><span class="p">,</span> <span class="n">highlight</span> <span class="o">=</span> <span class="n">highlight</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a03e75ae874260f55b25dff2bba3818d5d848f907760442d9f10cea19a553bb7.png" src="../_images/a03e75ae874260f55b25dff2bba3818d5d848f907760442d9f10cea19a553bb7.png" />
</div>
</div>
</section>
<section id="token-similarity">
<h3><span class="section-number">6.3.2. </span>Token similarity<a class="headerlink" href="#token-similarity" title="Link to this heading">#</a></h3>
<p>All of the above applies to tokens as well. Transpose the DTM and you can
derive cosine similarity scores between tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">token_sim</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">dtm</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">token_sim</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">token_sim</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">dtm</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">dtm</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">token_sim</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>aachen</th>
      <th>aahs</th>
      <th>aane</th>
      <th>aarau</th>
      <th>aaron</th>
      <th>aau</th>
      <th>aaugh</th>
      <th>aayega</th>
      <th>aba</th>
      <th>ababa</th>
      <th>...</th>
      <th>zrathustra</th>
      <th>zuber</th>
      <th>zuker</th>
      <th>zukor</th>
      <th>zukors</th>
      <th>zula</th>
      <th>zululand</th>
      <th>zurich</th>
      <th>zvai</th>
      <th>zwilich</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>gregory</th>
      <td>0.0</td>
      <td>0.534522</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.077152</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>check</th>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.032898</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.080582</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.113961</td>
      <td>0.041787</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>row</th>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.138675</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.320256</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>astronomique</th>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>gheradi</th>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31165 columns</p>
</div></div></div>
</div>
<p>Similar token listings:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;music&quot;</span><span class="p">,</span> <span class="s2">&quot;politics&quot;</span><span class="p">,</span> <span class="s2">&quot;country&quot;</span><span class="p">,</span> <span class="s2">&quot;royal&quot;</span><span class="p">):</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">k_nearest_neighbors</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">token_sim</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Token&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token        Score
---------  -------
music        1
musical      0.738
piano        0.721
orchestra    0.701
musician     0.672

Token         Score
----------  -------
politics      1
democrats     0.693
democratic    0.669
political     0.652
campaign      0.647

Token      Score
-------  -------
country    1
time       0.773
people     0.77
great      0.757
end        0.757

Token      Score
-------  -------
royal      1
prince     0.862
osborne    0.853
coburg     0.85
saxe       0.85
</pre></div>
</div>
</div>
</div>
<p>We can also project these vectors into a two-dimensional space. The code below
extracts the top-10 nearest tokens for a query token, samples the cosine
similarities, adds the two together into a single DataFrame, and plots them.
Note that we turn off normalization; cosine similarity is already normed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the subset, then sample</span>
<span class="n">subset</span> <span class="o">=</span> <span class="n">token_sim</span><span class="p">[</span><span class="s2">&quot;royal&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">index</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">token_sim</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="o">~</span><span class="n">sample</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">subset</span><span class="p">)]</span>

<span class="c1"># Make the DataFrame for plotting</span>
<span class="n">to_plot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">token_sim</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">],</span> <span class="n">sample</span><span class="p">])</span>
<span class="n">highlight</span> <span class="o">=</span> <span class="p">[</span><span class="n">to_plot</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">subset</span><span class="p">]</span>

<span class="c1"># Plot</span>
<span class="n">scatter_2d</span><span class="p">(</span><span class="n">to_plot</span><span class="p">,</span> <span class="n">norm</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">highlight</span> <span class="o">=</span> <span class="n">highlight</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1e3e8c459af4a94645a404d8938581a419276f02f1d298f1e53326bff85865d0.png" src="../_images/1e3e8c459af4a94645a404d8938581a419276f02f1d298f1e53326bff85865d0.png" />
</div>
</div>
<p>Technically, one could call these vectors definitions of each token. But they
aren’t very good definitions—for a number of reasons, the first of which is
that the only data we have to create these vectors comes from a small corpus of
obituaries. We’d need a much larger corpus to generalize our token
representations to a point where these vectors might reflect semantics as we
understand it.</p>
</section>
</section>
<section id="word-embeddings">
<h2><span class="section-number">6.4. </span>Word Embeddings<a class="headerlink" href="#word-embeddings" title="Link to this heading">#</a></h2>
<p>Whence word embeddings. These vectors are the result of single-layer neural
network training on millions, or even billions, of tokens. Networks will differ
slightly in their architectures, but all are trained on co-occurrence
tabulations, that is, the number of times a token appears alongside a set of
tokens. The result: every token in the training data is <strong>embedded</strong> in a
continuous vector space of many dimensions.</p>
<p>The two most common word embedding methods are <strong>Word2Vec</strong> and <strong>GloVe</strong>. The
former uses a smaller <strong>window size</strong> of co-occurrences for a token, while the
latter uses global co-occurrence values. Take a look at Jay Alammar’s
<a class="reference external" href="https://jalammar.github.io/illustrated-word2vec/">Illustrated Word2Vec</a> for a detailed walk-through of how these
models are trained.</p>
<p>Below, we will use a 200-dimension version of <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe</a>, which continues
to be made available by the Stanford NLP group. It has been trained on a 2014
dump of Wikipedia and <a class="reference external" href="https://catalog.ldc.upenn.edu/LDC2011T07">Gigaword 5</a>. We define a custom class,
<code class="docutils literal notranslate"><span class="pre">WordEmbeddings</span></code>, to work with these vectors. Why do this? Word embeddings of
this kind have fallen out of favor by researchers and developers, and the
libraries that used to support embeddings have gone into maintenance-only mode.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">WordEmbeddings</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A minimal wrapper for Word2Vec-style embeddings.</span>
<span class="sd">    </span>
<span class="sd">    Loosely based on the Gensim wrapper: https://radimrehurek.com/gensim.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the embeddings.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str or Path</span>
<span class="sd">            Path to the embeddings parquet file</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the data and store some metadata about the vocabulary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>

        <span class="c1"># Fit a nearest neighbors graph using cosine similarity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neighbors</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span>
            <span class="n">metric</span> <span class="o">=</span> <span class="s2">&quot;cosine&quot;</span><span class="p">,</span> <span class="n">n_neighbors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neighbors</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Define a length for the embeddings.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Index the embeddings to retrieve a vector.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        key : str</span>
<span class="sd">            Index key</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        vector : np.ndarray</span>
<span class="sd">            The vector</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> not in vocabulary.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Find the similarity between two tokens.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        source : str</span>
<span class="sd">            Source token</span>
<span class="sd">        target : str</span>
<span class="sd">            Target token</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : float</span>
<span class="sd">            Similarity score</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Query the neighbors graph to find the distances between it and all</span>
        <span class="c1"># other vectors</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">source</span><span class="p">]</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neighbors</span><span class="o">.</span><span class="n">kneighbors_graph</span><span class="p">([</span><span class="n">query</span><span class="p">],</span> <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;distance&quot;</span><span class="p">)</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">distances</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Shift the distances to similarities</span>
        <span class="n">similarities</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">distances</span>

        <span class="c1"># Return the target token&#39;s similarity using by indexing the</span>
        <span class="c1"># similarities</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">similarities</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">most_similar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Find the k-most similar tokens to a query vector.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        query : str or np.ndarray</span>
<span class="sd">            A query string or vector</span>
<span class="sd">        k : int</span>
<span class="sd">            Number of neighbors to return</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        output : list[tuple]</span>
<span class="sd">            Nearest tokens and their similarity scores</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If passed a string, get the vector</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">query</span><span class="p">]</span>

        <span class="c1"># Query the nearest neighbor graph. This returns the index positions</span>
        <span class="c1"># for the top-k most similar tokens and the distance values for each of</span>
        <span class="c1"># those tokens </span>
        <span class="n">distances</span><span class="p">,</span> <span class="n">knn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neighbors</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">([</span><span class="n">query</span><span class="p">],</span> <span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>

        <span class="c1"># Convert distances to similarities and squeeze out the extra</span>
        <span class="c1"># dimension. Then retrieve tokens (and squeeze out that dimension, too)</span>
        <span class="n">similarities</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">distances</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">knn</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span><span class="o">.</span><span class="n">index</span>

        <span class="c1"># Pair the tokens with the similarities</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">[(</span><span class="n">tok</span><span class="p">,</span> <span class="n">sim</span><span class="p">)</span> <span class="k">for</span> <span class="n">tok</span><span class="p">,</span> <span class="n">sim</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">similarities</span><span class="p">)]</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">analogize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">this</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">to_that</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">as_this</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform an analogy and return the result&#39;s k-most similar neighbors.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        this : str</span>
<span class="sd">            Analogy&#39;s source</span>
<span class="sd">        to_that : str</span>
<span class="sd">            Source&#39;s complement</span>
<span class="sd">        as_this : str</span>
<span class="sd">            Analogy&#39;s target</span>
<span class="sd">        k : int</span>
<span class="sd">            Number of neighbors to return</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        output : list[tuple]</span>
<span class="sd">            Nearest tokens and their similarity scores</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get the vectors for input terms</span>
        <span class="n">this</span><span class="p">,</span> <span class="n">to_that</span><span class="p">,</span> <span class="n">as_this</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">this</span><span class="p">],</span> <span class="bp">self</span><span class="p">[</span><span class="n">to_that</span><span class="p">],</span> <span class="bp">self</span><span class="p">[</span><span class="n">as_this</span><span class="p">]</span>

        <span class="c1"># Subtract the analogy&#39;s source from the analogy&#39;s target to capture</span>
        <span class="c1"># the relationship between the two (that is, their difference). Then,</span>
        <span class="c1"># apply this relationship to the source&#39;s complement via addition</span>
        <span class="n">is_to_what</span> <span class="o">=</span> <span class="p">(</span><span class="n">as_this</span> <span class="o">-</span> <span class="n">this</span><span class="p">)</span> <span class="o">+</span> <span class="n">to_that</span>

        <span class="c1"># Get the most similar tokens to this new vector</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">is_to_what</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With the wrapper defined, we load the embeddings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">glove</span> <span class="o">=</span> <span class="n">WordEmbeddings</span><span class="p">(</span><span class="s2">&quot;data/models/glove.6B.200d.parquet&quot;</span><span class="p">)</span>
<span class="n">n_vocab</span><span class="p">,</span> <span class="n">n_dim</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embeddings size and shape: </span><span class="si">{</span><span class="n">n_vocab</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">n_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Embeddings size and shape: 400,000, 200
</pre></div>
</div>
</div>
</div>
<p>And here’s an example embedding:</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">glove</span><span class="p">[</span><span class="s2">&quot;book&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-2.0744e-01,  4.1585e-01,  8.1915e-02,  5.8763e-02,  2.2087e-01,
        9.7051e-02, -4.2692e-01, -5.0030e-02,  3.1870e-01, -3.0463e-01,
        2.4791e-01,  5.6221e-01, -1.1726e-01,  3.1970e-01,  5.2722e-01,
        2.6074e-02, -3.6409e-01,  4.1153e-01, -7.6841e-01,  1.0559e-01,
        7.7134e-01,  2.3773e+00,  2.4155e-01, -1.1650e-01, -4.8823e-02,
        2.4152e-01, -2.0366e-01, -6.7341e-03, -7.4826e-02, -1.3317e-01,
       -4.3025e-01,  7.2237e-01,  6.4573e-01, -1.7853e-01,  3.4218e-01,
        2.0579e-01, -2.6898e-01, -4.5916e-01,  6.5838e-01, -7.6909e-01,
       -1.3438e-02,  2.2939e-01, -5.8398e-01,  3.0186e-01,  6.7211e-05,
        8.9954e-02,  1.1644e+00,  2.0050e-01,  8.2978e-02,  4.4839e-01,
       -3.2783e-01,  2.0404e-02,  5.9874e-01, -8.3142e-02,  1.8843e-01,
        1.2994e-01, -4.6333e-01,  4.8369e-01,  5.1809e-01,  3.5972e-02,
       -3.3864e-01,  5.1722e-01, -7.9211e-01, -4.8147e-01, -5.4961e-01,
       -3.3807e-01,  3.8443e-01,  3.7494e-01, -9.4575e-02,  4.6171e-01,
        1.4233e-02, -1.7931e-01,  1.7033e-01,  1.0492e-01,  3.2059e-01,
        1.8625e-01, -2.6142e-01, -2.5889e-01, -6.4226e-01, -1.4243e-01,
       -6.9478e-02,  3.6479e-01, -4.6202e-01,  3.9706e-01,  6.0458e-02,
       -5.4798e-02, -2.4068e-01, -1.5600e-01,  4.5201e-01, -6.4922e-01,
        1.4555e-01, -1.9309e-02,  2.3783e-01, -2.2749e-02,  1.9767e-01,
       -1.4548e-02,  1.4615e-01, -3.3319e-01, -5.0904e-01,  1.2364e-01,
       -2.4694e-01,  1.1290e-01,  7.1005e-01,  4.1276e-01,  5.3504e-02,
       -2.6021e-01, -1.2569e-01,  6.5647e-01, -4.7484e-01,  5.7874e-01,
        3.9066e-01, -3.3889e-01, -3.5945e-01, -2.6475e-01, -3.3406e-01,
       -1.9232e-02, -4.2046e-01,  6.1108e-01, -7.2916e-01, -2.7222e-01,
        1.2946e-02,  5.2870e-01, -4.1545e-01, -8.4026e-01, -8.8573e-02,
        6.2217e-02,  5.9892e-01,  2.3597e-01,  2.6234e-01, -6.3584e-01,
       -2.8773e-01,  2.9159e-02, -2.5226e-01,  4.2811e-02,  2.6993e-01,
        1.2566e-01, -1.2526e-01, -1.8764e-01, -3.6771e-01,  1.6834e-01,
        1.6387e-01, -9.1672e-02,  9.8863e-02,  2.2640e-01,  7.9722e-01,
       -1.2103e-01,  3.6880e-01, -9.5826e-01, -2.8817e-01,  1.4173e-01,
        5.4916e-01,  3.0036e-01, -6.5198e-01, -4.8353e-01,  5.2975e-01,
        3.4400e-01, -2.5168e-01, -2.5082e-01,  4.1813e-01, -4.3527e-02,
        3.8635e-01,  3.1218e-02,  2.6251e-01,  2.4787e-01,  7.2785e-01,
       -8.0236e-01, -6.1127e-01, -7.6405e-01, -3.0026e-01,  2.4254e-01,
       -2.3853e-01,  2.0024e-01,  5.7175e-01,  1.8369e-01, -1.5088e-01,
        6.9597e-01, -7.7065e-01,  4.4209e-01,  2.5416e-01,  2.3234e-01,
        1.0590e+00,  1.7118e-01, -2.9142e-01,  1.1264e-02, -9.5082e-01,
       -1.1275e-01, -4.1507e-01, -1.6546e-01,  1.7465e-01,  6.6374e-02,
       -4.1581e-01, -5.0738e-02,  4.2403e-01, -3.6238e-01,  6.1508e-01,
       -2.6538e-01, -4.6146e-01,  2.9840e-01,  5.1868e-01, -1.9248e-01])
</pre></div>
</div>
</div>
</div>
<section id="token-similarity-redux">
<h3><span class="section-number">6.4.1. </span>Token similarity (redux)<a class="headerlink" href="#token-similarity-redux" title="Link to this heading">#</a></h3>
<p>With the embeddings loaded, we query the same set of tokens from above to find
their nearest neighbors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;music&quot;</span><span class="p">,</span> <span class="s2">&quot;politics&quot;</span><span class="p">,</span> <span class="s2">&quot;country&quot;</span><span class="p">,</span> <span class="s2">&quot;royal&quot;</span><span class="p">):</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Token&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token         Score
---------  --------
music      1
musical    0.733881
songs      0.725357
pop        0.690601
musicians  0.687654
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token           Score
-----------  --------
politics     1
political    0.767441
politicians  0.623498
religion     0.604117
liberal      0.59728
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token         Score
---------  --------
country    1
nation     0.81192
countries  0.692018
has        0.682141
now        0.66038
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token        Score
--------  --------
royal     1
queen     0.607112
imperial  0.606977
british   0.573261
palace    0.562962
</pre></div>
</div>
</div>
</div>
<p>Some listings, like the one for “music,” are fairly comparable, but listings
for “country” and “royal” are more general.</p>
<p>To get the least similar token, query for all tokens and then select the last
element. You may think this would be a token’s antonym.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">glove</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;good&quot;</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">glove</span><span class="p">))[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;cw96&#39;, -0.6553233434020003)
</pre></div>
</div>
</div>
</div>
<p>Here, semantics and vector spaces diverge. The vector for the above token is in
the opposite direction of the one for “good,” but that doesn’t line up with
semantic opposition. In fact, the token we’d likely expect, “evil,” is
relatively similar to “good.”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">glove</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;good&quot;</span><span class="p">,</span> <span class="s2">&quot;evil&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.33780358010030886
</pre></div>
</div>
</div>
</div>
<p>Why is this? Since word embeddings are trained on co-occurrence data, tokens
that appear in similar contexts will be more similar in a mathematical sense.
We often speak of “good” and “evil” in interchangeable ways.</p>
<p>It’s important to keep this in mind for considerations of bias. Because
embeddings reflect the interchangeability of tokens, they can reinforce
negative, even harmful patterns in their training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;doctor&quot;</span><span class="p">,</span> <span class="s2">&quot;nurse&quot;</span><span class="p">):</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Token&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token         Score
---------  --------
doctor     1
physician  0.736021
doctors    0.672406
surgeon    0.655147
dr.        0.652498
nurse      0.651449
medical    0.648189
hospital   0.63638
patient    0.619159
dentist    0.584747
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token           Score
-----------  --------
nurse        1
nurses       0.714051
doctor       0.651449
nursing      0.626938
midwife      0.614592
anesthetist  0.610603
physician    0.610359
hospital     0.609222
mother       0.586503
therapist    0.580488
</pre></div>
</div>
</div>
</div>
</section>
<section id="concept-modeling">
<h3><span class="section-number">6.4.2. </span>Concept modeling<a class="headerlink" href="#concept-modeling" title="Link to this heading">#</a></h3>
<p>Recall the vector operations we discussed above. We can perform these
operations on word embeddings (since embeddings are vectors), and doing so
provides a way to explore semantic space. One way to think about modeling
concepts, for example, is by adding two vectors together. We’d expect the
nearest neighbors of the resultant vector to reflect the concept we’ve tried to
build.</p>
<p>The dictionary below has keys for concepts. Its tuples are the two vectors
we’ll add together to create that concept.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">concepts</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;beach&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;sand&quot;</span><span class="p">,</span> <span class="s2">&quot;ocean&quot;</span><span class="p">),</span>
    <span class="s2">&quot;hotel&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;vacation&quot;</span><span class="p">,</span> <span class="s2">&quot;room&quot;</span><span class="p">),</span>
    <span class="s2">&quot;airplane&quot;</span><span class="p">:</span> <span class="p">(</span><span class="s2">&quot;air&quot;</span><span class="p">,</span> <span class="s2">&quot;car&quot;</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s iterate through the concepts.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">concept</span> <span class="ow">in</span> <span class="n">concepts</span><span class="p">:</span>
    <span class="c1"># Build the concept by adding the two component vectors</span>
    <span class="n">A</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">concepts</span><span class="p">[</span><span class="n">concept</span><span class="p">]</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">glove</span><span class="p">[</span><span class="n">A</span><span class="p">]</span> <span class="o">+</span> <span class="n">glove</span><span class="p">[</span><span class="n">B</span><span class="p">]</span>

    <span class="c1"># Query the embeddings</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">vector</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Token&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The expected concepts aren’t the top-most tokens for each result, but they’re
in the top 10.</p>
</section>
<section id="analogies">
<h3><span class="section-number">6.4.3. </span>Analogies<a class="headerlink" href="#analogies" title="Link to this heading">#</a></h3>
<p>Most famously, word embeddings enable quasi-logical reasoning. Whereas
synonym/antonym pairs do not map to vector operations, certain analogies
do—or, sometimes they do. To construct these analogies, we define a
relationship between a source and a target word by subtracting the vector for
the former from the latter. Then, we add the source’s complement to the result.
As above, the nearest neighbors for this new vector should reflect analogical
reasoning.</p>
<p>Below: “Strong is to stronger what clear is to X?”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">analogize</span><span class="p">(</span>
    <span class="n">this</span> <span class="o">=</span> <span class="s2">&quot;strong&quot;</span><span class="p">,</span> <span class="n">to_that</span> <span class="o">=</span> <span class="s2">&quot;stronger&quot;</span><span class="p">,</span> <span class="n">as_this</span> <span class="o">=</span> <span class="s2">&quot;clear&quot;</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
<span class="p">)</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Token&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token        Score
--------  --------
clear     0.707581
easier    0.635169
clearer   0.62249
stronger  0.609107
should    0.601482
harder    0.59936
better    0.59212
sooner    0.580909
must      0.57448
need      0.573058
</pre></div>
</div>
</div>
</div>
<p>“Paris is to France what Berlin is to X?”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">analogize</span><span class="p">(</span>
    <span class="n">this</span> <span class="o">=</span> <span class="s2">&quot;paris&quot;</span><span class="p">,</span> <span class="n">to_that</span> <span class="o">=</span> <span class="s2">&quot;france&quot;</span><span class="p">,</span> <span class="n">as_this</span> <span class="o">=</span> <span class="s2">&quot;berlin&quot;</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
<span class="p">)</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Token&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token       Score
-------  --------
germany  0.832399
berlin   0.678563
german   0.672207
france   0.62612
austria  0.616403
poland   0.59143
germans  0.575792
belgium  0.557132
britain  0.553339
spain    0.538451
</pre></div>
</div>
</div>
</div>
<p>Let’s plot the above example in two dimensions to demonstrate this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the embeddings</span>
<span class="n">paris</span><span class="p">,</span> <span class="n">france</span> <span class="o">=</span> <span class="n">glove</span><span class="p">[</span><span class="s2">&quot;paris&quot;</span><span class="p">],</span> <span class="n">glove</span><span class="p">[</span><span class="s2">&quot;france&quot;</span><span class="p">]</span>
<span class="n">berlin</span><span class="p">,</span> <span class="n">germany</span> <span class="o">=</span> <span class="n">glove</span><span class="p">[</span><span class="s2">&quot;berlin&quot;</span><span class="p">],</span> <span class="n">glove</span><span class="p">[</span><span class="s2">&quot;germany&quot;</span><span class="p">]</span>

<span class="c1"># Create a target and stack the five vectors</span>
<span class="n">target</span> <span class="o">=</span> <span class="p">(</span><span class="n">berlin</span> <span class="o">-</span> <span class="n">paris</span><span class="p">)</span> <span class="o">+</span> <span class="n">france</span>
<span class="n">stacked</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">paris</span><span class="p">,</span> <span class="n">france</span><span class="p">,</span> <span class="n">berlin</span><span class="p">,</span> <span class="n">germany</span><span class="p">,</span> <span class="n">target</span><span class="p">])</span>

<span class="c1"># Plot</span>
<span class="n">scatter_2d</span><span class="p">(</span><span class="n">stacked</span><span class="p">,</span> <span class="n">norm</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">highlight</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2baefc1e9f55a096ffe2c44f675aa5e10b39dea01dcc0626984a4b0d2c2b7412.png" src="../_images/2baefc1e9f55a096ffe2c44f675aa5e10b39dea01dcc0626984a4b0d2c2b7412.png" />
</div>
</div>
<p>See how the target (in red) forms a perfect square with the other terms of the
analogy? And its nearest neighbor is the vector we’d expect to find. That said:
note that the latter does <em>not</em> form a perfect square. Analogizing in this
manner fudges things.</p>
<p>Consider the following: “Arm is to hand what leg is to X?”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">analogize</span><span class="p">(</span>
    <span class="n">this</span> <span class="o">=</span> <span class="s2">&quot;arm&quot;</span><span class="p">,</span> <span class="n">to_that</span> <span class="o">=</span> <span class="s2">&quot;hand&quot;</span><span class="p">,</span> <span class="n">as_this</span> <span class="o">=</span> <span class="s2">&quot;leg&quot;</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
<span class="p">)</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Token&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token        Score
--------  --------
leg       0.763518
hand      0.606446
final     0.538706
legs      0.533707
table     0.515848
saturday  0.51571
round     0.512425
match     0.509738
draw      0.507376
second    0.501668
</pre></div>
</div>
</div>
</div>
<p>We’d expect “foot” but no such luck.</p>
</section>
</section>
<section id="part-of-speech-disambiguation">
<h2><span class="section-number">6.5. </span>Part-of-Speech Disambiguation<a class="headerlink" href="#part-of-speech-disambiguation" title="Link to this heading">#</a></h2>
<p>Clearly these embeddings pick up at least some information about semantics. But
it’s challenging to identify where this information lies in the embeddings:
sequences of positive and negative numbers don’t tell us much on their own.
Let’s see if we can design an experiment to make these numbers more
interpretable.</p>
<p>The following constructs a method for determining the part-of-speech (POS) of a
vector. Using embeddings for POS-tagged WordNet lemmas, we train a classifier
to make this determination.</p>
<section id="data-preparation">
<h3><span class="section-number">6.5.1. </span>Data preparation<a class="headerlink" href="#data-preparation" title="Link to this heading">#</a></h3>
<p>First, we load the embeddings for our WordNet tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wordnet</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span>
    <span class="s2">&quot;data/datasets/wordnet_embeddings_glove.6B.200d.parquet&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Words can have more than one POS tag.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wordnet</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="s2">&quot;good&quot;</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">))]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>190</th>
      <th>191</th>
      <th>192</th>
      <th>193</th>
      <th>194</th>
      <th>195</th>
      <th>196</th>
      <th>197</th>
      <th>198</th>
      <th>199</th>
    </tr>
    <tr>
      <th>label</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>N</th>
      <td>0.51507</td>
      <td>0.35596</td>
      <td>0.1571</td>
      <td>-0.074075</td>
      <td>-0.25446</td>
      <td>-0.11357</td>
      <td>-0.49943</td>
      <td>-0.12626</td>
      <td>0.38851</td>
      <td>0.54204</td>
      <td>...</td>
      <td>-0.048109</td>
      <td>-0.38057</td>
      <td>-0.35258</td>
      <td>-0.006266</td>
      <td>0.27227</td>
      <td>-0.16222</td>
      <td>-0.31979</td>
      <td>0.14338</td>
      <td>-0.072859</td>
      <td>0.17815</td>
    </tr>
    <tr>
      <th>A</th>
      <td>0.51507</td>
      <td>0.35596</td>
      <td>0.1571</td>
      <td>-0.074075</td>
      <td>-0.25446</td>
      <td>-0.11357</td>
      <td>-0.49943</td>
      <td>-0.12626</td>
      <td>0.38851</td>
      <td>0.54204</td>
      <td>...</td>
      <td>-0.048109</td>
      <td>-0.38057</td>
      <td>-0.35258</td>
      <td>-0.006266</td>
      <td>0.27227</td>
      <td>-0.16222</td>
      <td>-0.31979</td>
      <td>0.14338</td>
      <td>-0.072859</td>
      <td>0.17815</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 200 columns</p>
</div></div></div>
</div>
<p>But the vectors are the same because static embeddings make no distinction in
context-dependent determinations. We therefore drop duplicate vectors from our
data. First though: we shuffle the rows. That will randomly distribute the POS
tag distribution across all duplicated vectors (in other words, no one POS tag
will unfairly benefit from dropping duplicates).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wordnet</span> <span class="o">=</span> <span class="n">wordnet</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">wordnet</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now split the data into train/test sets. Our labels are the POS tags.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">wordnet</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">wordnet</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">get_level_values</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">357</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Regardless of duplication, the tags themselves are fairly imbalanced.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tag_counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">tag_counts</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>label
N    27184
A    12515
V     4477
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>We derive a weighting to tell the model how often it should expect to see a
tag.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">tag_counts</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">tag_counts</span><span class="p">)</span>
<span class="n">class_weight</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">idx</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">class_weight</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">class_weight</span><span class="o">.</span><span class="n">index</span>
<span class="p">}</span>
<span class="n">class_weight</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;N&#39;: 0.62, &#39;A&#39;: 0.28, &#39;V&#39;: 0.1}
</pre></div>
</div>
</div>
</div>
</section>
<section id="fitting-a-logistic-regression-model">
<h3><span class="section-number">6.5.2. </span>Fitting a logistic regression model<a class="headerlink" href="#fitting-a-logistic-regression-model" title="Link to this heading">#</a></h3>
<p>We use a <strong>logistic regression</strong> model for this experiment. It models the
probability that input data belongs to a specific class. In our case, input and
classes are vectors and POS tags, respectively. For every dimension, or
<strong>feature</strong>, in these vectors the model has a corresponding weight, or
<strong>coefficient</strong>. Coefficients represent the change in the <strong>log-odds</strong> of the
outcome for a one-unit change in the corresponding feature. Log-odds, or
logits, are the logarithm of the odds of an event occurring (which is in turn
the ratio of the probability of an event occurring to the probability of that
event not occurring).</p>
<p>When training on data, the model multiplies features by their coefficients and
sums them up to compute the log-odds of the outcome. This sum is used to obtain
a probability for each class. The model adjusts its coefficients to increase
the likelihood that the vector of features will be categorized with the correct
label. At each step in the training process, a <strong>loss function</strong> (typically log
loss) measures the difference between the predicted probabilities and the
actual labels. The model uses this loss to update its coefficients in a way
that reduces error.</p>
<p>Coefficients are represented as:</p>
<div class="math notranslate nohighlight">
\[
z = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(z\)</span> is a vector of coefficients corresponding to each feature in the input <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept, or bias term</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_nx_n\)</span> is the product of the coefficient <span class="math notranslate nohighlight">\(\beta_n\)</span> for the <span class="math notranslate nohighlight">\(n\)</span>-th
feature in the input <span class="math notranslate nohighlight">\(x\)</span></p></li>
</ul>
<p>Transforming <span class="math notranslate nohighlight">\(z\)</span> into a probability for each class uses the logistic, or
<strong>sigmoid</strong>, function:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y}\)</span> is the predicted probability</p></li>
<li><p>Which is the result of the sigmoid function <span class="math notranslate nohighlight">\(\sigma(z)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{1}{1 + e^{-z}}\)</span> is sigmoid function formula</p></li>
</ul>
<p>As with Naive Bayes, there’s no need to implement this ourselves when
<code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> can do it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span>
    <span class="n">solver</span> <span class="o">=</span> <span class="s2">&quot;liblinear&quot;</span><span class="p">,</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
    <span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span><span class="p">,</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">5_000</span><span class="p">,</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="mi">357</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LogisticRegression(C=2.0, class_weight={&#x27;A&#x27;: 0.28, &#x27;N&#x27;: 0.62, &#x27;V&#x27;: 0.1},
                   max_iter=5000, random_state=357, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;LogisticRegression<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html">?<span>Documentation for LogisticRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>LogisticRegression(C=2.0, class_weight={&#x27;A&#x27;: 0.28, &#x27;N&#x27;: 0.62, &#x27;V&#x27;: 0.1},
                   max_iter=5000, random_state=357, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div></div>
</div>
<p>Let’s look at a classification report.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">report</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

           A       0.83      0.54      0.65      1258
           N       0.76      0.95      0.85      2696
           V       0.92      0.48      0.63       464

    accuracy                           0.78      4418
   macro avg       0.84      0.66      0.71      4418
weighted avg       0.80      0.78      0.77      4418
</pre></div>
</div>
</div>
</div>
<p>Not bad. For a such a simple model, it achieves decent accuracy, and the
precision is relatively high. Recall suffers, however. This is likely due to
the class imbalance.</p>
</section>
<section id="examining-model-coefficients">
<h3><span class="section-number">6.5.3. </span>Examining model coefficients<a class="headerlink" href="#examining-model-coefficients" title="Link to this heading">#</a></h3>
<p>Let’s extract the coefficients from the model. These represent the weightings
on each dimension of the word vectors that adjust the associated values to be
what the model expects for a certain class.</p>
<ul class="simple">
<li><p><strong>Positive coefficient:</strong> An increase in the feature value increases the
log-odds of the classification outcome; the probability of a token belonging
to the class increases</p></li>
<li><p><strong>Negative coefficient:</strong> An increase in the feature value decreases the
log-odds of the classification outcome; the probability of a token belonging
to the class decreases</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">coef</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>N</th>
      <th>V</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.176840</td>
      <td>-0.003730</td>
      <td>0.149001</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.856859</td>
      <td>0.455467</td>
      <td>0.756065</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.465970</td>
      <td>-0.202974</td>
      <td>-0.490308</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.027669</td>
      <td>-0.019530</td>
      <td>0.006378</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.470529</td>
      <td>0.379956</td>
      <td>0.311610</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We’ll use this DataFrame in a moment, but for now let’s reformat to create a
series of bar plots showing how each dimension of the embeddings interacts with
the model’s coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coef_plot</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;index&quot;</span><span class="p">:</span> <span class="s2">&quot;dimension&quot;</span><span class="p">})</span>
<span class="n">coef_plot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span>
    <span class="n">coef_plot</span><span class="p">,</span>
    <span class="n">id_vars</span> <span class="o">=</span> <span class="s2">&quot;dimension&quot;</span><span class="p">,</span>
    <span class="n">var_name</span> <span class="o">=</span> <span class="s2">&quot;POS&quot;</span><span class="p">,</span>
    <span class="n">value_name</span> <span class="o">=</span> <span class="s2">&quot;coefficient&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Time to plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharex</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">classes_</span><span class="p">):</span>
    <span class="n">subplot_data</span> <span class="o">=</span> <span class="n">coef_plot</span><span class="p">[</span><span class="n">coef_plot</span><span class="p">[</span><span class="s2">&quot;POS&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">pos</span><span class="p">]</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;dimension&quot;</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;coefficient&quot;</span><span class="p">,</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">coef_plot</span><span class="p">[</span><span class="n">coef_plot</span><span class="p">[</span><span class="s2">&quot;POS&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">pos</span><span class="p">],</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">g</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pos</span><span class="si">}</span><span class="s2"> coefficient&quot;</span><span class="p">,</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="p">[])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;POS coefficients across word embedding dimensions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/6ae40fba211103c0ab7819d7ad1d21deac3517d25f98c67c220bda7fb4456f68.png" src="../_images/6ae40fba211103c0ab7819d7ad1d21deac3517d25f98c67c220bda7fb4456f68.png" />
</div>
</div>
<p>No one single dimension stands out as the indicator for a POS tag; rather,
groups of dimensions determine POS tags. This is in part what we mean by a
distributed representation. We can use <strong>recursive feature elimination</strong> to
extract dimensions that are particularly important. This process fits our model
on the embedding features and then evaluates the importance of each feature.
Important features are those with high coefficients (absolute values). It then
prunes out the least important features, refits the model, re-evaluates feature
importance, and so on.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">n_features_to_select</span></code> to set the number of features you want. Supplying an
argument to <code class="docutils literal notranslate"><span class="pre">step</span></code> will determine how many features are pruned out at every
step.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selector</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n_features_to_select</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-2 {
  color: var(--sklearn-color-text);
}

#sk-container-id-2 pre {
  padding: 0;
}

#sk-container-id-2 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-2 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-2 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-2 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-2 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-2 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-2 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-2 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-2 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-2 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-2 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-2 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-2 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-2 div.sk-label label.sk-toggleable__label,
#sk-container-id-2 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-2 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-2 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-2 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-2 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-2 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-2 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-2 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-2 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RFE(estimator=LogisticRegression(C=2.0,
                                 class_weight={&#x27;A&#x27;: 0.28, &#x27;N&#x27;: 0.62, &#x27;V&#x27;: 0.1},
                                 max_iter=5000, random_state=357,
                                 solver=&#x27;liblinear&#x27;),
    n_features_to_select=10, step=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" ><label for="sk-estimator-id-2" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;RFE<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.feature_selection.RFE.html">?<span>Documentation for RFE</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>RFE(estimator=LogisticRegression(C=2.0,
                                 class_weight={&#x27;A&#x27;: 0.28, &#x27;N&#x27;: 0.62, &#x27;V&#x27;: 0.1},
                                 max_iter=5000, random_state=357,
                                 solver=&#x27;liblinear&#x27;),
    n_features_to_select=10, step=5)</pre></div> </div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" ><label for="sk-estimator-id-3" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">estimator: LogisticRegression</label><div class="sk-toggleable__content fitted"><pre>LogisticRegression(C=2.0, class_weight={&#x27;A&#x27;: 0.28, &#x27;N&#x27;: 0.62, &#x27;V&#x27;: 0.1},
                   max_iter=5000, random_state=357, solver=&#x27;liblinear&#x27;)</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" ><label for="sk-estimator-id-4" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;LogisticRegression<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html">?<span>Documentation for LogisticRegression</span></a></label><div class="sk-toggleable__content fitted"><pre>LogisticRegression(C=2.0, class_weight={&#x27;A&#x27;: 0.28, &#x27;N&#x27;: 0.62, &#x27;V&#x27;: 0.1},
                   max_iter=5000, random_state=357, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>
</div>
<p>Collect the dimension names from the feature selector and plot the
corresponding coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">important_features</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">(</span><span class="n">indices</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
    <span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;dimension&quot;</span><span class="p">,</span>
    <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;coefficient&quot;</span><span class="p">,</span>
    <span class="n">hue</span> <span class="o">=</span> <span class="s2">&quot;POS&quot;</span><span class="p">,</span>
    <span class="n">palette</span> <span class="o">=</span> <span class="s2">&quot;tab10&quot;</span><span class="p">,</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">coef_plot</span><span class="p">[</span><span class="n">coef_plot</span><span class="p">[</span><span class="s2">&quot;dimension&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">important_features</span><span class="p">)],</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span>
    
<span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Ten most important features for determining a POS tag&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">move_legend</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/798b88016411882c0c6ac653e4b9cd58c87aa4d400fcc7c11a9471dbfb018dda.png" src="../_images/798b88016411882c0c6ac653e4b9cd58c87aa4d400fcc7c11a9471dbfb018dda.png" />
</div>
</div>
</section>
<section id="tag-associations">
<h3><span class="section-number">6.5.4. </span>Tag associations<a class="headerlink" href="#tag-associations" title="Link to this heading">#</a></h3>
<p>If we multiply the vectors by the model coefficients, we can determine the
degree to which each sample is associated with a POS tag. This will concretely
ground the above graphs in specific tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">associations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wordnet</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
<span class="n">associations</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">associations</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">wordnet</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">classes_</span>
<span class="p">)</span>
<span class="n">associations</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>A</th>
      <th>N</th>
      <th>V</th>
    </tr>
    <tr>
      <th>token</th>
      <th>label</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>combustible</th>
      <th>A</th>
      <td>3.417755</td>
      <td>-2.567523</td>
      <td>-0.679895</td>
    </tr>
    <tr>
      <th>placidity</th>
      <th>N</th>
      <td>0.870346</td>
      <td>1.037917</td>
      <td>-3.231196</td>
    </tr>
    <tr>
      <th>afflicted</th>
      <th>A</th>
      <td>4.185911</td>
      <td>-3.294707</td>
      <td>-0.743858</td>
    </tr>
    <tr>
      <th>telepathy</th>
      <th>N</th>
      <td>-0.092694</td>
      <td>-0.032201</td>
      <td>-0.156638</td>
    </tr>
    <tr>
      <th>gaudy</th>
      <th>N</th>
      <td>2.875812</td>
      <td>-2.123844</td>
      <td>-0.567156</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s look at some tokens. Below, we show tokens that are unambiguously one
kind of word or another. Using the <code class="docutils literal notranslate"><span class="pre">.idxmax()</span></code> method will return which column
has the highest coefficient.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inspect</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;colorless&quot;</span><span class="p">,</span> <span class="s2">&quot;lion&quot;</span><span class="p">,</span> <span class="s2">&quot;recline&quot;</span><span class="p">]</span>
<span class="n">associations</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">inspect</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">))]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>token      label
colorless  A        A
lion       N        N
recline    V        V
dtype: object
</pre></div>
</div>
</div>
</div>
<p>That seems to work pretty well! But let’s look at a word that can be more than
one POS.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inspect</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;good&quot;</span><span class="p">,</span> <span class="s2">&quot;goad&quot;</span><span class="p">,</span> <span class="s2">&quot;great&quot;</span><span class="p">]</span>
<span class="n">associations</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">inspect</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">))]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>token  label
good   A        A
goad   V        V
great  A        N
dtype: object
</pre></div>
</div>
</div>
</div>
<p>Here we see the constraints of static embeddings. The coefficients for these
words favor one POS over another, and that should make sense: the underlying
embeddings make no distinction between two. One reason why our classifier
doesn’t always perform well very likely has to do with this as well.</p>
<p>That said, getting class probabilities tells a slightly more encouraging story.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">inspect</span><span class="p">:</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">glove</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([</span><span class="n">vector</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predictions for &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39;:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">classes_</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">probs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predictions for &#39;good&#39;:
  A: 0.33%
  N: 0.66%
  V: 0.01%
Predictions for &#39;goad&#39;:
  A: 0.04%
  N: 0.39%
  V: 0.57%
Predictions for &#39;great&#39;:
  A: 0.01%
  N: 0.99%
  V: 0.00%
</pre></div>
</div>
</div>
</div>
<p>The label with the second-highest probability tends to be for a token’s other
POS tag.</p>
</section>
<section id="shifting-vectors">
<h3><span class="section-number">6.5.5. </span>Shifting vectors<a class="headerlink" href="#shifting-vectors" title="Link to this heading">#</a></h3>
<p>The above probabilities suggest that a token is somewhat like one POS and
somewhat like another. On the one hand that conforms with how we understand POS
tags to work; but on the other, it matches the additive nature of logistic
regression: each set of coefficients indicates how much a given feature in the
vector contributes to a particular class prediction. This property about
coefficients also suggests a way to modify vectors so that they are more like
one POS tag or another.</p>
<p>The function below does just that. It shifts the direction of a vector in
vector space so that its orientation is more like a particular POS tag. How
does it do so? With simple addition: we add the coefficients for our desired
POS tag to the vector in question.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shift_vector</span><span class="p">(</span><span class="n">vector</span><span class="p">,</span> <span class="n">coef</span><span class="p">,</span> <span class="n">glove</span> <span class="o">=</span> <span class="n">glove</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">25</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Shift the direction of a vector by combining it with a coefficient.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vector : nd.ndarray</span>
<span class="sd">        The vector to shift</span>
<span class="sd">    coef : np.ndarray</span>
<span class="sd">        Coefficient vector</span>
<span class="sd">    glove : WordEmbeddings</span>
<span class="sd">        The word embeddings</span>
<span class="sd">    k : int</span>
<span class="sd">        Number of nearest neighbors to return</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    neighbors : list[tuple]</span>
<span class="sd">        The k-nearest neighbors for the original and shifted vectors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Find the vector&#39;s k-nearest neighbors</span>
    <span class="n">vector_knn</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">vector</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

    <span class="c1"># Now shift it by adding the coefficient. Then normalize and find the</span>
    <span class="c1"># k-nearest neighbors of this new vector</span>
    <span class="n">shifted</span> <span class="o">=</span> <span class="n">vector</span> <span class="o">+</span> <span class="n">coef</span>
    <span class="n">shifted</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">shifted</span><span class="p">)</span>
    <span class="n">shifted_knn</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">shifted</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

    <span class="c1"># Extract the tokens and put them into a DataFrame</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">tok1</span><span class="p">,</span> <span class="n">tok2</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">tok1</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="p">(</span><span class="n">tok2</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vector_knn</span><span class="p">,</span> <span class="n">shifted_knn</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="n">neighbors</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s try this with a few tokens. Below, we make “dessert” more like an
adjective.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shifted</span> <span class="o">=</span> <span class="n">shift_vector</span><span class="p">(</span><span class="n">glove</span><span class="p">[</span><span class="s2">&quot;dessert&quot;</span><span class="p">],</span> <span class="n">coef</span><span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">])</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">shifted</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Original&quot;</span><span class="p">,</span> <span class="s2">&quot;Shifted&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original    Shifted
----------  -----------
dessert     dessert
desserts    desserts
delicious   delicious
appetizer   appetizer
cake        tasting
chocolate   tasty
appetizers  appetizers
cakes       dishes
salad       pastries
dish        cakes
salads      salads
tasting     savory
dishes      baked
pastries    concoctions
baked       flavors
menu        wines
pudding     delectable
entree      concoction
cream       recipes
wine        dish
meal        chocolate
sorbet      pies
cookies     cake
flavors     confection
tiramisu    meals
</pre></div>
</div>
</div>
</div>
<p>How about “desert”?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shifted</span> <span class="o">=</span> <span class="n">shift_vector</span><span class="p">(</span><span class="n">glove</span><span class="p">[</span><span class="s2">&quot;desert&quot;</span><span class="p">],</span> <span class="n">coef</span><span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">])</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">shifted</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Original&quot;</span><span class="p">,</span> <span class="s2">&quot;Shifted&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original     Shifted
-----------  ------------
desert       desert
deserts      deserts
arid         arid
mojave       desolate
mountains    barren
chihuahuan   terrain
sonoran      coastal
namib        mountains
barren       mojave
gobi         mountainous
desolate     rugged
coastal      inhospitable
sahara       vast
negev        remote
dunes        populated
taklamakan   sonoran
terrain      areas
southern     chihuahuan
mountain     oases
cholistan    dunes
plains       namib
vast         surrounded
mountainous  parched
taklimakan   expanse
remote       beaches
</pre></div>
</div>
</div>
</div>
<p>Now make “language” more like a verb.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shifted</span> <span class="o">=</span> <span class="n">shift_vector</span><span class="p">(</span><span class="n">glove</span><span class="p">[</span><span class="s2">&quot;language&quot;</span><span class="p">],</span> <span class="n">coef</span><span class="p">[</span><span class="s2">&quot;V&quot;</span><span class="p">])</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">shifted</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Original&quot;</span><span class="p">,</span> <span class="s2">&quot;Shifted&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original     Shifted
-----------  -----------
language     language
languages    languages
arabic       speak
word         arabic
english      word
spoken       english
words        learn
speak        translate
vocabulary   words
meaning      teach
translation  describe
speaking     meaning
literature   vocabulary
dialect      write
bilingual    understand
hindi        spoken
writing      dialects
urdu         speakers
dialects     dialect
programming  hindi
text         translation
hebrew       read
speakers     bilingual
learning     refer
means        define
</pre></div>
</div>
</div>
</div>
<p>The modifications here are subtle, but they do exist: while the top-most
similar tokens tend to be the same between the original and shifted vectors,
their ordering moves around, often in ways that conform to what you’d expect to
see with the POS’s valences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shifted</span> <span class="o">=</span> <span class="n">shift_vector</span><span class="p">(</span><span class="n">glove</span><span class="p">[</span><span class="s2">&quot;drive&quot;</span><span class="p">],</span> <span class="n">coef</span><span class="p">[</span><span class="s2">&quot;N&quot;</span><span class="p">])</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">shifted</span><span class="p">,</span> <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Original&quot;</span><span class="p">,</span> <span class="s2">&quot;Shifted&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original    Shifted
----------  ---------
drive       drive
drives      drives
drove       drove
driving     driving
driven      car
push        driven
turn        comes
car         goes
effort      vehicle
hard        push
off         stop
way         off
run         driver
back        wheel
end         runs
vehicle     end
go          a
trying      hard
stop        speed
pull        effort
move        running
attempt     back
just        another
cars        front
get         turn
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="05_vectorization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Vectorization</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">6.1. Preliminaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-space">6.2. Vector Space</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-components">6.2.1. Vector components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-operations">6.2.2. Vector operations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-similarity">6.3. Cosine Similarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#document-similarity">6.3.1. Document similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-similarity">6.3.2. Token similarity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embeddings">6.4. Word Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-similarity-redux">6.4.1. Token similarity (redux)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#concept-modeling">6.4.2. Concept modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analogies">6.4.3. Analogies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-of-speech-disambiguation">6.5. Part-of-Speech Disambiguation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">6.5.1. Data preparation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-a-logistic-regression-model">6.5.2. Fitting a logistic regression model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-model-coefficients">6.5.3. Examining model coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tag-associations">6.5.4. Tag associations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shifting-vectors">6.5.5. Shifting vectors</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tyler Shoemaker
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>